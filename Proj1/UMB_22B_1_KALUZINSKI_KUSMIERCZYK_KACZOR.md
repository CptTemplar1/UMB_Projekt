# Uczenie Maszynowe w BezpieczeÅ„stwie
## Projekt 1
### Grupa 22B
### Autorzy: PrzemysÅ‚aw KaÅ‚uÅ¼iÅ„ski, Jakub KuÅ›mierczyk, MichaÅ‚ Kaczor

### Zadanie 1
PobraÄ‡, rozpakowaÄ‡ i przeanalizowaÄ‡ strukturÄ™ plikÃ³w i katalogÃ³w archiwum zawierajÄ…cego wiadomoÅ›ci poczty elektronicznej.  
Dane te dostÄ™pne sÄ… pod adresem:  
https://plg.uwaterloo.ca/~gvcormac/treccorpus07/  
**Uwaga.** Nie naleÅ¼y otwieraÄ‡ plikÃ³w z archiwum ani w przeglÄ…darce HTML ani w programie pocztowym!

#### Wyniki

PomyÅ›lnie pobrano i rozpakowano archiwum `TREC 2007 Public Corpus`, ktÃ³re bÄ™dzie wykorzystywane w dalszych zadaniach projektu. Ze wzglÄ™du na rozmiar archiwum (okoÅ‚o 450 MB) oraz potencjalnie niebezpiecznÄ… zawartoÅ›Ä‡ wiadomoÅ›ci spam (ktÃ³re mogÄ… zawieraÄ‡ zÅ‚oÅ›liwe linki lub nieodpowiednie treÅ›ci), archiwum zostaÅ‚o wykluczone z repozytorium Git poprzez dodanie do pliku `.gitignore`.

**Archiwum TREC 2007**  
TREC 2007 Public Corpus to publicznie dostÄ™pne archiwum wiadomoÅ›ci email uÅ¼ywane do badaÅ„ nad filtrowaniem spamu. ZbiÃ³r zostaÅ‚ opracowany w ramach Text Retrieval Conference (TREC) i stanowi standardowy benchmark do testowania algorytmÃ³w klasyfikacji wiadomoÅ›ci email.

Archiwum posiada nastÄ™pujÄ…cÄ… strukturÄ™ katalogÃ³w:  
trec07p/  
â”œâ”€â”€ data/ - GÅ‚Ã³wny folder z wiadomoÅ›ciami  
â”œâ”€â”€ full/ - Folder z peÅ‚nym indeksem  
â”œâ”€â”€ delay/ - Dane feedback tylko dla pierwszych 10,000 wiadomoÅ›ci  
â””â”€â”€ partial/ - Dane feedback tylko dla 30,388 wiadomoÅ›ci odpowiadajÄ…cych 1 odbiorcy

**Folder `data/`**:
- Zawiera 75,419 wiadomoÅ›ci email w postaci plikÃ³w tekstowych
- Pliki majÄ… nazwy w formacie `inmail.X`, gdzie X to liczba od 1 do 75419
- KaÅ¼dy plik zawiera peÅ‚nÄ… wiadomoÅ›Ä‡ email w formacie MIME

**Folder `full/`**:
- Zawiera plik `index` bÄ™dÄ…cy sÅ‚ownikiem klasyfikacji
- Format wpisÃ³w: `[etykieta] [Å›cieÅ¼ka_do_pliku]`, np. `spam ../data/inmail.1`
- Etykiety: "spam" (niechciane wiadomoÅ›ci) lub "ham" (poÅ¼Ä…dane wiadomoÅ›ci)

**Foldery dodatkowe (nieuÅ¼ywane w projekcie)**:
- `delay/` - zawiera dane feedback tylko dla pierwszych 10,000 wiadomoÅ›ci
- `partial/` - zawiera dane feedback tylko dla 30,388 wiadomoÅ›ci odpowiadajÄ…cych jednemu odbiorcy

---

**Statystyki zbioru danych**:
- **ÅÄ…czna liczba wiadomoÅ›ci**: 75,419
- **WiadomoÅ›ci ham (poÅ¼Ä…dane)**: 25,220 (33.4%)
- **WiadomoÅ›ci spam (niechciane)**: 50,199 (66.6%)
- **RozkÅ‚ad**: Przewaga wiadomoÅ›ci spam

---

**Parametry sprzÄ™towe**:  
Eksperymenty przeprowadzono na laptopie z nastÄ™pujÄ…cymi parametrami:
- Procesor: AMD Ryzen 5 4500U 2.38 GHz, 6 rdzeni, 6 wÄ…tkÃ³w
- GPU: AMD Radeon Graphics 497 MB
- PamiÄ™Ä‡ RAM: 16 GB 2666 MHz
- System operacyjny: Windows 11 Pro 64-bit

### Zadanie 2
WykorzystujÄ…c informacje z wykÅ‚adu oraz stosujÄ…c technikÄ™ zakazanych sÅ‚Ã³w kluczowych (blacklist), dokonaÄ‡ klasyfikacji binarnej wiadomoÅ›ci z archiwum z podziaÅ‚em na: spam (wiadomoÅ›ci typu spam) oraz ham (wiadomoÅ›ci poÅ¼Ä…dane).

**Uwagi:**
1. Przed przystÄ…pieniem do procesu klasyfikacji usunÄ…Ä‡ z wiadomoÅ›ci stopping words (np. the, is, are, . . . ),
dokonaÄ‡ stemizacji sÅ‚Ã³w w wiadomoÅ›ciach oraz ekstrakcji tokenÃ³w.
2. Do realizacji zadania uÅ¼yÄ‡ jÄ™zyka Python oraz bibliotek: string, email, NLTK, os.
3. ZbiÃ³r zakazanych sÅ‚Ã³w kluczowych powinien byÄ‡ wygenerowany na podstawie danych z podzbioru treningowego,
natomiast ewaluacja danych uzyskanych z podzbioru testowego.
4. Wynikiem ewaluacji powinna byÄ‡ macierz konfuzji (procentowa) oraz wartoÅ›Ä‡ wskaÅºnika accuracy, rÃ³wnieÅ¼ w
postaci procentowej.

#### Implementacja

Ze wzglÄ™du na fakt, Å¼e kod implementujÄ…cy zadania 2 i 3 jest ze sobÄ… Å›ciÅ›le powiÄ…zany, to peÅ‚na implementacja obu zadaÅ„ zostaÅ‚a umieszczona w rozdziale **implementacja** zadania 3. 

#### Wyniki

Podobnie jak implementacja, wyniki obu zadaÅ„ 2 i 3 zostaÅ‚y przedstawione w rozdziale **wyniki** zadania 3, poniewaÅ¼ kod programu zwraca wyniki obu zadaÅ„ jednoczeÅ›nie.

### Zadanie 3
ZweryfikowaÄ‡ wpÅ‚yw stemizacji na pracÄ™ algorytmu zadania drugiego a nastÄ™pnie porÃ³wnaÄ‡ uzyskane wyniki.

#### Implementacja

**1. Konfiguracja globalna**

Na wstÄ™pie programu znajduje siÄ™ kod, ktÃ³ry definiuje staÅ‚e konfiguracyjne uÅ¼ywane w caÅ‚ym programie. UÅ‚atwia to dostosowanie parametrÃ³w bez koniecznoÅ›ci modyfikowania logiki programu.

**Kod:**
``` python
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
TOP_N = 100                             # liczba sÅ‚Ã³w w blacklist
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_stemming.txt"   # nazwa pliku wynikowego
```

**2. Funkcja `load_index`**

**WejÅ›cie:**  
- `index_path` (string) - Å›cieÅ¼ka do pliku z indeksem wiadomoÅ›ci

**WyjÅ›cie:**  
- `entries` (list) - lista krotek zawierajÄ…cych peÅ‚nÄ… Å›cieÅ¼kÄ™ do pliku i etykietÄ™ (spam/ham)

**Opis:**  
Funkcja wczytuje plik indeksu, gdzie kaÅ¼da linia zawiera etykietÄ™ (spam/ham) i Å›cieÅ¼kÄ™ do pliku z wiadomoÅ›ciÄ…. Parsuje kaÅ¼dÄ… liniÄ™, tworzy peÅ‚nÄ… Å›cieÅ¼kÄ™ do pliku (usuwajÄ…c "../" z oryginalnej Å›cieÅ¼ki) i zwraca listÄ™ wszystkich wpisÃ³w.

**Kod:**
``` python
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            label, path = line.strip().split()
            full_path = os.path.join(DATA_PATH, path.replace("../", ""))
            entries.append((full_path, label))
    return entries
```

---

**3. Funkcja `preprocess_text`**

**WejÅ›cie:**  
- `text` (string) - tekst wiadomoÅ›ci email do przetworzenia
- `use_stemming` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ stemizacjÄ™

**WyjÅ›cie:**  
- `tokens` (list) - lista przetworzonych tokenÃ³w (sÅ‚Ã³w)

**Opis:**  
Funkcja przeprowadza peÅ‚ne przetwarzanie tekstu: konwersja na maÅ‚e litery, usuwanie znakÃ³w interpunkcyjnych, tokenizacja na pojedyncze sÅ‚owa, usuwanie stopwords (sÅ‚Ã³w bez znaczenia) oraz opcjonalna stemizacja przy uÅ¼yciu algorytmu PorterStemmer. Zwraca listÄ™ oczyszczonych tokenÃ³w.

**Kod:**
``` python
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if w not in stop_words and w.isalpha()]

    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(w) for w in tokens]

    return tokens
```

---

**4. Funkcja `load_email_content`**

**WejÅ›cie:**  
- `filepath` (string) - Å›cieÅ¼ka do pliku z wiadomoÅ›ciÄ… email

**WyjÅ›cie:**  
- `text` (string) - wyekstrahowana treÅ›Ä‡ wiadomoÅ›ci lub pusty string w przypadku bÅ‚Ä™du

**Opis:**  
Funkcja wczytuje i parsuje wiadomoÅ›Ä‡ email przy uÅ¼yciu biblioteki email. ObsÅ‚uguje wiadomoÅ›ci wieloczÄ™Å›ciowe (multipart), dekoduje zawartoÅ›Ä‡ i zwraca czysty tekst wiadomoÅ›ci. W przypadku bÅ‚Ä™dÃ³w zwraca pusty string.

**Kod:**
``` python
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            if msg.is_multipart():
                parts = [p.get_payload(decode=True) for p in msg.get_payload() if p.get_payload()]
                text = " ".join([str(p) for p in parts])
            else:
                text = msg.get_payload(decode=True)
            if text:
                text = text.decode(errors="ignore") if isinstance(text, bytes) else text
                return text
            else:
                return ""
    except Exception:
        return ""
```

---

**5. Funkcja `build_blacklist`**

**WejÅ›cie:**  
- `train_data` (list) - lista krotek (tokens, label) z danych treningowych
- `top_n` (int) - liczba sÅ‚Ã³w do umieszczenia na blackliÅ›cie

**WyjÅ›cie:**  
- `blacklist` (list) - lista sÅ‚Ã³w kluczowych najbardziej charakterystycznych dla spamu

**Opis:**  
Funkcja analizuje dane treningowe, zliczajÄ…c wystÄ…pienia sÅ‚Ã³w w spamie i hamie. Dla kaÅ¼dego sÅ‚owa oblicza stosunek czÄ™stotliwoÅ›ci w spamie do czÄ™stotliwoÅ›ci w hamie. Zwraca listÄ™ `top_n` sÅ‚Ã³w z najwyÅ¼szym stosunkiem, ktÃ³re bÄ™dÄ… uÅ¼ywane jako zakazane sÅ‚owa kluczowe.

**Kod:**
``` python
def build_blacklist(train_data, top_n=100):
    spam_words = {}
    ham_words = {}
    for tokens, label in train_data:
        for token in tokens:
            if label == "spam":
                spam_words[token] = spam_words.get(token, 0) + 1
            else:
                ham_words[token] = ham_words.get(token, 0) + 1

    spam_ratio = {word: spam_words[word] / (ham_words.get(word, 0) + 1) for word in spam_words}
    sorted_words = sorted(spam_ratio.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:top_n]]
```

---

**6. Funkcja `classify_email`**

**WejÅ›cie:**  
- `tokens` (list) - lista tokenÃ³w z przetworzonej wiadomoÅ›ci
- `blacklist` (list) - lista zakazanych sÅ‚Ã³w kluczowych

**WyjÅ›cie:**  
- `"spam"` lub `"ham"` (string) - wynik klasyfikacji

**Opis:**  
Funkcja klasyfikuje wiadomoÅ›Ä‡ jako spam, jeÅ›li ktÃ³rykolwiek z tokenÃ³w znajduje siÄ™ na blackliÅ›cie. W przeciwnym przypadku klasyfikuje jako ham. Jest to prosty klasyfikator oparty na zasadzie "czarnej listy".

**Kod:**  
``` python
def classify_email(tokens, blacklist):
    return "spam" if any(word in blacklist for word in tokens) else "ham"
```

---

**7. Funkcja `evaluate_model`**

**WejÅ›cie:**  
- `train_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych treningowych
- `test_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych testowych
- `use_stemming` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ stemizacjÄ™

**WyjÅ›cie:**  
- `acc` (float) - dokÅ‚adnoÅ›Ä‡ klasyfikacji w procentach
- `cm_percent` (numpy.ndarray) - macierz konfuzji w procentach
- `elapsed` (float) - czas wykonania w sekundach

**Opis:**  
Funkcja przeprowadza peÅ‚ny proces uczenia i ewaluacji modelu: przetwarza dane treningowe, buduje blacklistÄ™, klasyfikuje wiadomoÅ›ci testowe i oblicza metryki wydajnoÅ›ci. Zwraca accuracy, macierz konfuzji i czas wykonania.

**Kod:**
``` python
def evaluate_model(train_entries, test_entries, use_stemming):
    start_time = time.time()

    train_data = []
    for path, label in train_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        train_data.append((tokens, label))

    blacklist = build_blacklist(train_data, top_n=TOP_N)

    y_true, y_pred = [], []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        prediction = classify_email(tokens, blacklist)
        y_true.append(label)
        y_pred.append(prediction)

    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_true, y_pred) * 100

    return acc, cm_percent, elapsed
```

---

**8. Funkcja `main`**

**WejÅ›cie:**  
- Brak parametrÃ³w wejÅ›ciowych

**WyjÅ›cie:**  
- Brak bezpoÅ›redniego wyjÅ›cia (funkcja wykonuje program i zapisuje wyniki do pliku)

**Opis:**  
GÅ‚Ã³wna funkcja programu, ktÃ³ra koordynuje caÅ‚y proces: wczytuje i tasuje dane, dzieli na zbiÃ³r treningowy i testowy, przeprowadza dwa eksperymenty (ze stemizacjÄ… i bez), porÃ³wnuje wyniki, wyÅ›wietla raport i zapisuje wyniki do pliku tekstowego.

**Kod:**
``` python
def main():
    print("ğŸ“‚ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    results_log = []

    # Test 1: ZE STEMIZACJÄ„
    print("ğŸ§  Test 1: ZE STEMIZACJÄ„")
    acc_stem, cm_stem, time_stem = evaluate_model(train_entries, test_entries, use_stemming=True)
    print(f"ğŸ¯ Accuracy (stem): {acc_stem:.2f}% | â± Czas: {time_stem:.2f}s")
    results_log.append(f"Test 1 (ze stemizacjÄ…): accuracy={acc_stem:.2f}%, czas={time_stem:.2f}s")

    # Test 2: BEZ STEMIZACJI
    print("\nğŸ§  Test 2: BEZ STEMIZACJI")
    acc_no_stem, cm_no_stem, time_no_stem = evaluate_model(train_entries, test_entries, use_stemming=False)
    print(f"ğŸ¯ Accuracy (no stem): {acc_no_stem:.2f}% | â± Czas: {time_no_stem:.2f}s")
    results_log.append(f"Test 2 (bez stemizacji): accuracy={acc_no_stem:.2f}%, czas={time_no_stem:.2f}s")

    # PorÃ³wnanie wynikÃ³w
    diff_acc = acc_stem - acc_no_stem
    diff_time = time_stem - time_no_stem

    summary = (
        "\nğŸ“Š PORÃ“WNANIE WYNIKÃ“W\n"
        f"Ze stemizacjÄ…:    {acc_stem:.2f}% ({time_stem:.2f}s)\n"
        f"Bez stemizacji:  {acc_no_stem:.2f}% ({time_no_stem:.2f}s)\n"
        f"ğŸ§© RÃ³Å¼nica dokÅ‚adnoÅ›ci: {diff_acc:+.2f}%\n"
        f"â± RÃ³Å¼nica czasu: {diff_time:+.2f}s (wartoÅ›Ä‡ dodatnia = wolniej ze stemizacjÄ…)\n"
    )

    print(summary)
    results_log.append(summary)

    # Macierze konfuzji
    matrix_report = (
        "\nğŸ“Š MACIERZ KONFUZJI (ZE STEMIZACJÄ„):\n"
        f"      spam      ham\n"
        f"spam  {cm_stem[0,0]:6.2f}%   {cm_stem[0,1]:6.2f}%\n"
        f"ham   {cm_stem[1,0]:6.2f}%   {cm_stem[1,1]:6.2f}%\n\n"
        "ğŸ“Š MACIERZ KONFUZJI (BEZ STEMIZACJI):\n"
        f"      spam      ham\n"
        f"spam  {cm_no_stem[0,0]:6.2f}%   {cm_no_stem[0,1]:6.2f}%\n"
        f"ham   {cm_no_stem[1,0]:6.2f}%   {cm_no_stem[1,1]:6.2f}%\n"
    )
    print(matrix_report)
    results_log.append(matrix_report)

    # Zapis wynikÃ³w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_log))

    print(f"ğŸ“ Wyniki zapisano do pliku: {RESULTS_FILE}")
```

---

**9. Kompletny kod**  
PoniÅ¼ej znajduje siÄ™ kompletny kod programu, ktÃ³ry moÅ¼na uruchomiÄ‡.

**Kod:**
``` python
import os
import string
import random
import time
from email import message_from_file
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.metrics import confusion_matrix, accuracy_score
import numpy as np

# === KONFIGURACJA ===
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
TOP_N = 100                             # liczba sÅ‚Ã³w w blacklist
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_stemming.txt"   # nazwa pliku wynikowego


# === FUNKCJE ===
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            label, path = line.strip().split()
            full_path = os.path.join(DATA_PATH, path.replace("../", ""))
            entries.append((full_path, label))
    return entries

# Funcja do przetwarzania tekstu. Przeprowadza takie funkcje jak: czyszczenie, tokenizacja, usuwanie stopwords i (opcjonalnie) stemizacja
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if w not in stop_words and w.isalpha()]

    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(w) for w in tokens]

    return tokens

# Wczytuje zawartoÅ›Ä‡ e-maila.
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            if msg.is_multipart():
                parts = [p.get_payload(decode=True) for p in msg.get_payload() if p.get_payload()]
                text = " ".join([str(p) for p in parts])
            else:
                text = msg.get_payload(decode=True)
            if text:
                text = text.decode(errors="ignore") if isinstance(text, bytes) else text
                return text
            else:
                return ""
    except Exception:
        return ""

# Tworzy listÄ™ sÅ‚Ã³w kluczowych na podstawie danych treningowych.
def build_blacklist(train_data, top_n=100):
    spam_words = {}
    ham_words = {}
    for tokens, label in train_data:
        for token in tokens:
            if label == "spam":
                spam_words[token] = spam_words.get(token, 0) + 1
            else:
                ham_words[token] = ham_words.get(token, 0) + 1

    spam_ratio = {word: spam_words[word] / (ham_words.get(word, 0) + 1) for word in spam_words}
    sorted_words = sorted(spam_ratio.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:top_n]]

# Zwraca etykietÄ™ spam/ham w zaleÅ¼noÅ›ci od obecnoÅ›ci sÅ‚Ã³w zakazanych.
def classify_email(tokens, blacklist):
    return "spam" if any(word in blacklist for word in tokens) else "ham"

# Trenuje i testuje klasyfikator; zwraca accuracy, macierz konfuzji i czas.
def evaluate_model(train_entries, test_entries, use_stemming):
    start_time = time.time()

    train_data = []
    for path, label in train_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        train_data.append((tokens, label))

    blacklist = build_blacklist(train_data, top_n=TOP_N)

    y_true, y_pred = [], []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        prediction = classify_email(tokens, blacklist)
        y_true.append(label)
        y_pred.append(prediction)

    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_true, y_pred) * 100

    return acc, cm_percent, elapsed


# === GÅÃ“WNY PROGRAM ===
def main():
    print("ğŸ“‚ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    results_log = []

    # Test 1: ZE STEMIZACJÄ„
    print("ğŸ§  Test 1: ZE STEMIZACJÄ„")
    acc_stem, cm_stem, time_stem = evaluate_model(train_entries, test_entries, use_stemming=True)
    print(f"ğŸ¯ Accuracy (stem): {acc_stem:.2f}% | â± Czas: {time_stem:.2f}s")
    results_log.append(f"Test 1 (ze stemizacjÄ…): accuracy={acc_stem:.2f}%, czas={time_stem:.2f}s")

    # Test 2: BEZ STEMIZACJI
    print("\nğŸ§  Test 2: BEZ STEMIZACJI")
    acc_no_stem, cm_no_stem, time_no_stem = evaluate_model(train_entries, test_entries, use_stemming=False)
    print(f"ğŸ¯ Accuracy (no stem): {acc_no_stem:.2f}% | â± Czas: {time_no_stem:.2f}s")
    results_log.append(f"Test 2 (bez stemizacji): accuracy={acc_no_stem:.2f}%, czas={time_no_stem:.2f}s")

    # PorÃ³wnanie wynikÃ³w
    diff_acc = acc_stem - acc_no_stem
    diff_time = time_stem - time_no_stem

    summary = (
        "\nğŸ“Š PORÃ“WNANIE WYNIKÃ“W\n"
        f"ZE stemizacjÄ…:    {acc_stem:.2f}% ({time_stem:.2f}s)\n"
        f"Bez stemizacji:  {acc_no_stem:.2f}% ({time_no_stem:.2f}s)\n"
        f"ğŸ§© RÃ³Å¼nica dokÅ‚adnoÅ›ci: {diff_acc:+.2f}%\n"
        f"â± RÃ³Å¼nica czasu: {diff_time:+.2f}s (wartoÅ›Ä‡ dodatnia = wolniej ze stemizacjÄ…)\n"
    )

    print(summary)
    results_log.append(summary)

    # Macierze konfuzji
    matrix_report = (
        "\nğŸ“Š MACIERZ KONFUZJI (ZE STEMIZACJÄ„):\n"
        f"      spam      ham\n"
        f"spam  {cm_stem[0,0]:6.2f}%   {cm_stem[0,1]:6.2f}%\n"
        f"ham   {cm_stem[1,0]:6.2f}%   {cm_stem[1,1]:6.2f}%\n\n"
        "ğŸ“Š MACIERZ KONFUZJI (BEZ STEMIZACJI):\n"
        f"      spam      ham\n"
        f"spam  {cm_no_stem[0,0]:6.2f}%   {cm_no_stem[0,1]:6.2f}%\n"
        f"ham   {cm_no_stem[1,0]:6.2f}%   {cm_no_stem[1,1]:6.2f}%\n"
    )
    print(matrix_report)
    results_log.append(matrix_report)

    # Zapis wynikÃ³w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_log))

    print(f"ğŸ“ Wyniki zapisano do pliku: {RESULTS_FILE}")


if __name__ == "__main__":
    main()
```

#### Wyniki

```text
ğŸ“‚ Wczytywanie danych...
ğŸ§  Test 1: ZE STEMIZACJÄ„
ğŸ¯ Accuracy (stem): 61.83% | â± Czas: 2465.29s

ğŸ§  Test 2: BEZ STEMIZACJI
ğŸ¯ Accuracy (no stem): 58.64% | â± Czas: 239.89s

ğŸ“Š PORÃ“WNANIE WYNIKÃ“W
Ze stemizacjÄ…:    61.83% (2465.29s)
Bez stemizacji:  58.64% (239.89s)
ğŸ§© RÃ³Å¼nica dokÅ‚adnoÅ›ci: +3.20%
â± RÃ³Å¼nica czasu: +2225.39s (wartoÅ›Ä‡ dodatnia = wolniej ze stemizacjÄ…)


ğŸ“Š MACIERZ KONFUZJI (ZE STEMIZACJÄ„):
      spam      ham
spam  28.65%    38.07%
ham   0.09%     33.18%

ğŸ“Š MACIERZ KONFUZJI (BEZ STEMIZACJI):
      spam      ham
spam  25.45%    41.28%
ham   0.09%     33.19%

ğŸ“ Wyniki zapisano do pliku: results_stemming.txt
```

#### Wnioski

W zadaniach 2 i 3 zastosowano metodÄ™ blacklisty sÅ‚Ã³w kluczowych do klasyfikacji wiadomoÅ›ci email na spam i ham. PoniÅ¼ej przedstawiono szczegÃ³Å‚y dotyczÄ…ce tej metody oraz analizÄ™ uzyskanych wynikÃ³w.

**Opis metody**  
Metoda blacklisty sÅ‚Ã³w kluczowych polega na identyfikacji najbardziej charakterystycznych sÅ‚Ã³w dla kategorii spamu i wykorzystaniu ich do klasyfikacji nowych wiadomoÅ›ci. Algorytm dziaÅ‚a w dwÃ³ch etapach:
1. **Faza treningowa**: Analiza danych treningowych w celu zidentyfikowania sÅ‚Ã³w o najwyÅ¼szym stosunku wystÄ™powania w wiadomoÅ›ciach spam do ham, tworzÄ…c listÄ™ zakazanych sÅ‚Ã³w kluczowych (blacklistÄ™).
2. **Faza klasyfikacji**: Oznaczenie wiadomoÅ›ci jako spam, jeÅ›li ta zawiera ktÃ³rekolwiek ze sÅ‚Ã³w z blacklisty; w przeciwnym razie oznaczenie jako ham.

**Zalety metody**  
- **Prostota implementacji** - niewielka zÅ‚oÅ¼onoÅ›Ä‡ obliczeniowa
- **ÅatwoÅ›Ä‡ interpretacji** - moÅ¼liwoÅ›Ä‡ analizy ktÃ³re sÅ‚owa decydujÄ… o klasyfikacji
- **SzybkoÅ›Ä‡ klasyfikacji** - w fazie predykcji wymaga tylko sprawdzenia obecnoÅ›ci sÅ‚Ã³w
- **Niskie wymagania pamiÄ™ciowe** - przechowuje tylko listÄ™ sÅ‚Ã³w kluczowych

**Wady metody**  
- **Niska dokÅ‚adnoÅ›Ä‡** - prosty model moÅ¼e nie uchwyciÄ‡ zÅ‚oÅ¼onych wzorcÃ³w w danych
- **PodatnoÅ›Ä‡ na zmiany** - nowe formy spamu mogÄ… ominÄ…Ä‡ istniejÄ…cÄ… blacklistÄ™
- **Brak kontekstu** - nie uwzglÄ™dnia relacji miÄ™dzy sÅ‚owami ani ich kolejnoÅ›ci
- **Problemy z faÅ‚szywymi pozytywami** - sÅ‚owa mogÄ… mieÄ‡ rÃ³Å¼ne znaczenia w rÃ³Å¼nych kontekstach

---

Program realizowaÅ‚ dwa testy: z zastosowaniem stemizacji oraz bez niej.  
**Stemizacja** to proces redukcji sÅ‚Ã³w do ich formy podstawowej, poprzez usuniÄ™cie prefiksÃ³w i sufiksÃ³w. Na przykÅ‚ad:
- "running", "runs", "ran" â†’ "run"
- "connection", "connected", "connecting" â†’ "connect"

Celem stosowania stemizacji jest zredukowanie wymiarowoÅ›ci danych tekstowych poprzez grupowanie rÃ³Å¼nych form tego samego sÅ‚owa, co powinno poprawiÄ‡ skutecznoÅ›Ä‡ klasyfikacji poprzez lepsze uogÃ³lnienie wzorcÃ³w.

---

**Konfiguracja programu**  
Na wstÄ™pie okreÅ›lono parametry eksperymentu, takie jak Å›cieÅ¼ki do danych, stosunek podziaÅ‚u na zbiÃ³r treningowy i testowy oraz liczba sÅ‚Ã³w w blacklist. NajwaÅ¼niejsze parametry to:
- **`TRAIN_RATIO = 0.8`** - Standardowy podziaÅ‚ 80/20, ktÃ³ry jest powszechnie stosowany w uczeniu maszynowym, zapewniajÄ…c wystarczajÄ…cÄ… iloÅ›Ä‡ danych do treningu (60,335 wiadomoÅ›ci) przy zachowaniu reprezentatywnego zbioru testowego (15,084 wiadomoÅ›ci).
- **`TOP_N = 100`** - Limit 100 sÅ‚Ã³w w blackliÅ›cie stanowi kompromis miÄ™dzy skutecznoÅ›ciÄ… a specyficznoÅ›ciÄ…. Zbyt maÅ‚a lista mogÅ‚aby pomijaÄ‡ istotne wzorce, a zbyt duÅ¼a zwiÄ™kszaÅ‚aby ryzyko nadmiernego dopasowania.
- **`SAMPLE_SIZE = None`** - UÅ¼ycie caÅ‚ego zbioru danych zapewnia wiarygodnoÅ›Ä‡ wynikÃ³w, jednak parametr umoÅ¼liwia szybkie testy na mniejszych prÃ³bkach podczas rozwoju algorytmu.

---

**Analiza wynikÃ³w**  
PoniÅ¼sza tabela przedstawia porÃ³wnanie kluczowych metryk uzyskanych w obu testach:

| Metryka | Ze stemizacjÄ… | Bez stemizacji | RÃ³Å¼nica | Wnioski |
|---------|---------------|----------------|---------|---------|
| **Accuracy** | 61.83% | 58.64% | **+3.20%** | Stemizacja poprawia ogÃ³lnÄ… dokÅ‚adnoÅ›Ä‡ klasyfikacji |
| **Czas wykonania** | 2465.29s (â‰ˆ41 min) | 239.89s (â‰ˆ4 min) | **+2225.39s** | Stemizacja znaczÄ…co wydÅ‚uÅ¼a czas przetwarzania |
| **Poprawny spam** | 28.65% | 25.45% | **+3.20%** | Lepsze wykrywanie wiadomoÅ›ci spam |
| **FaÅ‚szywe negatywy** | 38.07% | 41.28% | **-3.21%** | Mniej spamu przechodzi niezauwaÅ¼one |
| **FaÅ‚szywe pozytywy** | 0.09% | 0.09% | **0.00%** | Brak wpÅ‚ywu na bÅ‚Ä™dne oznaczanie ham |
| **Poprawny ham** | 33.18% | 33.19% | **-0.01%** | Klasyfikacja ham pozostaje niezmienna |

**EfektywnoÅ›Ä‡ stemizacji**  
Stemizacja przynosi wymierne korzyÅ›ci w skutecznoÅ›ci klasyfikacji, zwiÄ™kszajÄ…c accuracy o 3.20%. Poprawa koncentruje siÄ™ gÅ‚Ã³wnie na lepszym wykrywaniu spamu, gdzie obserwujemy wzrost poprawnie sklasyfikowanych wiadomoÅ›ci spam o 3.20% i redukcjÄ™ faÅ‚szywych negatywÃ³w o 3.21%.

**Koszt wydajnoÅ›ciowy**  
Czas przetwarzania ze stemizacjÄ… jest okoÅ‚o 10x dÅ‚uÅ¼szy (2465s vs 240s), co stanowi istotny kompromis w zastosowaniach wymagajÄ…cych szybkiego przetwarzania duÅ¼ych zbiorÃ³w danych.

**WpÅ‚yw na rÃ³Å¼ne kategorie wiadomoÅ›ci**  
- **Spam**: Stemizacja znaczÄ…co poprawia wykrywalnoÅ›Ä‡ (+3.20%)
- **Ham**: Brak zauwaÅ¼alnego wpÅ‚ywu na klasyfikacjÄ™
- **FaÅ‚szywe pozytywy**: Minimalne i identyczne w obu wersjach (0.09%)

---

Pomimo poprawy dziÄ™ki stemizacji, ogÃ³lna dokÅ‚adnoÅ›Ä‡ na poziomie ~60% potwierdza, Å¼e prosta **blacklista** sÅ‚Ã³w kluczowych ma fundamentalne ograniczenia i powinna byÄ‡ traktowana jako element szerszego systemu filtrowania spamu, a nie samodzielne rozwiÄ…zanie.  
Wersja ze stemizacjÄ… jest preferowana ze wzglÄ™du na lepsze wykrywanie spamu, jednak kosztem znacznie dÅ‚uÅ¼szego czasu przetwarzania. WybÃ³r miÄ™dzy wersjami powinien uwzglÄ™dniaÄ‡ specyficzne wymagania dotyczÄ…ce dokÅ‚adnoÅ›ci i wydajnoÅ›ci w danym zastosowaniu.  
Obie wersje mogÄ… skutecznie sÅ‚uÅ¼yÄ‡ jako pierwsza linia obrony, jednak wysoki odsetek faÅ‚szywych negatywÃ³w (~38-41%) wskazuje na potrzebÄ™ dodatkowych metod weryfikacji.

### Zadanie 4
DokonaÄ‡ klasyfikacji binarnej wiadomoÅ›ci z archiwum (zadanie 1) na spam i ham, stosujÄ…c algorytmy rozmytego haszowania.

**Uwagi:**
1. Do tego celu uÅ¼yÄ‡ algorytmu LSH (MinHash, MinHashLSH) z biblioteki datasketch.
2. Wyniki pracy algorytmu przedstawiÄ‡ przy pomocy procentowej macierzy konfuzji i wskaÅºnika accuracy.
3. SprawdziÄ‡ pracÄ™ programu dla rÃ³Å¼nych wartoÅ›ci parametru threshold funkcji MinHashLSH.
4. PorÃ³wnaÄ‡ uzyskane wyniki z wynikami z poprzednich zadaÅ„.

#### Implementacja

**1. Konfiguracja globalna**

Na wstÄ™pie programu znajduje siÄ™ kod, ktÃ³ry definiuje staÅ‚e konfiguracyjne uÅ¼ywane w caÅ‚ym programie. UÅ‚atwia to dostosowanie parametrÃ³w bez koniecznoÅ›ci modyfikowania logiki programu.

**Kod:**  
``` python
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_lsh.txt"        # nazwa pliku wynikowego

# Parametry LSH / MinHash
NUM_PERM = 128                          # liczba permutacji w MinHash
SHINGLE_SIZE = 3                        # rozmiar shingli (k-gramÃ³w)
USE_STEMMING = True                     # czy stosowaÄ‡ stemizacjÄ™
THRESHOLDS = [0.1, 0.3, 0.5, 0.7, 0.9]  # testowane progi LSH
DEFAULT_LABEL = "ham"                   # etykieta domyÅ›lna, gdy brak dopasowaÅ„ w LSH

random.seed(42)                         # ustawienie ziarna losowoÅ›ci
```

**2. Funkcja `load_index`**

**WejÅ›cie:**  
- `index_path` (string) - Å›cieÅ¼ka do pliku z indeksem wiadomoÅ›ci

**WyjÅ›cie:**  
- `entries` (list) - lista krotek zawierajÄ…cych peÅ‚nÄ… Å›cieÅ¼kÄ™ do pliku i etykietÄ™ (spam/ham)

**Opis:**  
Funkcja wczytuje plik indeksu, parsuje kaÅ¼dÄ… liniÄ™ rozdzielajÄ…c jÄ… na etykietÄ™ i Å›cieÅ¼kÄ™ do pliku. Normalizuje Å›cieÅ¼ki usuwajÄ…c "../" i tworzy peÅ‚ne Å›cieÅ¼ki wzglÄ™dem gÅ‚Ã³wnego katalogu danych. Zwraca listÄ™ wszystkich wpisÃ³w gotowych do dalszego przetwarzania.

**Kod:**  
``` python
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                # Normalizuje Å›cieÅ¼kÄ™: '../data/inmail.X' -> 'trec07p/data/inmail.X'
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries
```

---

**3. Funkcja `load_email_content`**

**WejÅ›cie:**  
- `filepath` (string) - Å›cieÅ¼ka do pliku z wiadomoÅ›ciÄ… email

**WyjÅ›cie:**  
- `payload` (string) - wyekstrahowana treÅ›Ä‡ wiadomoÅ›ci lub pusty string w przypadku bÅ‚Ä™du

**Opis:**  
Funkcja wczytuje i parsuje wiadomoÅ›Ä‡ email przy uÅ¼yciu biblioteki email. ObsÅ‚uguje zarÃ³wno wiadomoÅ›ci wieloczÄ™Å›ciowe (multipart) jak i pojedyncze. Dla wiadomoÅ›ci wieloczÄ™Å›ciowych iteruje przez wszystkie czÄ™Å›ci i wyciÄ…ga tylko te o typie tekstowym. Dekoduje zawartoÅ›Ä‡ binarnÄ… i obsÅ‚uguje bÅ‚Ä™dy kodowania. W przypadku wyjÄ…tkÃ³w zwraca pusty string.

**Kod:**  
``` python
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            payload = ""
            if msg.is_multipart():
                # zÅ‚Ä…cz wszystkie czÄ™Å›ci tekstowe
                parts = []
                for part in msg.walk():
                    # tylko tekstowe czÄ™Å›ci (ignore attachments)
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return payload or ""
    except Exception:
        return ""
```

---

**4. Funkcja `preprocess_text`**

**WejÅ›cie:**  
- `text` (string) - tekst wiadomoÅ›ci email do przetworzenia
- `use_stemming` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ stemizacjÄ™

**WyjÅ›cie:**  
- `tokens` (list) - lista przetworzonych tokenÃ³w (sÅ‚Ã³w)

**Opis:**  
Funkcja przeprowadza peÅ‚ne przetwarzanie tekstu przed uÅ¼yciem w algorytmie LSH: konwersja na maÅ‚e litery, usuwanie znakÃ³w interpunkcyjnych, tokenizacja na pojedyncze sÅ‚owa, filtrowanie tylko sÅ‚Ã³w alfabetycznych, usuwanie stopwords (sÅ‚Ã³w bez znaczenia) oraz opcjonalna stemizacja przy uÅ¼yciu algorytmu PorterStemmer.

**Kod:**  
``` python
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(t) for t in tokens]
    return tokens
```

---

**5. Funkcja `get_shingles`**

**WejÅ›cie:**  
- `tokens` (list) - lista tokenÃ³w z przetworzonej wiadomoÅ›ci
- `k` (int) - rozmiar shingli (k-gramÃ³w)

**WyjÅ›cie:**  
- `shingles` (list) - lista shingli utworzonych z ciÄ…gÅ‚ych sekwencji tokenÃ³w

**Opis:**  
Funkcja tworzy k-gramy (shingle) z ciÄ…gÅ‚ych sekwencji tokenÃ³w. Dla podanej listy tokenÃ³w tworzy wszystkie moÅ¼liwe ciÄ…gÅ‚e sekwencje o dÅ‚ugoÅ›ci k, Å‚Ä…czÄ…c je w stringi. JeÅ›li dÅ‚ugoÅ›Ä‡ tokenÃ³w jest mniejsza niÅ¼ k, zwraca oryginalne tokeny jako fallback.

**Kod:**  
``` python
def get_shingles(tokens, k=3):
    if len(tokens) < k:
        # fallback: uÅ¼yj pojedynczych tokenÃ³w
        return tokens
    shingles = []
    for i in range(len(tokens) - k + 1):
        sh = " ".join(tokens[i:i + k])
        shingles.append(sh)
    return shingles
```

---

**6. Funkcja `build_minhash_from_shingles`**

**WejÅ›cie:**  
- `shingles` (list) - lista shingli (k-gramÃ³w)
- `num_perm` (int) - liczba permutacji dla algorytmu MinHash

**WyjÅ›cie:**  
- `m` (MinHash) - obiekt MinHash reprezentujÄ…cy dokument

**Opis:**  
Funkcja tworzy obiekt MinHash dla dokumentu na podstawie jego shingli. UÅ¼ywa zestawu unikalnych shingli aby uniknÄ…Ä‡ duplikatÃ³w. KaÅ¼dy shingle jest kodowany do postaci bajtÃ³w przed dodaniem do MinHash. Parametr num_perm okreÅ›la dokÅ‚adnoÅ›Ä‡ haszowania.

**Kod:**  
``` python
def build_minhash_from_shingles(shingles, num_perm=128):
    m = MinHash(num_perm=num_perm)
    # uÅ¼ywamy zestawu, aby uniknÄ…Ä‡ wielokrotnego dodawania tego samego shingla
    for s in set(shingles):
        m.update(s.encode("utf8"))
    return m
```

---

**7. Funkcja `prepare_train_min_hashes`**

**WejÅ›cie:**  
- `train_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych treningowych
- `use_stemming` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ stemizacjÄ™
- `shingle_k` (int) - rozmiar shingli
- `num_perm` (int) - liczba permutacji dla MinHash

**WyjÅ›cie:**  
- `id_to_minhash` (dict) - sÅ‚ownik mapujÄ…cy ID dokumentu na jego MinHash
- `id_to_label` (dict) - sÅ‚ownik mapujÄ…cy ID dokumentu na jego etykietÄ™

**Opis:**  
Funkcja przetwarza wszystkie dokumenty treningowe: wczytuje treÅ›Ä‡, przetwarza tekst, tworzy shingle, buduje MinHash dla kaÅ¼dego dokumentu. Przypisuje unikalne ID kaÅ¼demu dokumentowi i zwraca dwa sÅ‚owniki do dalszego uÅ¼ycia w LSH.

**Kod:**  
``` python
def prepare_train_min_hashes(train_entries, use_stemming=True, shingle_k=3, num_perm=128):
    id_to_minhash = {}
    id_to_label = {}
    for idx, (path, label) in enumerate(train_entries):
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        doc_id = f"doc{idx}"
        id_to_minhash[doc_id] = m
        id_to_label[doc_id] = label
    return id_to_minhash, id_to_label
```

---

**8. Funkcja `classify_with_lsh`**

**WejÅ›cie:**  
- `lsh` (MinHashLSH) - obiekt LSH z wstawionymi dokumentami treningowymi
- `train_label_map` (dict) - sÅ‚ownik mapujÄ…cy ID na etykiety treningowe
- `test_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych testowych
- `use_stemming` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ stemizacjÄ™
- `shingle_k` (int) - rozmiar shingli
- `num_perm` (int) - liczba permutacji dla MinHash

**WyjÅ›cie:**  
- `y_true` (list) - lista prawdziwych etykiet
- `y_pred` (list) - lista przewidywanych etykiet

**Opis:**  
Funkcja klasyfikuje dokumenty testowe uÅ¼ywajÄ…c LSH. Dla kaÅ¼dego dokumentu testowego: przetwarza tekst, tworzy shingle, buduje MinHash, pyta LSH o podobne dokumenty. JeÅ›li znaleziono dopasowania, przeprowadza gÅ‚osowanie wiÄ™kszoÅ›ciowe na podstawie etykiet dokumentÃ³w treningowych. JeÅ›li brak dopasowaÅ„, uÅ¼ywa etykiety domyÅ›lnej.

**Kod:**  
``` python
def classify_with_lsh(lsh, train_label_map, test_entries, use_stemming=True, shingle_k=3, num_perm=128):
    y_true = []
    y_pred = []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        matches = lsh.query(m)  # lista dopasowanych dokumentÃ³w treningowych
        if matches:
            # gÅ‚osowanie wiÄ™kszoÅ›ciowe etykiet
            votes = [train_label_map[mid] for mid in matches if mid in train_label_map]
            if votes:
                counter = Counter(votes)
                pred = counter.most_common(1)[0][0]
            else:
                pred = DEFAULT_LABEL
        else:
            pred = DEFAULT_LABEL
        y_true.append(label)
        y_pred.append(pred)
    return y_true, y_pred
```

---

**9. Funkcja `main`**

**WejÅ›cie:**  
- Brak parametrÃ³w wejÅ›ciowych

**WyjÅ›cie:**  
- Brak bezpoÅ›redniego wyjÅ›cia (funkcja wykonuje program i zapisuje wyniki do pliku)

**Opis:**  
GÅ‚Ã³wna funkcja programu koordynujÄ…ca caÅ‚y proces klasyfikacji LSH: wczytuje i tasuje dane, dzieli na zbiory treningowe i testowe, przygotowuje MinHash dla danych treningowych, testuje rÃ³Å¼ne wartoÅ›ci threshold dla LSH, oblicza metryki wydajnoÅ›ci dla kaÅ¼dego threshold i zapisuje szczegÃ³Å‚owe wyniki do pliku. Dla kaÅ¼dego threshold buduje nowy indeks LSH i przeprowadza klasyfikacjÄ™.

**Kod:**  
``` python
def main():
    print("ğŸ“‚ Wczytywanie indexu i danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"âš ï¸ SAMPLE_SIZE aktywne. WykorzystujÄ™ {len(index_entries)} pierwszych wpisÃ³w")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    print(f"ÅÄ…cznie: {len(index_entries)} dokumentÃ³w; trening: {len(train_entries)}; test: {len(test_entries)}")
    results_lines = []
    results_lines.append(f"LSH MinHash results\nSAMPLE_SIZE={SAMPLE_SIZE}\nNUM_PERM={NUM_PERM}\nSHINGLE_SIZE={SHINGLE_SIZE}\nUSE_STEMMING={USE_STEMMING}\n")

    # Przygotowuje MinHash na treningu (raz). BÄ™dzie ono wstawiane do nowych LSH dla rÃ³Å¼nych thresholdÃ³w
    print("ğŸ§  Budowanie MinHash dla zbioru treningowego...")
    t0 = time.time()
    train_mh_map, train_label_map = prepare_train_min_hashes(train_entries, use_stemming=USE_STEMMING,
                                                            shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
    t_prep = time.time() - t0
    print(f"Gotowe. Czas przygotowania MinHash treningu: {t_prep:.2f}s")
    results_lines.append(f"prepare_time={t_prep:.2f}s\n")

    # Dla kaÅ¼dego threshold buduje nowy MinHashLSH (z tym samym num_perm) i wstawia minhashy treningowe
    for thresh in THRESHOLDS:
        print(f"\nğŸ” Test dla threshold = {thresh}")
        results_lines.append(f"\nTHRESHOLD={thresh}\n")
        # buduje LSH z parametrem threshold
        t0 = time.time()
        lsh = MinHashLSH(threshold=thresh, num_perm=NUM_PERM)
        # wstawia minhashy treningowe
        for doc_id, mh in train_mh_map.items():
            lsh.insert(doc_id, mh)
        build_time = time.time() - t0
        print(f"LSH zbudowano w {build_time:.2f}s")

        # klasyfikacja testÃ³w
        t1 = time.time()
        y_true, y_pred = classify_with_lsh(lsh, train_label_map, test_entries,
                                          use_stemming=USE_STEMMING, shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
        elapsed = time.time() - t1

        # metryki
        labels = ["spam", "ham"]
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        cm_percent = cm / np.sum(cm) * 100
        acc = accuracy_score(y_true, y_pred) * 100

        # raport w konsoli
        print(f"ğŸ¯ Accuracy: {acc:.2f}% | â± Czas tworzenia LSH: {build_time:.2f}s | â± Czas klasyfikacji LSH: {elapsed:.2f}s")
        print("ğŸ“Š Confusion matrix (%):")
        print(f"      spam      ham")
        print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
        print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

        # zapis wynikÃ³w
        results_lines.append(f"accuracy={acc:.2f}%\n")
        results_lines.append(f"build_time={build_time:.2f}s classify_time={elapsed:.2f}s\n")
        results_lines.append("confusion_percent:\n")
        results_lines.append(f"spam_spam={cm_percent[0,0]:6.2f}% spam_ham={cm_percent[0,1]:6.2f}%\n")
        results_lines.append(f"ham_spam={cm_percent[1,0]:6.2f}% ham_ham={cm_percent[1,1]:6.2f}%\n")

    # zapis do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_lines))

    print(f"\nğŸ“ Wyniki zapisano do: {RESULTS_FILE}")
```

---

**10. Kompletny kod**  
PoniÅ¼ej znajduje siÄ™ kompletny kod programu, ktÃ³ry moÅ¼na uruchomiÄ‡.

**Kod:**  
``` python
import os
import string
import random
import time
from email import message_from_file
from collections import Counter, defaultdict

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

from datasketch import MinHash, MinHashLSH
from sklearn.metrics import confusion_matrix, accuracy_score
import numpy as np

# === KONFIGURACJA ===
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_lsh.txt"        # nazwa pliku wynikowego

# Parametry LSH / MinHash
NUM_PERM = 128                          # liczba permutacji w MinHash
SHINGLE_SIZE = 3                        # rozmiar shingli (k-gramÃ³w)
USE_STEMMING = True                     # czy stosowaÄ‡ stemizacjÄ™
THRESHOLDS = [0.1, 0.3, 0.5, 0.7, 0.9]  # testowane progi LSH
DEFAULT_LABEL = "ham"                   # etykieta domyÅ›lna, gdy brak dopasowaÅ„ w LSH

random.seed(42)                         # ustawienie ziarna losowoÅ›ci


# === POMOCNICZE FUNKCJE ===
# wczytuje index plikÃ³w i etykiet
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                # Normalizuje Å›cieÅ¼kÄ™: '../data/inmail.X' -> 'trec07p/data/inmail.X'
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries

# Wczytuje zawartoÅ›Ä‡ e-maila i zwraca string tekstowy (ignoruje bÅ‚Ä™dy kodowania)
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            payload = ""
            if msg.is_multipart():
                # zÅ‚Ä…cz wszystkie czÄ™Å›ci tekstowe
                parts = []
                for part in msg.walk():
                    # tylko tekstowe czÄ™Å›ci (ignore attachments)
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return payload or ""
    except Exception:
        return ""


# Przetwarza tekst: czyszczenie, tokenizacja, usuwanie stopwords i (opcjonalnie) stemizacja. zwraca listÄ™ tokenÃ³w
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(t) for t in tokens]
    return tokens

# Zwraca listÄ™ shingli (k-gramÃ³w) utworzonych z tokenÃ³w (continuous k-word shingles)
def get_shingles(tokens, k=3):
    if len(tokens) < k:
        # fallback: uÅ¼yj pojedynczych tokenÃ³w
        return tokens
    shingles = []
    for i in range(len(tokens) - k + 1):
        sh = " ".join(tokens[i:i + k])
        shingles.append(sh)
    return shingles


# Zwraca MinHash obliczony na zestawie shingles (unikatowych).
def build_minhash_from_shingles(shingles, num_perm=128):
    m = MinHash(num_perm=num_perm)
    # uÅ¼ywamy zestawu, aby uniknÄ…Ä‡ wielokrotnego dodawania tego samego shingla
    for s in set(shingles):
        m.update(s.encode("utf8"))
    return m


# === PROCEDURY TRENING / TEST ===
# Przygotowuje MinHash dla kaÅ¼dego dokumentu treningowego i mapuje identyfikatory na etykiety. Zwraca dwa sÅ‚owniki 
def prepare_train_min_hashes(train_entries, use_stemming=True, shingle_k=3, num_perm=128):
    id_to_minhash = {}
    id_to_label = {}
    for idx, (path, label) in enumerate(train_entries):
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        doc_id = f"doc{idx}"
        id_to_minhash[doc_id] = m
        id_to_label[doc_id] = label
    return id_to_minhash, id_to_label


# Dla kaÅ¼dego dokumentu testowego: oblicza MinHash, pyta LSH o dopasowania, jeÅ›li lista niepusta - dokonuje gÅ‚osowania etykiet (majority vote), jeÅ›li pusta - przypisuje DEFAULT_LABEL
def classify_with_lsh(lsh, train_label_map, test_entries, use_stemming=True, shingle_k=3, num_perm=128):
    y_true = []
    y_pred = []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        matches = lsh.query(m)  # lista dopasowanych dokumentÃ³w treningowych
        if matches:
            # gÅ‚osowanie wiÄ™kszoÅ›ciowe etykiet
            votes = [train_label_map[mid] for mid in matches if mid in train_label_map]
            if votes:
                counter = Counter(votes)
                pred = counter.most_common(1)[0][0]
            else:
                pred = DEFAULT_LABEL
        else:
            pred = DEFAULT_LABEL
        y_true.append(label)
        y_pred.append(pred)
    return y_true, y_pred


# === GÅÃ“WNY PROGRAM ===
def main():
    print("ğŸ“‚ Wczytywanie indexu i danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"âš ï¸ SAMPLE_SIZE aktywne. WykorzystujÄ™ {len(index_entries)} pierwszych wpisÃ³w")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    print(f"ÅÄ…cznie: {len(index_entries)} dokumentÃ³w; trening: {len(train_entries)}; test: {len(test_entries)}")
    results_lines = []
    results_lines.append(f"LSH MinHash results\nSAMPLE_SIZE={SAMPLE_SIZE}\nNUM_PERM={NUM_PERM}\nSHINGLE_SIZE={SHINGLE_SIZE}\nUSE_STEMMING={USE_STEMMING}\n")

    # Przygotowuje MinHash na treningu (raz). BÄ™dzie ono wstawiane do nowych LSH dla rÃ³Å¼nych thresholdÃ³w
    print("ğŸ§  Budowanie MinHash dla zbioru treningowego...")
    t0 = time.time()
    train_mh_map, train_label_map = prepare_train_min_hashes(train_entries, use_stemming=USE_STEMMING,
                                                            shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
    t_prep = time.time() - t0
    print(f"Gotowe. Czas przygotowania MinHash treningu: {t_prep:.2f}s")
    results_lines.append(f"prepare_time={t_prep:.2f}s\n")

    # Dla kaÅ¼dego threshold buduje nowy MinHashLSH (z tym samym num_perm) i wstawia minhashy treningowe
    for thresh in THRESHOLDS:
        print(f"\nğŸ” Test dla threshold = {thresh}")
        results_lines.append(f"\nTHRESHOLD={thresh}\n")
        # buduje LSH z parametrem threshold
        t0 = time.time()
        lsh = MinHashLSH(threshold=thresh, num_perm=NUM_PERM)
        # wstawia minhashy treningowe
        for doc_id, mh in train_mh_map.items():
            lsh.insert(doc_id, mh)
        build_time = time.time() - t0
        print(f"LSH zbudowano w {build_time:.2f}s")

        # klasyfikacja testÃ³w
        t1 = time.time()
        y_true, y_pred = classify_with_lsh(lsh, train_label_map, test_entries,
                                          use_stemming=USE_STEMMING, shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
        elapsed = time.time() - t1

        # metryki
        labels = ["spam", "ham"]
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        cm_percent = cm / np.sum(cm) * 100
        acc = accuracy_score(y_true, y_pred) * 100

        # raport w konsoli
        print(f"ğŸ¯ Accuracy: {acc:.2f}% | â± Czas tworzenia LSH: {build_time:.2f}s | â± Czas klasyfikacji LSH: {elapsed:.2f}s")
        print("ğŸ“Š Confusion matrix (%):")
        print(f"      spam      ham")
        print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
        print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

        # zapis wynikÃ³w
        results_lines.append(f"accuracy={acc:.2f}%\n")
        results_lines.append(f"build_time={build_time:.2f}s classify_time={elapsed:.2f}s\n")
        results_lines.append("confusion_percent:\n")
        results_lines.append(f"spam_spam={cm_percent[0,0]:6.2f}% spam_ham={cm_percent[0,1]:6.2f}%\n")
        results_lines.append(f"ham_spam={cm_percent[1,0]:6.2f}% ham_ham={cm_percent[1,1]:6.2f}%\n")

    # zapis do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_lines))

    print(f"\nğŸ“ Wyniki zapisano do: {RESULTS_FILE}")


if __name__ == "__main__":
    main()
```

#### Wyniki

``` text
ğŸ“‚ Wczytywanie indexu i danych...
ÅÄ…cznie: 75419 dokumentÃ³w; trening: 60335; test: 15084
ğŸ§  Budowanie MinHash dla zbioru treningowego...
Gotowe. Czas przygotowania MinHash treningu: 452.07s

ğŸ” Test dla threshold = 0.1
LSH zbudowano w 9.47s
ğŸ¯ Accuracy: 94.50% | â± Czas tworzenia LSH: 9.47s | â± Czas klasyfikacji LSH: 129.05s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  60.55%    5.32%
ham   0.18%     33.94%

ğŸ” Test dla threshold = 0.3
LSH zbudowano w 6.92s
ğŸ¯ Accuracy: 88.80% | â± Czas tworzenia LSH: 6.92s | â± Czas klasyfikacji LSH: 120.82s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  54.76%    11.12%
ham   0.09%     34.04%

ğŸ” Test dla threshold = 0.5
LSH zbudowano w 4.78s
ğŸ¯ Accuracy: 79.14% | â± Czas tworzenia LSH: 4.78s | â± Czas klasyfikacji LSH: 118.51s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  45.09%    20.79%
ham   0.07%     34.06%

ğŸ” Test dla threshold = 0.7
LSH zbudowano w 3.11s
ğŸ¯ Accuracy: 70.72% | â± Czas tworzenia LSH: 3.11s | â± Czas klasyfikacji LSH: 116.56s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  36.65%    29.23%
ham   0.05%     34.07%

ğŸ” Test dla threshold = 0.9
LSH zbudowano w 1.53s
ğŸ¯ Accuracy: 62.70% | â± Czas tworzenia LSH: 1.53s | â± Czas klasyfikacji LSH: 115.94s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  28.61%    37.26%
ham   0.04%     34.08%

ğŸ“ Wyniki zapisano do: results_lsh.txt
```

#### Wnioski

W zadaniu 4 zastosowano algorytm Locality Sensitive Hashing (LSH) z MinHash do klasyfikacji binarnej wiadomoÅ›ci email na spam i ham. Przetestowano rÃ³Å¼ne wartoÅ›ci progu (threshold) dla LSH, co miaÅ‚o wpÅ‚yw na dokÅ‚adnoÅ›Ä‡ klasyfikacji.

**Opis algorytmu**  
Algorytm LSH (Locality-Sensitive Hashing) z wykorzystaniem MinHash to zaawansowana technika oparta na teorii prawdopodobieÅ„stwa, sÅ‚uÅ¼Ä…ca do znajdowania podobnych dokumentÃ³w w duÅ¼ych zbiorach danych. Algorytm dziaÅ‚a w nastÄ™pujÄ…cych etapach:
1. **Tworzenie shingli**: PodziaÅ‚ tekstu na ciÄ…gÅ‚e sekwencje sÅ‚Ã³w (k-gramy)
2. **MinHash**: Generowanie sygnatur dokumentÃ³w poprzez wielokrotne haszowanie shingli i wybieranie minimalnych wartoÅ›ci hash
3. **LSH**: Grupowanie podobnych dokumentÃ³w w "koszykach" na podstawie podobieÅ„stwa ich sygnatur
4. **Klasyfikacja**: GÅ‚osowanie wiÄ™kszoÅ›ciowe etykiet spoÅ›rÃ³d najbliÅ¼szych sÄ…siadÃ³w w zbiorze treningowym

**Zalety metody**
- **SkalowalnoÅ›Ä‡** - efektywne przetwarzanie duÅ¼ych zbiorÃ³w danych 
- **OdpornoÅ›Ä‡ na permutacje** - niezaleÅ¼noÅ›Ä‡ od kolejnoÅ›ci sÅ‚Ã³w w dokumencie
- **Wykrywanie podobieÅ„stw** - zdolnoÅ›Ä‡ do identyfikacji dokumentÃ³w o podobnej treÅ›ci 
- **Probabilistyczna dokÅ‚adnoÅ›Ä‡** - kontrola precyzji poprzez parametr threshold

**Wady metody**
- **ZÅ‚oÅ¼onoÅ›Ä‡ konfiguracji** - wymaga dostrojenia wielu parametrÃ³w (num_perm, shingle_size, threshold) w celu uzyskania optymalnych wynikÃ³w
- **Koszt pamiÄ™ciowy** - przechowywanie sygnatur MinHash dla wszystkich dokumentÃ³w 
- **ZaleÅ¼noÅ›Ä‡ od jakoÅ›ci danych** - wraÅ¼liwoÅ›Ä‡ na preprocessing i dobÃ³r shingli

---

**Konfiguracja programu**  
Podobnie jak poprzednio, na wstÄ™pie naleÅ¼aÅ‚o zdefiniowaÄ‡ staÅ‚e konfiguracyjne, takie jak Å›cieÅ¼ki do danych, parametry LSH/MinHash oraz ustawienia dotyczÄ…ce przetwarzania tekstu (stemizacja, rozmiar shingli itp.). DziaÅ‚anie algorytmu zostaÅ‚o przetestowane dla rÃ³Å¼nych wartoÅ›ci progu (threshold) LSH: 0.1, 0.3, 0.5, 0.7, 0.9. NajwaÅ¼niejsze parametry to: 
- **`TRAIN_RATIO = 0.8`** - Standardowy podziaÅ‚ 80/20 zapewnia odpowiedniÄ… iloÅ›Ä‡ danych treningowych (60,335 wiadomoÅ›ci) przy zachowaniu reprezentatywnego zbioru testowego (15,084 wiadomoÅ›ci).
- **`NUM_PERM = 128`** - Liczba permutacji stanowi kompromis miÄ™dzy dokÅ‚adnoÅ›ciÄ… a wydajnoÅ›ciÄ…. WiÄ™ksza liczba zwiÄ™ksza precyzjÄ™, ale kosztem czasu przetwarzania.
- **`SHINGLE_SIZE = 3`** - Rozmiar shingli (3-gramÃ³w) pozwala na uchwycenie kontekstu sÅ‚Ã³w, co jest kluczowe dla identyfikacji podobieÅ„stw miÄ™dzy dokumentami.
- **`USE_STEMMING = True`** - WÅ‚Ä…czenie stemizacji, co wynika z pozytywnych doÅ›wiadczeÅ„ z Zadania 3, gdzie stemizacja poprawiÅ‚a skutecznoÅ›Ä‡ klasyfikacji.
- **`THRESHOLDS = [0.1, 0.3, 0.5, 0.7, 0.9]`** - Zakres progÃ³w testowych od bardzo  niskiego (0.1) do wysokiego (0.9), pozwala na kompleksowÄ… analizÄ™ kompromisu miÄ™dzy czuÅ‚oÅ›ciÄ… a specyficznoÅ›ciÄ….

---

**Tabela wynikÃ³w programu dla rÃ³Å¼nych wartoÅ›ci threshold**
| Threshold | Accuracy | Czas budowy LSH | Czas klasyfikacji | Poprawny spam | FaÅ‚szywe negatywy | FaÅ‚szywe pozytywy | Poprawny ham |
|-----------|----------|-----------------|-------------------|---------------|-------------------|-------------------|-------------|
| **0.1** | **94.50%** | 9.47s | 129.05s | **60.55%** | **5.32%** | 0.18% | 33.94% |
| **0.3** | 88.80% | 6.92s | 120.82s | 54.76% | 11.12% | **0.09%** | 34.04% |
| **0.5** | 79.14% | 4.78s | 118.51s | 45.09% | 20.79% | 0.07% | 34.06% |
| **0.7** | 70.72% | 3.11s | 116.56s | 36.65% | 29.23% | 0.05% | 34.07% |
| **0.9** | 62.70% | **1.53s** | **115.94s** | 28.61% | 37.26% | **0.04%** | **34.08%** |

**Optymalizacja parametru threshold**
Analiza wynikÃ³w pokazuje, Å¼e **Threshold = 0.1** osiÄ…ga najlepszÄ… dokÅ‚adnoÅ›Ä‡ (94.50%), co wskazuje na optymalny kompromis miÄ™dzy czuÅ‚oÅ›ciÄ… a specyficznoÅ›ciÄ…. NiÅ¼sze wartoÅ›ci threshold zwiÄ™kszajÄ… liczbÄ™ dopasowaÅ„, poprawiajÄ…c wykrywanie spamu kosztem niewielkiego wzrostu faÅ‚szywych pozytywÃ³w.

**WydajnoÅ›Ä‡ czasowa**
- **Czas przygotowania MinHash**: 452.07s - jednorazowy koszt inicjalizacji
- **Czas budowy LSH**: Maleje liniowo z wzrostem threshold (9.47s â†’ 1.53s)
- **Czas klasyfikacji**: Stabilny na poziomie ~115-129s, niezaleÅ¼nie od threshold

---

**PorÃ³wnanie z metodÄ… blacklisty (Zadania 2-3)**
| Metryka | LSH (threshold=0.1) | Blacklista (ze stemizacjÄ…) | Poprawa |
|---------|---------------------|----------------------------|---------|
| **Accuracy** | **94.50%** | 61.83% | **+32.67%** |
| **Poprawny spam** | **60.55%** | 28.65% | **+31.90%** |
| **FaÅ‚szywe negatywy** | **5.32%** | 38.07% | **-32.75%** |
| **FaÅ‚szywe pozytywne** | 0.18% | **0.09%** | +0.09% |
| **Czas przetwarzania** | ~591s | 2465s | **-1874s** |

Ocena efektywnoÅ›ci algorytmu LSH w porÃ³wnaniu z metodÄ… blacklisty wykazaÅ‚a jego znacznÄ… przewagÄ™, wyraÅ¼ajÄ…cÄ… siÄ™ wzrostem dokÅ‚adnoÅ›ci o 32,67% przy jednoczesnym skrÃ³ceniu czasu przetwarzania.

Kluczowym czynnikiem wpÅ‚ywajÄ…cym na skutecznoÅ›Ä‡ metody jest odpowiedni dobÃ³r parametru threshold, od ktÃ³rego zaleÅ¼y kompromis miÄ™dzy czuÅ‚oÅ›ciÄ… a specyficznoÅ›ciÄ… klasyfikatora. Ponadto algorytm LSH wyrÃ³Å¼nia siÄ™ doskonaÅ‚Ä… skalowalnoÅ›ciÄ…, zapewniajÄ…c przewidywalne czasy przetwarzania nawet przy pracy na duÅ¼ych zbiorach danych. Warto podkreÅ›liÄ‡, Å¼e we wszystkich testowanych konfiguracjach utrzymaÅ‚ on bardzo niski poziom faÅ‚szywych trafieÅ„, gdzie bÅ‚Ä™dne oznaczanie prawidÅ‚owych wiadomoÅ›ci jako spam nie przekroczyÅ‚o 0,2%.

W kontekÅ›cie praktycznych zastosowaÅ„, dla systemÃ³w produkcyjnych rekomendowane jest ustawienie threshold na poziomie 0,1, co gwarantuje wysokÄ… skutecznoÅ›Ä‡ wykrywania spamu przy zachowaniu akceptowalnego odsetka faÅ‚szywych alarmÃ³w.

Metoda LSH z wykorzystaniem MinHash okazaÅ‚a siÄ™ zdecydowanie bardziej efektywna niÅ¼ prosta blacklista sÅ‚Ã³w kluczowych, stanowiÄ…c profesjonalne i gotowe do wdroÅ¼enia rozwiÄ…zanie do klasyfikacji wiadomoÅ›ci email na skalÄ™ przemysÅ‚owÄ….

### Zadanie 5
DokonaÄ‡ klasyfikacji binarnej wiadomoÅ›ci z archiwum (zadanie 1) na spam i ham, stosujÄ…c algorytm Naive Bayes.

**Uwagi:**
1. Do realizacji zadania naleÅ¼y uÅ¼yÄ‡ implementacji algorytmu z biblioteki Scikit-learn. Algorytm dostÄ™pny jest poprzez obiekt MultinomialNB.
2. PorÃ³wnaÄ‡ dziaÅ‚anie algorytmu dla przypadkÃ³w:
   - algorytm pracuje na caÅ‚ych tematach i ciele wiadomoÅ›ci w postaci zwykÅ‚ego tekstu bez usuwania sÅ‚Ã³w przestankowych i stemizacji przy pomocy narzÄ™dzi z biblioteki NLTK.
   - algorytm pracuje na bazie stemizowanych danych z usuniÄ™tymi sÅ‚owami przestankowymi.
1. Uzyskane wyniki przedstawiÄ‡ przy pomocy macierzy konfuzji i wskaÅºnika accuracy.
2. PorÃ³wnaÄ‡ uzyskane wyniki do wynikÃ³w uzyskanych przy zastosowaniu metod z poprzednich zadaÅ„.

#### Implementacja

**1. Konfiguracja globalna**

Na wstÄ™pie programu znajduje siÄ™ kod, ktÃ³ry definiuje staÅ‚e konfiguracyjne uÅ¼ywane w caÅ‚ym programie. UÅ‚atwia to dostosowanie parametrÃ³w bez koniecznoÅ›ci modyfikowania logiki programu.

**Kod:**  
``` python
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_naive_bayes.txt"# nazwa pliku wynikowego

random.seed(42)                         # ustawienie ziarna losowoÅ›ci
```

**2. Funkcja `load_index`**

**WejÅ›cie:**  
- `index_path` (string) - Å›cieÅ¼ka do pliku z indeksem wiadomoÅ›ci

**WyjÅ›cie:**  
- `entries` (list) - lista krotek zawierajÄ…cych peÅ‚nÄ… Å›cieÅ¼kÄ™ do pliku i etykietÄ™ (spam/ham)

**Opis:**  
Funkcja wczytuje plik indeksu TREC07P, parsuje kaÅ¼dÄ… liniÄ™ rozdzielajÄ…c jÄ… na etykietÄ™ (spam/ham) i Å›cieÅ¼kÄ™ do pliku. Tworzy peÅ‚ne Å›cieÅ¼ki do plikÃ³w przez poÅ‚Ä…czenie Å›cieÅ¼ki bazowej DATA_PATH ze Å›cieÅ¼kÄ… z indeksu (po usuniÄ™ciu "../"). Zwraca listÄ™ wszystkich wpisÃ³w gotowych do przetwarzania.

**Kod:**  
``` python
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries
```

---

**3. Funkcja `load_email_content`**

**WejÅ›cie:**  
- `filepath` (string) - Å›cieÅ¼ka do pliku z wiadomoÅ›ciÄ… email

**WyjÅ›cie:**  
- `text` (string) - poÅ‚Ä…czony temat i treÅ›Ä‡ wiadomoÅ›ci lub pusty string w przypadku bÅ‚Ä™du

**Opis:**  
Funkcja wczytuje i parsuje wiadomoÅ›Ä‡ email, wyciÄ…gajÄ…c zarÃ³wno temat (Subject) jak i treÅ›Ä‡ wiadomoÅ›ci. ObsÅ‚uguje wiadomoÅ›ci wieloczÄ™Å›ciowe (multipart) - iteruje przez wszystkie czÄ™Å›ci i wyciÄ…ga tylko te o typie tekstowym. ÅÄ…czy temat z treÅ›ciÄ… w jeden string, co zapewnia, Å¼e algorytm Naive Bayes bÄ™dzie wykorzystywaÅ‚ caÅ‚Ä… dostÄ™pnÄ… informacjÄ™ tekstowÄ….

**Kod:**  
``` python
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            subject = msg.get("Subject", "")
            payload = ""
            if msg.is_multipart():
                parts = []
                for part in msg.walk():
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return subject + " " + payload
    except Exception:
        return ""
```

---

**4. Funkcja `preprocess_text`**

**WejÅ›cie:**  
- `text` (string) - tekst wiadomoÅ›ci email do przetworzenia

**WyjÅ›cie:**  
- `text` (string) - przetworzony tekst po stemizacji i usuniÄ™ciu stopwords

**Opis:**  
Funkcja przeprowadza peÅ‚ne przetwarzanie tekstu NLTK: konwersja na maÅ‚e litery, usuwanie znakÃ³w interpunkcyjnych, tokenizacja na pojedyncze sÅ‚owa, filtrowanie tylko sÅ‚Ã³w alfabetycznych, usuwanie stopwords (sÅ‚Ã³w bez znaczenia) oraz stemizacja przy uÅ¼yciu algorytmu PorterStemmer. Na koÅ„cu Å‚Ä…czy tokeny z powrotem w string dla kompatybilnoÅ›ci z CountVectorizer.

**Kod:**  
``` python
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(t) for t in tokens]
    return " ".join(tokens)
```

---

**5. Funkcja `prepare_data`**

**WejÅ›cie:**  
- `entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) do przetworzenia
- `use_preprocessing` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ przetwarzanie NLTK

**WyjÅ›cie:**  
- `texts` (list) - lista tekstÃ³w wiadomoÅ›ci (przetworzonych lub nie)
- `labels` (list) - lista etykiet (spam/ham)

**Opis:**  
Funkcja przetwarza wszystkie dokumenty z podanej listy. Dla kaÅ¼dego dokumentu wczytuje treÅ›Ä‡ emaila i opcjonalnie stosuje preprocessing NLTK w zaleÅ¼noÅ›ci od parametru `use_preprocessing`. Zwraca dwie listy: tekstÃ³w przygotowanych do wektoryzacji oraz odpowiadajÄ…cych im etykiet.

**Kod:**  
``` python
def prepare_data(entries, use_preprocessing=False):
    texts, labels = [], []
    for path, label in entries:
        text = load_email_content(path)
        if use_preprocessing:
            text = preprocess_text(text)
        texts.append(text)
        labels.append(label)
    return texts, labels
```

---

**6. Funkcja `run_naive_bayes`**

**WejÅ›cie:**  
- `train_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych treningowych
- `test_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych testowych
- `use_preprocessing` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ przetwarzanie NLTK

**WyjÅ›cie:**  
- `acc` (float) - dokÅ‚adnoÅ›Ä‡ klasyfikacji w procentach
- `cm_percent` (numpy.ndarray) - macierz konfuzji w procentach
- `elapsed` (float) - czas wykonania w sekundach

**Opis:**  
Funkcja przeprowadza peÅ‚ny eksperyment z klasyfikatorem Naive Bayes: przygotowuje dane treningowe i testowe, tworzy macierz cech przy uÅ¼yciu CountVectorizer (bag-of-words), trenuje model MultinomialNB, dokonuje predykcji na danych testowych i oblicza metryki wydajnoÅ›ci. WyÅ›wietla szczegÃ³Å‚owe wyniki w konsoli i zwraca wartoÅ›ci do dalszej analizy.

**Kod:**  
``` python
def run_naive_bayes(train_entries, test_entries, use_preprocessing=False):
    print(f"\nğŸ§  Uruchamianie Naive Bayes ({'z preprocessingiem' if use_preprocessing else 'Bez preprocessingu'})...")
    start_time = time.time()

    # Przygotowanie danych
    X_train_texts, y_train = prepare_data(train_entries, use_preprocessing)
    X_test_texts, y_test = prepare_data(test_entries, use_preprocessing)

    # Konwersja do macierzy cech (bag of words)
    vectorizer = CountVectorizer()
    X_train = vectorizer.fit_transform(X_train_texts)
    X_test = vectorizer.transform(X_test_texts)

    # Trening
    model = MultinomialNB()
    model.fit(X_train, y_train)

    # Predykcja
    y_pred = model.predict(X_test)

    # Ewaluacja
    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_test, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_test, y_pred) * 100

    # WyÅ›wietlenie wynikÃ³w
    print(f"ğŸ¯ Accuracy: {acc:.2f}% | â± Czas wykonania: {elapsed:.2f}s")
    print("ğŸ“Š Confusion matrix (%):")
    print(f"      spam      ham")
    print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
    print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

    return acc, cm_percent, elapsed
```

---

**7. Funkcja `main`**

**WejÅ›cie:**  
- Brak parametrÃ³w wejÅ›ciowych

**WyjÅ›cie:**  
- Brak bezpoÅ›redniego wyjÅ›cia (funkcja wykonuje program i zapisuje wyniki do pliku)

**Opis:**  
GÅ‚Ã³wna funkcja programu koordynujÄ…ca eksperymenty z Naive Bayes: wczytuje i tasuje dane, dzieli na zbiory treningowe i testowe, przeprowadza dwa eksperymenty (bez przetwarzania tekstu i z peÅ‚nym przetwarzaniem NLTK), porÃ³wnuje wyniki pod wzglÄ™dem accuracy i macierzy konfuzji, oraz zapisuje szczegÃ³Å‚owe wyniki do pliku tekstowego. Eksperymenty pozwalajÄ… na porÃ³wnanie wpÅ‚ywu preprocessingu na skutecznoÅ›Ä‡ klasyfikacji.

**Kod:**  
``` python
def main():
    print("ğŸ“‚ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"âš ï¸ SAMPLE_SIZE aktywne. WykorzystujÄ™ {len(index_entries)} pierwszych wpisÃ³w")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]
    print(f"ÅÄ…cznie: {len(index_entries)} dokumentÃ³w; trening: {len(train_entries)}; test: {len(test_entries)}")

    # Wyniki
    results = []

    # Wersja bez preprocessingu (peÅ‚ny tekst)
    acc_raw, cm_raw, t_raw = run_naive_bayes(train_entries, test_entries, use_preprocessing=False)
    results.append(("Bez preprocessingu", acc_raw, cm_raw, t_raw))

    # Wersja z preprocessingiem (usuwanie stopwords i stemizacja)
    acc_clean, cm_clean, t_clean = run_naive_bayes(train_entries, test_entries, use_preprocessing=True)
    results.append(("Z preprocessingiem (NLTK)", acc_clean, cm_clean, t_clean))

    # Zapis wynikÃ³w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("Naive Bayes Results\n\n")
        for title, acc, cm, t in results:
            f.write(f"{title}\n")
            f.write(f"Accuracy: {acc:.2f}%\nCzas: {t:.2f}s\n")
            f.write("Confusion matrix (%):\n")
            f.write(f"spam_spam={cm[0,0]:.2f}% spam_ham={cm[0,1]:.2f}%\n")
            f.write(f"ham_spam={cm[1,0]:.2f}% ham_ham={cm[1,1]:.2f}%\n\n")

    print(f"\nğŸ“ Wyniki zapisano do: {RESULTS_FILE}")
```

---

**8. Kompletny kod**  
PoniÅ¼ej znajduje siÄ™ kompletny kod programu, ktÃ³ry moÅ¼na uruchomiÄ‡.

**Kod:**  
``` python
import os
import string
import random
import time
from email import message_from_file

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, accuracy_score
import numpy as np

# === KONFIGURACJA ===
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_naive_bayes.txt"# nazwa pliku wynikowego

random.seed(42)                         # ustawienie ziarna losowoÅ›ci


# === POMOCNICZE FUNKCJE ===
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries

# Wczytuje treÅ›Ä‡ e-maila (temat + ciaÅ‚o) jako zwykÅ‚y tekst
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            subject = msg.get("Subject", "")
            payload = ""
            if msg.is_multipart():
                parts = []
                for part in msg.walk():
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return subject + " " + payload
    except Exception:
        return ""

# Usuwa interpunkcjÄ™, stopwords i dokonuje stemizacji
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(t) for t in tokens]
    return " ".join(tokens)

# Zwraca listÄ™ tekstÃ³w i etykiet (spam/ham), z opcjonalnym preprocessingiem.
def prepare_data(entries, use_preprocessing=False):
    texts, labels = [], []
    for path, label in entries:
        text = load_email_content(path)
        if use_preprocessing:
            text = preprocess_text(text)
        texts.append(text)
        labels.append(label)
    return texts, labels


# === FUNKCJA EKSPERYMENTU ===
#  Trenuje i testuje klasyfikator MultinomialNB dla zbioru TREC07P. Zwraca accuracy, macierz konfuzji i czas wykonania.
def run_naive_bayes(train_entries, test_entries, use_preprocessing=False):
    print(f"\nğŸ§  Uruchamianie Naive Bayes ({'z preprocessingiem' if use_preprocessing else 'Bez preprocessingu'})...")
    start_time = time.time()

    # Przygotowanie danych
    X_train_texts, y_train = prepare_data(train_entries, use_preprocessing)
    X_test_texts, y_test = prepare_data(test_entries, use_preprocessing)

    # Konwersja do macierzy cech (bag of words)
    vectorizer = CountVectorizer()
    X_train = vectorizer.fit_transform(X_train_texts)
    X_test = vectorizer.transform(X_test_texts)

    # Trening
    model = MultinomialNB()
    model.fit(X_train, y_train)

    # Predykcja
    y_pred = model.predict(X_test)

    # Ewaluacja
    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_test, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_test, y_pred) * 100

    # WyÅ›wietlenie wynikÃ³w
    print(f"ğŸ¯ Accuracy: {acc:.2f}% | â± Czas wykonania: {elapsed:.2f}s")
    print("ğŸ“Š Confusion matrix (%):")
    print(f"      spam      ham")
    print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
    print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

    return acc, cm_percent, elapsed


# === GÅÃ“WNY PROGRAM ===
def main():
    print("ğŸ“‚ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"âš ï¸ SAMPLE_SIZE aktywne. WykorzystujÄ™ {len(index_entries)} pierwszych wpisÃ³w")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]
    print(f"ÅÄ…cznie: {len(index_entries)} dokumentÃ³w; trening: {len(train_entries)}; test: {len(test_entries)}")

    # Wyniki
    results = []

    # Wersja bez preprocessingu (peÅ‚ny tekst)
    acc_raw, cm_raw, t_raw = run_naive_bayes(train_entries, test_entries, use_preprocessing=False)
    results.append(("Bez preprocessingu", acc_raw, cm_raw, t_raw))

    # Wersja z preprocessingiem (usuwanie stopwords i stemizacja)
    acc_clean, cm_clean, t_clean = run_naive_bayes(train_entries, test_entries, use_preprocessing=True)
    results.append(("Z preprocessingiem (NLTK)", acc_clean, cm_clean, t_clean))

    # Zapis wynikÃ³w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("Naive Bayes Results\n\n")
        for title, acc, cm, t in results:
            f.write(f"{title}\n")
            f.write(f"Accuracy: {acc:.2f}%\nCzas: {t:.2f}s\n")
            f.write("Confusion matrix (%):\n")
            f.write(f"spam_spam={cm[0,0]:.2f}% spam_ham={cm[0,1]:.2f}%\n")
            f.write(f"ham_spam={cm[1,0]:.2f}% ham_ham={cm[1,1]:.2f}%\n\n")

    print(f"\nğŸ“ Wyniki zapisano do: {RESULTS_FILE}")

if __name__ == "__main__":
    main()
```


#### Wyniki

``` text
ğŸ“‚ Wczytywanie danych...
ÅÄ…cznie: 75419 dokumentÃ³w; trening: 60335; test: 15084

ğŸ§  Uruchamianie Naive Bayes (Bez preprocessingu)...
ğŸ¯ Accuracy: 99.24% | â± Czas wykonania: 69.66s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  65.46%    0.42%
ham   0.34%     33.78%

ğŸ§  Uruchamianie Naive Bayes (z preprocessingiem)...
ğŸ¯ Accuracy: 98.72% | â± Czas wykonania: 381.12s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  64.85%    1.03%
ham   0.25%     33.87%

ğŸ“ Wyniki zapisano do: results_naive_bayes.txt
```

#### Wnioski

W zadaniu 5 zastosowano klasyfikator Naive Bayes (MultinomialNB) do binarnej klasyfikacji wiadomoÅ›ci email na spam i ham. Przeprowadzono dwa eksperymenty: jeden bez preprocessingu tekstu, a drugi z peÅ‚nym przetwarzaniem NLTK (usuwanie stopwords i stemizacja).

**Opis algorytmu**  
Klasyfikator Naive Bayes to probabilistyczny algorytm oparty na twierdzeniu Bayesa, ktÃ³ry zakÅ‚ada niezaleÅ¼noÅ›Ä‡ cech (sÅ‚Ã³w) przy danej etykiecie. Algorytm dziaÅ‚a w nastÄ™pujÄ…cych etapach:
1. **Ekstrakcja cech**: PrzeksztaÅ‚cenie tekstu na reprezentacjÄ™ numerycznÄ… (Bag-of-Words)
2. **Trening**: Obliczenie prawdopodobieÅ„stw warunkowych dla kaÅ¼dego sÅ‚owa w kontekÅ›cie klas spam/ham
3. **Klasyfikacja**: Obliczenie prawdopodobieÅ„stwa posterior dla nowej wiadomoÅ›ci i przypisanie do klasy o wyÅ¼szym prawdopodobieÅ„stwie

**Zalety metody**
- **Wysoka skutecznoÅ›Ä‡** - doskonaÅ‚e wyniki w klasyfikacji tekstu
- **SzybkoÅ›Ä‡ treningu** - efektywne obliczenia probabilistyczne
- **SkalowalnoÅ›Ä‡** - dobre dziaÅ‚anie na duÅ¼ych zbiorach danych
- **OdpornoÅ›Ä‡ na szum** - stabilnoÅ›Ä‡ wobec czÄ™Å›ciowo nieistotnych cech

**Wady metody**
- **ZaÅ‚oÅ¼enie niezaleÅ¼noÅ›ci** - nierealistyczne zaÅ‚oÅ¼enie o niezaleÅ¼noÅ›ci sÅ‚Ã³w
- **WraÅ¼liwoÅ›Ä‡ na rzadkie sÅ‚owa** - problem z wyrazami nieobecnymi w zbiorze treningowym
- **ZaleÅ¼noÅ›Ä‡ od preprocessingu** - wyniki mogÄ… siÄ™ rÃ³Å¼niÄ‡ w zaleÅ¼noÅ›ci od przygotowania danych

---

**Konfiguracja programu**  
Podobnie jak poprzednio, na wstÄ™pie naleÅ¼aÅ‚o zdefiniowaÄ‡ staÅ‚e konfiguracyjne. Konfiguracja programu zostaÅ‚a zaprojektowana w celu porÃ³wnania wpÅ‚ywu preprocessingu tekstu na skutecznoÅ›Ä‡ algorytmu, dlatego eksperyment obejmuje dwa scenariusze: pracÄ™ na surowym tekÅ›cie oraz na danych po peÅ‚nym przetworzeniu NLTK. NajwaÅ¼niejsze parametry to:
- **`TRAIN_RATIO = 0.8`** - Standardowy podziaÅ‚ 80/20, ktÃ³ry jest powszechnie stosowany w uczeniu maszynowym, zapewniajÄ…c wystarczajÄ…cÄ… iloÅ›Ä‡ danych do treningu (60,335 wiadomoÅ›ci) przy zachowaniu reprezentatywnego zbioru testowego (15,084 wiadomoÅ›ci).
- **`SAMPLE_SIZE = None`** - UÅ¼ycie caÅ‚ego zbioru danych zapewnia wiarygodnoÅ›Ä‡ wynikÃ³w, jednak parametr umoÅ¼liwia szybkie testy na mniejszych prÃ³bkach podczas rozwoju algorytmu.
- **`random.seed(42)`** - Gwarancja powtarzalnoÅ›ci eksperymentÃ³w poprzez ustalenie ziarna losowoÅ›ci.

---

**Tabela porÃ³wnawcza wynikÃ³w:**
| Metryka | Bez preprocessingu | Z preprocessingiem | RÃ³Å¼nica | Wnioski |
|---------|-------------------|-------------------|---------|---------|
| **Accuracy** | **99.24%** | 98.72% | **-0.52%** | Preprocessing nieznacznie obniÅ¼a dokÅ‚adnoÅ›Ä‡ |
| **Czas wykonania** | **69.66s** | 381.12s | **+311.46s** | Preprocessing znaczÄ…co wydÅ‚uÅ¼a czas |
| **Poprawny spam** | **65.46%** | 64.85% | **-0.61%** | Nieznacznie lepsze bez preprocessingu |
| **FaÅ‚szywe negatywy** | **0.42%** | 1.03% | **+0.61%** | WiÄ™cej spamu przechodzi z preprocessingiem |
| **FaÅ‚szywe pozytywne** | 0.34% | **0.25%** | **-0.09%** | Preprocessing redukuje bÅ‚Ä™dy hamâ†’spam |
| **Poprawny ham** | 33.78% | **33.87%** | **+0.09%** | Nieznacznie lepsze z preprocessingiem |

**Wersja bez preprocessingu osiÄ…ga nieznacznie lepszÄ… dokÅ‚adnoÅ›Ä‡ (99.24% vs 98.72%)**, co jest niespodziewanym wynikiem, poniewaÅ¼ preprocessing teoretycznie powinien poprawiaÄ‡ jakoÅ›Ä‡ cech. Sugeruje to, Å¼e niektÃ³re sÅ‚owa pomocnicze mogÄ… byÄ‡ charakterystyczne dla spamu i ich usuniÄ™cie obniÅ¼a skutecznoÅ›Ä‡ klasyfikacji. Dodatkowo moÅ¼na zaÅ‚oÅ¼yÄ‡, Å¼e znaki specjalne mogÄ… byÄ‡ istotnymi wskaÅºnikami spamu (np. `!!!`, `$$$$`), a ich usuniÄ™cie w preprocessingie prowadzi do utraty informacji. Co wiÄ™cej, sprowadzenie sÅ‚Ã³w do ich podstawowych form (stemizacja) moÅ¼e usuwaÄ‡ subtelne rÃ³Å¼nice miÄ™dzy wyrazami, ktÃ³re sÄ… istotne dla klasyfikacji. 

**Preprocessing zwiÄ™ksza czas wykonania ponad 5-krotnie** (69.66s â†’ 381.12s), co wynika z dodatkowych operacji lingwistycznych na kaÅ¼dym dokumencie.

W rezultacie moÅ¼emy przyjÄ…Ä‡ poniÅ¼sze podejÅ›cie w zaleÅ¼noÅ›ci od priorytetÃ³w systemu:
- **Bez preprocessingu**: Lepsze wykrywanie spamu, ale wiÄ™cej faÅ‚szywych pozytywnych i szybsze dziaÅ‚anie
- **Z preprocessingiem**: Gorsze wykrywanie spamu, ale mniej faÅ‚szywych pozytywnych kosztem czasu

---

**PorÃ³wnanie z metodami z poprzednich zadaÅ„**
| Metoda | Najlepsza accuracy | Czas przetwarzania | Zalety | Wady |
|--------|-------------------|-------------------|--------|------|
| **Blacklista** | 61.83% | 2465s | Prosta, interpretowalna | Niska skutecznoÅ›Ä‡ |
| **LSH (threshold=0.1)** | 94.50% | 591s | Skalowalna, dobre podobieÅ„stwa | ZaleÅ¼na od parametrÃ³w |
| **Naive Bayes** | **99.24%** | **70s** | **NajwyÅ¼sza dokÅ‚adnoÅ›Ä‡**, szybki | ZaÅ‚oÅ¼enie niezaleÅ¼noÅ›ci |

Metoda Naive Bayes wyraÅºnie dominuje nad pozostaÅ‚ymi testowanymi rozwiÄ…zaniami, osiÄ…gajÄ…c najwyÅ¼szÄ… skutecznoÅ›Ä‡ klasyfikacji na poziomie 99,24%. Wynik ten znaczÄ…co przewyÅ¼sza zarÃ³wno efektywnoÅ›Ä‡ metody opartej na LSH, jak i klasycznej blacklisty â€“ odpowiednio o 4,74% oraz aÅ¼ o 37,41%. Co waÅ¼ne, analiza pokazaÅ‚a, Å¼e stosowanie preprocessingu nie zawsze przekÅ‚ada siÄ™ na poprawÄ™ jakoÅ›ci klasyfikacji. W przypadku Naive Bayes prostsze podejÅ›cie, pozbawione dodatkowego czyszczenia danych, okazaÅ‚o siÄ™ nie tylko skuteczniejsze, ale takÅ¼e szybsze. Niewielki spadek dokÅ‚adnoÅ›ci przy uÅ¼yciu preprocessingu moÅ¼e wynikaÄ‡ z utraty pewnych istotnych informacji, takich jak stopwords czy interpunkcja, ktÃ³re â€“ choÄ‡ czÄ™sto traktowane jako szum â€“ mogÄ… w niektÃ³rych przypadkach nieÅ›Ä‡ wartoÅ›ciowe wskazÃ³wki dla klasyfikatora.

W praktyce sugeruje siÄ™ jednak korzystanie z wersji metody Naive Bayes bez preprocessingu, szczegÃ³lnie w Å›rodowiskach produkcyjnych, gdzie kluczowe sÄ… zarÃ³wno wysoka dokÅ‚adnoÅ›Ä‡, jak i krÃ³tki czas przetwarzania. Wyniki badaÅ„ potwierdzajÄ… znacznÄ… przewagÄ™ tego algorytmu nad wczeÅ›niej stosowanymi metodami, co czyni go optymalnym wyborem w zadaniach zwiÄ…zanych z filtrowaniem spamu i innymi formami klasyfikacji tekstu.

### Zadanie 6
DokonaÄ‡ klasyfikacji binarnej wiadomoÅ›ci z archiwum (zadanie 1) na spam i ham, stosujÄ…c model gÄ™sto Å‚Ä…czonej gÅ‚Ä™bokiej sieci neuronowej i technikÄ™ uczenia nadzorowanego.
**Uwagi:**
1. ZaproponowaÄ‡ sposÃ³b translacji danych wejÅ›ciowych do postaci akceptowanego przez sieÄ‡ tensora wejÅ›ciowego.
2. ZaproponowaÄ‡ liczbÄ™ warstw ukrytych oraz liczbÄ™ wÄ™zÅ‚Ã³w w poszczegÃ³lnych warstwach.
3. ZaproponowaÄ‡ funkcje aktywacji dla wÄ™zÅ‚Ã³w w warstwach ukrytych oraz w warstwie wyjÅ›ciowej.
4. ZaproponowaÄ‡ metrykÄ™ dokÅ‚adnoÅ›ci.
5. ZaproponowaÄ‡ optymalizator.
6. Do realizacji zadania zastosowaÄ‡ narzÄ™dzia z biblioteki TensorFLow.
7. W wyniku realizacji zadania wygenerowaÄ‡ macierz konfuzji oraz wartoÅ›Ä‡ wskaÅºnika accuracy.
8. PorÃ³wnaÄ‡ uzyskane wyniki dla rÃ³Å¼nych modeli (to znaczy: iloÅ›ci warstw ukrytych, iloÅ›ci wÄ™zÅ‚Ã³w w warstwach, funkcji aktywacji).
9. PorÃ³wnaÄ‡ uzyskane wyniki z wynikami uzyskanym w ramach realizacji poprzednich zadaÅ„.

#### Implementacja

**1. Konfiguracja globalna**
Na wstÄ™pie programu znajduje siÄ™ kod, ktÃ³ry definiuje staÅ‚e konfiguracyjne uÅ¼ywane w caÅ‚ym programie. UÅ‚atwia to dostosowanie parametrÃ³w bez koniecznoÅ›ci modyfikowania logiki programu.

**Kod:**  
``` python
INDEX_PATH = "trec07p/full/index"   # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"               # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                   # stosunek danych treningowych do testowych
SAMPLE_SIZE = None                  # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
MAX_FEATURES = 20000                # rozmiar wektora TF-IDF (zmniejsz do 5000 jeÅ›li brakuje pamiÄ™ci)
SAMPLE_SEED = 42                    # ustawienie ziarna losowoÅ›ci

EPOCHS = 5                          # liczba epok treningu 
BATCH_SIZE = 128                    # rozmiar batcha
RESULTS_FILE = "results_dnn.txt"    # nazwa pliku wynikowego

USE_PREPROCESSING = True            # Czy uÅ¼yÄ‡ preprocessingu NLTK (stopwords + stemming) przed TF-IDF

# Modele do przetestowania: lista dictÃ³w (nazwa, architektura, activation_hidden)
MODEL_CONFIGS = [
    {"name": "small", "layers": [64], "activation": "relu"},
    {"name": "medium", "layers": [128, 64], "activation": "relu"},
    {"name": "large", "layers": [256, 128, 64], "activation": "relu"},
    {"name": "small_tanh", "layers": [64], "activation": "tanh"},
]

# Ustawienie ziarna losowoÅ›ci dla powtarzalnoÅ›ci
random.seed(SAMPLE_SEED)
np.random.seed(SAMPLE_SEED)
tf.random.set_seed(SAMPLE_SEED)
```

---

**2. Funkcja `load_index`**

**WejÅ›cie:**  
- `index_path` (string) - Å›cieÅ¼ka do pliku z indeksem wiadomoÅ›ci

**WyjÅ›cie:**  
- `entries` (list) - lista krotek zawierajÄ…cych peÅ‚nÄ… Å›cieÅ¼kÄ™ do pliku i etykietÄ™ (spam/ham)

**Opis:**  
Funkcja wczytuje plik indeksu TREC07P, parsuje kaÅ¼dÄ… liniÄ™ rozdzielajÄ…c jÄ… na etykietÄ™ (spam/ham) i Å›cieÅ¼kÄ™ do pliku. Tworzy peÅ‚ne Å›cieÅ¼ki do plikÃ³w przez poÅ‚Ä…czenie Å›cieÅ¼ki bazowej DATA_PATH ze Å›cieÅ¼kÄ… z indeksu (po usuniÄ™ciu "../"). Zwraca listÄ™ wszystkich wpisÃ³w gotowych do przetwarzania.

**Kod:**  
``` python
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries
```

---

**3. Funkcja `load_email_content`**

**WejÅ›cie:**  
- `filepath` (string) - Å›cieÅ¼ka do pliku z wiadomoÅ›ciÄ… email

**WyjÅ›cie:**  
- `text` (string) - poÅ‚Ä…czony temat i treÅ›Ä‡ wiadomoÅ›ci lub pusty string w przypadku bÅ‚Ä™du

**Opis:**  
Funkcja wczytuje i parsuje wiadomoÅ›Ä‡ email, wyciÄ…gajÄ…c zarÃ³wno temat (Subject) jak i treÅ›Ä‡ wiadomoÅ›ci. ObsÅ‚uguje wiadomoÅ›ci wieloczÄ™Å›ciowe (multipart) - iteruje przez wszystkie czÄ™Å›ci i wyciÄ…ga tylko te o typie tekstowym. ÅÄ…czy temat z treÅ›ciÄ… w jeden string. Dekoduje zawartoÅ›Ä‡ binarnÄ… i obsÅ‚uguje bÅ‚Ä™dy kodowania przy uÅ¼yciu kodowania latin-1.

**Kod:**  
``` python
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            subject = msg.get("Subject", "") or ""
            payload = ""
            if msg.is_multipart():
                parts = []
                for part in msg.walk():
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return (subject + " " + payload).strip()
    except Exception:
        return ""
```

---

**4. Funkcja `preprocess_text`**

**WejÅ›cie:**  
- `text` (string) - tekst wiadomoÅ›ci email do przetworzenia

**WyjÅ›cie:**  
- `text` (string) - przetworzony tekst po stemizacji i usuniÄ™ciu stopwords

**Opis:**  
Funkcja przeprowadza peÅ‚ne przetwarzanie tekstu NLTK: konwersja na maÅ‚e litery, usuwanie znakÃ³w interpunkcyjnych, tokenizacja na pojedyncze sÅ‚owa, filtrowanie tylko sÅ‚Ã³w alfabetycznych, usuwanie stopwords (sÅ‚Ã³w bez znaczenia) oraz stemizacja przy uÅ¼yciu algorytmu PorterStemmer. Na koÅ„cu Å‚Ä…czy tokeny z powrotem w string dla kompatybilnoÅ›ci z TF-IDF Vectorizer.

**Kod:**  
``` python
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(t) for t in tokens]
    return " ".join(tokens)
```

---

**5. Funkcja `prepare_corpus`**

**WejÅ›cie:**  
- `entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) do przetworzenia
- `use_preprocessing` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ przetwarzanie NLTK
- `sample_size` (int) - ograniczenie liczby przetwarzanych dokumentÃ³w

**WyjÅ›cie:**  
- `texts` (list) - lista tekstÃ³w wiadomoÅ›ci (przetworzonych lub nie)
- `labels` (numpy.ndarray) - tablica etykiet numerycznych (spam=1, ham=0)

**Opis:**  
Funkcja przetwarza wszystkie dokumenty z podanej listy. Dla kaÅ¼dego dokumentu wczytuje treÅ›Ä‡ emaila i opcjonalnie stosuje preprocessing NLTK. Konwertuje etykiety tekstowe na numeryczne (spam=1, ham=0) dla kompatybilnoÅ›ci z TensorFlow. Zwraca listÄ™ tekstÃ³w i tablicÄ™ etykiet numerycznych.

**Kod:**  
``` python
def prepare_corpus(entries, use_preprocessing=True, sample_size=None):
    texts = []
    labels = []
    count = 0
    for path, label in entries:
        if sample_size and count >= sample_size:
            break
        txt = load_email_content(path)
        if use_preprocessing:
            txt = preprocess_text(txt)
        texts.append(txt)
        labels.append(1 if label == "spam" else 0)  # spam=1, ham=0
        count += 1
    return texts, np.array(labels)
```

---

**6. Funkcja `build_vectorizer`**

**WejÅ›cie:**  
- `texts` (list) - lista tekstÃ³w do wektoryzacji
- `max_features` (int) - maksymalna liczba cech w wektorze TF-IDF

**WyjÅ›cie:**  
- `vec` (TfidfVectorizer) - wytrenowany obiekt vectorizera
- `X` (scipy.sparse matrix) - macierz cech w formacie TF-IDF

**Opis:**  
Funkcja tworzy i trenuje vectorizer TF-IDF na podanych tekstach. UÅ¼ywa zakresu n-gramÃ³w (1,2), co oznacza, Å¼e uwzglÄ™dnia zarÃ³wno pojedyncze sÅ‚owa jak i pary kolejnych sÅ‚Ã³w. Ogranicza liczbÄ™ cech do `max_features` w celu kontroli wymiarowoÅ›ci danych. Zwraca wytrenowany vectorizer i przeksztaÅ‚conÄ… macierz cech.

**Kod:**  
``` python
def build_vectorizer(texts, max_features=20000):
    vec = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))
    X = vec.fit_transform(texts)
    return vec, X
```

---

**7. Funkcja `build_model`**

**WejÅ›cie:**  
- `input_dim` (int) - wymiarowoÅ›Ä‡ danych wejÅ›ciowych
- `layer_sizes` (list) - lista okreÅ›lajÄ…ca liczbÄ™ neuronÃ³w w kolejnych warstwach
- `activation_hidden` (string) - funkcja aktywacji dla warstw ukrytych
- `dropout` (float) - wspÃ³Å‚czynnik dropout dla regularyzacji
- `lr` (float) - learning rate dla optymalizatora

**WyjÅ›cie:**  
- `model` (Sequential) - skompilowany model sieci neuronowej

**Opis:**  
Funkcja buduje sekwencyjny model DNN zgodnie z podanÄ… architekturÄ…. Tworzy warstwy gÄ™ste z okreÅ›lonÄ… liczbÄ… neuronÃ³w i funkcjami aktywacji. Po kaÅ¼dej warstwie dodaje warstwÄ™ Dropout dla zapobiegania przeuczeniu. Ostatnia warstwa uÅ¼ywa funkcji sigmoid dla klasyfikacji binarnej. Kompiluje model z optymalizatorem Adam, funkcjÄ… straty binary_crossentropy i metrykÄ… accuracy.

**Kod:**  
``` python
def build_model(input_dim, layer_sizes, activation_hidden="relu", dropout=0.2, lr=1e-3):
    model = Sequential()
    # Warstwa wejÅ›ciowa jest czÄ™Å›ciÄ… pierwszej warstwy ukrytej
    for i, size in enumerate(layer_sizes):
        if i == 0:
            model.add(Dense(size, activation=activation_hidden, input_shape=(input_dim,)))
        else:
            model.add(Dense(size, activation=activation_hidden))
        model.add(Dropout(dropout))
    # Warstwa wyjÅ›ciowa - sigmoid dla binarnej klasyfikacji
    model.add(Dense(1, activation="sigmoid"))
    model.compile(optimizer=Adam(learning_rate=lr),
                  loss="binary_crossentropy",
                  metrics=["accuracy"])
    return model
```

---

**8. Funkcja `main`**

**WejÅ›cie:**  
- Brak parametrÃ³w wejÅ›ciowych

**WyjÅ›cie:**  
- Brak bezpoÅ›redniego wyjÅ›cia (funkcja wykonuje program i zapisuje wyniki do pliku)

**Opis:**  
GÅ‚Ã³wna funkcja programu koordynujÄ…ca caÅ‚y proces: wczytuje i tasuje dane, przygotowuje korpus tekstowy, tworzy wektory TF-IDF, testuje rÃ³Å¼ne konfiguracje modeli DNN, trenuje modele, dokonuje predykcji, oblicza metryki wydajnoÅ›ci i zapisuje szczegÃ³Å‚owe wyniki do pliku. Dla kaÅ¼dej konfiguracji modelu z listy MODEL_CONFIGS przeprowadza peÅ‚ny cykl treningu i ewaluacji.

**Kod:**  
``` python
def main():
    print("ğŸ“‚ Wczytywanie danych...")
    entries = load_index(INDEX_PATH)
    random.shuffle(entries)

    if SAMPLE_SIZE:
        use_entries = entries[:SAMPLE_SIZE]
        print(f"âš ï¸ SAMPLE_SIZE aktywne. WykorzystujÄ™ {len(use_entries)} pierwszych wpisÃ³w.")
    else:
        use_entries = entries

    # Przygotowanie tekstÃ³w i etykiet
    print("ğŸ§¾ Przygotowanie korpusu tekstÃ³w (preprocessing = %s)..." % USE_PREPROCESSING)
    texts, labels = prepare_corpus(use_entries, use_preprocessing=USE_PREPROCESSING, sample_size=None)
    print(f"Przygotowano {len(texts)} dokumentÃ³w.")

    # PodziaÅ‚ na trening/test (z zachowaniem TRAIN_RATIO)
    split_point = int(len(texts) * TRAIN_RATIO)
    X_texts_train = texts[:split_point]
    X_texts_test = texts[split_point:]
    y_train = labels[:split_point]
    y_test = labels[split_point:]
    print(f"Trening: {len(X_texts_train)}, Test: {len(X_texts_test)}")

    # Tworzenie wektorÃ³w TF-IDF
    print(f"ğŸ”¤ Tworzenie TF-IDF (max_features={MAX_FEATURES})...")
    vectorizer, X_train_sparse = build_vectorizer(X_texts_train, max_features=MAX_FEATURES)
    X_test_sparse = vectorizer.transform(X_texts_test)

    # Konwersja do dense (Keras wymaga gÄ™stych (Dense) macierzy)
    print("Konwersja do macierzy gÄ™stych...")
    X_train = X_train_sparse.toarray().astype(np.float32)
    X_test = X_test_sparse.toarray().astype(np.float32)
    input_dim = X_train.shape[1]
    print(f"Input dim = {input_dim}")

    results_lines = []
    results_lines.append(f"DNN TF-IDF results\nSAMPLE_SIZE={SAMPLE_SIZE}\nMAX_FEATURES={MAX_FEATURES}\nEPOCHS={EPOCHS}\nBATCH_SIZE={BATCH_SIZE}\nUSE_PREPROCESSING={USE_PREPROCESSING}\n\n")

    # Dla kaÅ¼dej konfiguracji modelu trenuje, testuje i zapisuje wyniki
    for cfg in MODEL_CONFIGS:
        name = cfg["name"]
        layers = cfg["layers"]
        activation = cfg.get("activation", "relu")
        print(f"\n=== Model: {name} | layers={layers} | activation={activation} ===")
        model = build_model(input_dim=input_dim, layer_sizes=layers, activation_hidden=activation)

        # Trening modelu
        t0 = time.time()
        history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)
        train_time = time.time() - t0
        print(f"Trening zakoÅ„czony w {train_time:.2f}s")

        # Predykcja na zbiorze testowym
        t1 = time.time()
        y_prob = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0).ravel()
        y_pred = (y_prob >= 0.5).astype(int)
        predict_time = time.time() - t1

        # Metryki ewaluacyjne
        acc = accuracy_score(y_test, y_pred) * 100.0
        labels_order = [1, 0]  # spam=1, ham=0
        cm = confusion_matrix(y_test, y_pred, labels=labels_order)
        cm_percent = cm / np.sum(cm) * 100.0

        # Wypisuje wyniki i zapisuje je do pliku
        print(f"ğŸ¯ Accuracy: {acc:.2f}% | Czas treningu: {train_time:.2f}s | Czas predykcji: {predict_time:.2f}s")
        print("ğŸ“Š Confusion matrix (%):")
        print("      spam      ham")
        print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
        print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

        results_lines.append(f"Model: {name}\n")
        results_lines.append(f"layers={layers} activation={activation}\n")
        results_lines.append(f"accuracy={acc:.2f}% train_time={train_time:.2f}s predict_time={predict_time:.2f}s\n")
        results_lines.append(f"confusion_percent:\nspam_spam={cm_percent[0,0]:6.2f}% spam_ham={cm_percent[0,1]:6.2f}%\n")
        results_lines.append(f"ham_spam={cm_percent[1,0]:6.2f}% ham_ham={cm_percent[1,1]:6.2f}%\n\n")

        # Zwolnij pamiÄ™Ä‡ modelu przed kolejnym testem
        tf.keras.backend.clear_session()

    # Zapis do pliku wynikÃ³w
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_lines))
    
    print(f"\nğŸ“ Wyniki zapisano do: {RESULTS_FILE}")
```

---

**9. Kompletny kod**  
PoniÅ¼ej znajduje siÄ™ kompletny kod programu, ktÃ³ry moÅ¼na uruchomiÄ‡.

**Kod:**  
``` python
import os
import string
import random
import time
from email import message_from_file

import numpy as np
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

# === KONFIGURACJA ===
INDEX_PATH = "trec07p/full/index"   # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"               # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                   # stosunek danych treningowych do testowych
SAMPLE_SIZE = None                  # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
MAX_FEATURES = 20000                # rozmiar wektora TF-IDF (zmniejsz do 5000 jeÅ›li brakuje pamiÄ™ci)
SAMPLE_SEED = 42                    # ustawienie ziarna losowoÅ›ci

EPOCHS = 5                          # liczba epok treningu 
BATCH_SIZE = 128                    # rozmiar batcha
RESULTS_FILE = "results_dnn.txt"    # nazwa pliku wynikowego

USE_PREPROCESSING = True            # Czy uÅ¼yÄ‡ preprocessingu NLTK (stopwords + stemming) przed TF-IDF

# Modele do przetestowania: lista dictÃ³w (nazwa, architektura, activation_hidden)
MODEL_CONFIGS = [
    {"name": "small", "layers": [64], "activation": "relu"},
    {"name": "medium", "layers": [128, 64], "activation": "relu"},
    {"name": "large", "layers": [256, 128, 64], "activation": "relu"},
    {"name": "small_tanh", "layers": [64], "activation": "tanh"},
]

# Ustawienie ziarna losowoÅ›ci dla powtarzalnoÅ›ci
random.seed(SAMPLE_SEED)
np.random.seed(SAMPLE_SEED)
tf.random.set_seed(SAMPLE_SEED)


# === POMOCNICZE FUNKCJE ===
# Wczytuje indeks plikÃ³w e-maili i ich etykiety (spam/ham)
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries

# Wczytuje treÅ›Ä‡ e-maila (temat + ciaÅ‚o) jako zwykÅ‚y tekst. Ignoruje bÅ‚Ä™dy kodowania.
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            subject = msg.get("Subject", "") or ""
            payload = ""
            if msg.is_multipart():
                parts = []
                for part in msg.walk():
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return (subject + " " + payload).strip()
    except Exception:
        return ""


# Usuwa interpunkcjÄ™, stopwords i dokonuje stemizacji
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(t) for t in tokens]
    return " ".join(tokens)


# Wczytuje teksty i etykiety, przeprowadza opcjonalny preprocessing, zwraca listÄ™ tekstÃ³w i tablicÄ™ etykiet (spam/ham)
def prepare_corpus(entries, use_preprocessing=True, sample_size=None):
    texts = []
    labels = []
    count = 0
    for path, label in entries:
        if sample_size and count >= sample_size:
            break
        txt = load_email_content(path)
        if use_preprocessing:
            txt = preprocess_text(txt)
        texts.append(txt)
        labels.append(1 if label == "spam" else 0)  # spam=1, ham=0
        count += 1
    return texts, np.array(labels)


# Tworzy i dopasowuje wektorizer TF-IDF, zwraca wektorizer i macierz cech
def build_vectorizer(texts, max_features=20000):
    vec = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))
    X = vec.fit_transform(texts)
    return vec, X


# Buduje model DNN wedÅ‚ug podanej architektury 
def build_model(input_dim, layer_sizes, activation_hidden="relu", dropout=0.2, lr=1e-3):
    model = Sequential()
    # Warstwa wejÅ›ciowa jest czÄ™Å›ciÄ… pierwszej warstwy ukrytej
    for i, size in enumerate(layer_sizes):
        if i == 0:
            model.add(Dense(size, activation=activation_hidden, input_shape=(input_dim,)))
        else:
            model.add(Dense(size, activation=activation_hidden))
        model.add(Dropout(dropout))
    # Warstwa wyjÅ›ciowa - sigmoid dla binarnej klasyfikacji
    model.add(Dense(1, activation="sigmoid"))
    model.compile(optimizer=Adam(learning_rate=lr),
                  loss="binary_crossentropy",
                  metrics=["accuracy"])
    return model


# === GÅÃ“WNY PROGRAM ===
def main():
    print("ğŸ“‚ Wczytywanie danych...")
    entries = load_index(INDEX_PATH)
    random.shuffle(entries)

    if SAMPLE_SIZE:
        use_entries = entries[:SAMPLE_SIZE]
        print(f"âš ï¸ SAMPLE_SIZE aktywne. WykorzystujÄ™ {len(use_entries)} pierwszych wpisÃ³w.")
    else:
        use_entries = entries

    # Przygotowanie tekstÃ³w i etykiet
    print("ğŸ§¾ Przygotowanie korpusu tekstÃ³w (preprocessing = %s)..." % USE_PREPROCESSING)
    texts, labels = prepare_corpus(use_entries, use_preprocessing=USE_PREPROCESSING, sample_size=None)
    print(f"Przygotowano {len(texts)} dokumentÃ³w.")

    # PodziaÅ‚ na trening/test (z zachowaniem TRAIN_RATIO)
    split_point = int(len(texts) * TRAIN_RATIO)
    X_texts_train = texts[:split_point]
    X_texts_test = texts[split_point:]
    y_train = labels[:split_point]
    y_test = labels[split_point:]
    print(f"Trening: {len(X_texts_train)}, Test: {len(X_texts_test)}")

    # Tworzenie wektorÃ³w TF-IDF
    print(f"ğŸ”¤ Tworzenie TF-IDF (max_features={MAX_FEATURES})...")
    vectorizer, X_train_sparse = build_vectorizer(X_texts_train, max_features=MAX_FEATURES)
    X_test_sparse = vectorizer.transform(X_texts_test)

    # Konwersja do dense (Keras wymaga gÄ™stych (Dense) macierzy)
    print("Konwersja do macierzy gÄ™stych...")
    X_train = X_train_sparse.toarray().astype(np.float32)
    X_test = X_test_sparse.toarray().astype(np.float32)
    input_dim = X_train.shape[1]
    print(f"Input dim = {input_dim}")

    results_lines = []
    results_lines.append(f"DNN TF-IDF results\nSAMPLE_SIZE={SAMPLE_SIZE}\nMAX_FEATURES={MAX_FEATURES}\nEPOCHS={EPOCHS}\nBATCH_SIZE={BATCH_SIZE}\nUSE_PREPROCESSING={USE_PREPROCESSING}\n\n")

    # Dla kaÅ¼dej konfiguracji modelu trenuje, testuje i zapisuje wyniki
    for cfg in MODEL_CONFIGS:
        name = cfg["name"]
        layers = cfg["layers"]
        activation = cfg.get("activation", "relu")
        print(f"\n=== Model: {name} | layers={layers} | activation={activation} ===")
        model = build_model(input_dim=input_dim, layer_sizes=layers, activation_hidden=activation)

        # Trening modelu
        t0 = time.time()
        history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)
        train_time = time.time() - t0
        print(f"Trening zakoÅ„czony w {train_time:.2f}s")

        # Predykcja na zbiorze testowym
        t1 = time.time()
        y_prob = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0).ravel()
        y_pred = (y_prob >= 0.5).astype(int)
        predict_time = time.time() - t1

        # Metryki ewaluacyjne
        acc = accuracy_score(y_test, y_pred) * 100.0
        labels_order = [1, 0]  # spam=1, ham=0
        cm = confusion_matrix(y_test, y_pred, labels=labels_order)
        cm_percent = cm / np.sum(cm) * 100.0

        # Wypisuje wyniki i zapisuje je do pliku
        print(f"ğŸ¯ Accuracy: {acc:.2f}% | Czas treningu: {train_time:.2f}s | Czas predykcji: {predict_time:.2f}s")
        print("ğŸ“Š Confusion matrix (%):")
        print("      spam      ham")
        print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
        print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

        results_lines.append(f"Model: {name}\n")
        results_lines.append(f"layers={layers} activation={activation}\n")
        results_lines.append(f"accuracy={acc:.2f}% train_time={train_time:.2f}s predict_time={predict_time:.2f}s\n")
        results_lines.append(f"confusion_percent:\nspam_spam={cm_percent[0,0]:6.2f}% spam_ham={cm_percent[0,1]:6.2f}%\n")
        results_lines.append(f"ham_spam={cm_percent[1,0]:6.2f}% ham_ham={cm_percent[1,1]:6.2f}%\n\n")

        # Zwolnij pamiÄ™Ä‡ modelu przed kolejnym testem
        tf.keras.backend.clear_session()

    # Zapis do pliku wynikÃ³w
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_lines))
    
    print(f"\nğŸ“ Wyniki zapisano do: {RESULTS_FILE}")

if __name__ == "__main__":
    main()
```


#### Wyniki

Z wynikÃ³w zostaÅ‚y usuniÄ™te komunikaty oraz warningi TensorFlow dla zwiÄ™kszenia czytelnoÅ›ci wynikÃ³w.

``` text
ğŸ“‚ Wczytywanie danych...
ğŸ§¾ Przygotowanie korpusu tekstÃ³w (preprocessing = True)...
Przygotowano 75419 dokumentÃ³w.
Trening: 60335, Test: 15084
ğŸ”¤ Tworzenie TF-IDF (max_features=20000)...
Konwersja do macierzy gÄ™stych...
Input dim = 20000

=== Model: small | layers=[64] | activation=relu ===
Trening zakoÅ„czony w 34.64s
ğŸ¯ Accuracy: 99.67% | Czas treningu: 34.64s | Czas predykcji: 1.32s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  65.78%    0.09%
ham   0.24%     33.88%

=== Model: medium | layers=[128, 64] | activation=relu ===
Trening zakoÅ„czony w 56.67s
ğŸ¯ Accuracy: 99.64% | Czas treningu: 56.67s | Czas predykcji: 1.03s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  65.78%    0.09%
ham   0.27%     33.85%

=== Model: large | layers=[256, 128, 64] | activation=relu ===
Trening zakoÅ„czony w 99.26s
ğŸ¯ Accuracy: 99.61% | Czas treningu: 99.26s | Czas predykcji: 1.17s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  65.76%    0.12%
ham   0.27%     33.85%

=== Model: small_tanh | layers=[64] | activation=tanh ===
Trening zakoÅ„czony w 31.32s
ğŸ¯ Accuracy: 99.64% | Czas treningu: 31.32s | Czas predykcji: 0.83s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  65.77%    0.11%
ham   0.25%     33.87%

ğŸ“ Wyniki zapisano do: results_dnn.txt
```

#### Wnioski

W zadaniu 6 zastosowano gÄ™sto Å‚Ä…czonÄ… gÅ‚Ä™bokÄ… sieÄ‡ neuronowÄ… (DNN) do klasyfikacji binarnej wiadomoÅ›ci email na spam i ham. Wykorzystano rÃ³Å¼ne konfiguracje architektury sieci, testujÄ…c modele o rÃ³Å¼nej liczbie warstw ukrytych i liczbie neuronÃ³w w kaÅ¼dej warstwie.

**Opis metody**  
Metoda wykorzystuje gÄ™sto Å‚Ä…czonÄ… sieÄ‡ neuronowÄ… (DNN) do klasyfikacji wiadomoÅ›ci email, poÅ‚Ä…czonÄ… z technikÄ… ekstrakcji cech TF-IDF. Algorytm dziaÅ‚a w nastÄ™pujÄ…cych etapach:
1. **Preprocessing tekstu**: Stemizacja i usuwanie stopwords przy uÅ¼yciu NLTK 
2. **Ekstrakcja cech TF-IDF**: PrzeksztaÅ‚cenie tekstu na wektory numeryczne z uÅ¼yciem n-gramÃ³w (1,2)
3. **Architektura DNN**: Wielowarstwowa sieÄ‡ neuronowa z warstwami gÄ™stymi i dropout dla regularyzacji:
   - Warstwy ukryte: RÃ³Å¼ne konfiguracje (np. [64], [128, 64], [256, 128, 64])
   - Funkcje aktywacji: ReLU lub tanh w warstwach ukrytych, sigmoid w warstwie wyjÅ›ciowej
4. **Trening**: Uczenie nadzorowane z optymalizatorem Adam i funkcjÄ… straty binary_crossentropy
5. **Klasyfikacja**: Predykcja przy uÅ¼yciu funkcji sigmoid w warstwie wyjÅ›ciowej

**Zalety metody**
- **Wysoka zdolnoÅ›Ä‡ uogÃ³lniania** - sieci neuronowe dobrze radzÄ… sobie ze zÅ‚oÅ¼onymi wzorcami w danych
- **Automatyczna ekstrakcja cech** - TF-IDF automatycznie identyfikuje istotne sÅ‚owa i frazy
- **SkalowalnoÅ›Ä‡** - moÅ¼liwoÅ›Ä‡ obsÅ‚ugi duÅ¼ych zbiorÃ³w danych
- **ElastycznoÅ›Ä‡ architektury** - Å‚atwa modyfikacja liczby warstw i neuronÃ³w 

**Wady metody**
- **Wysokie wymagania obliczeniowe** - dÅ‚uÅ¼szy czas treningu w porÃ³wnaniu do prostszych metod 
- **ZÅ‚oÅ¼onoÅ›Ä‡ interpretacji** - trudnoÅ›Ä‡ w zrozumieniu, ktÃ³re cechy sÄ… najwaÅ¼niejsze 
- **ZaleÅ¼noÅ›Ä‡ od preprocessingu** - jakoÅ›Ä‡ danych wejÅ›ciowych znaczÄ…co wpÅ‚ywa na wyniki 

---

**Konfiguracja programu**  
Podobnie jak poprzednio, na wstÄ™pie naleÅ¼aÅ‚o zdefiniowaÄ‡ staÅ‚e konfiguracyjne. Konfiguracja zostaÅ‚a zaprojektowana do porÃ³wnania rÃ³Å¼nych architektur sieci neuronowych pod kÄ…tem skutecznoÅ›ci klasyfikacji spam/ham. Eksperyment obejmuje testowanie rÃ³Å¼nych rozmiarÃ³w sieci oraz funkcji aktywacji. NajwaÅ¼niejsze parametry to:  
- **`MAX_FEATURES = 20000`** - Optymalny kompromis miÄ™dzy dokÅ‚adnoÅ›ciÄ… a wymaganiami pamiÄ™ciowymi 
- **`EPOCHS = 5`** - WystarczajÄ…ca liczba epok dla zbieÅ¼noÅ›ci przy zachowaniu rozsÄ…dnego czasu treningu
- **`BATCH_SIZE = 128`** - Efektywny rozmiar batcha dla duÅ¼ego zbioru danych 
- **`USE_PREPROCESSING = True`** - Wykorzystanie peÅ‚nego preprocessingu NLTK 

---

**Tabela porÃ³wnawcza wynikÃ³w dla rÃ³Å¼nych konfiguracji DNN:**
| Model | Warstwy | Aktywacja | Accuracy | Czas treningu | Czas predykcji | Poprawny spam | FaÅ‚szywe negatywy | FaÅ‚szywe pozytywne | Poprawny ham |
|-------|---------|-----------|----------|---------------|----------------|---------------|-------------------|-------------------|-------------|
| **small** | [64] | relu | **99.67%** | 34.64s | 1.32s | **65.78%** | **0.09%** | 0.24% | 33.88% |
| **medium** | [128, 64] | relu | 99.64% | 56.67s | 1.03s | **65.78%** | **0.09%** | 0.27% | 33.85% |
| **large** | [256, 128, 64] | relu | 99.61% | 99.26s | 1.17s | 65.76% | 0.12% | 0.27% | 33.85% |
| **small_tanh** | [64] | tanh | 99.64% | **31.32s** | **0.83s** | 65.77% | 0.11% | **0.25%** | **33.87%** |

**WpÅ‚yw architektury sieci na skutecznoÅ›Ä‡**  
Analiza wpÅ‚ywu architektury sieci neuronowych na skutecznoÅ›Ä‡ klasyfikacji pokazaÅ‚a, Å¼e najprostszy z testowanych modeli â€“ `wariant small` â€“ osiÄ…gnÄ…Å‚ najwyÅ¼szÄ… dokÅ‚adnoÅ›Ä‡ na poziomie 99,67%. Wynik ten sugeruje, Å¼e w przypadku tego konkretnego zadania bardziej zÅ‚oÅ¼one i gÅ‚Ä™bsze architektury nie wnoszÄ… dodatkowych korzyÅ›ci. Proste modele dysponujÄ… wystarczajÄ…cÄ… pojemnoÅ›ciÄ…, aby skutecznie uchwyciÄ‡ zaleÅ¼noÅ›ci w danych, natomiast zwiÄ™kszanie liczby warstw nie tylko nie poprawia jakoÅ›ci, ale moÅ¼e wrÄ™cz prowadziÄ‡ do nieznacznego przeuczenia, co potwierdzajÄ… sÅ‚absze wyniki modelu large.

**Analiza funkcji aktywacji**  
W badaniu funkcji aktywacji najlepiej wypadÅ‚a funkcja `ReLU` zastosowana w modelu `small`, choÄ‡ rÃ³Å¼nice wzglÄ™dem `tanh` okazaÅ‚y siÄ™ minimalne â€“ odpowiednio 99,67% i 99,64% dokÅ‚adnoÅ›ci. Co ciekawe, wariant `small_tanh` zapewniÅ‚ najszybszy czas zarÃ³wno treningu, jak i predykcji, co czyni go atrakcyjnÄ… alternatywÄ… w kontekÅ›cie optymalizacji wydajnoÅ›ci.

**WydajnoÅ›Ä‡ czasowa**
Zgodnie z oczekiwaniami, czas treningu rÃ³sÅ‚ wraz ze zÅ‚oÅ¼onoÅ›ciÄ… architektury â€“ od 31 sekund w przypadku najprostszego modelu do 99 sekund dla najgÅ‚Ä™bszego. Czasy predykcji dla wszystkich wariantÃ³w pozostawaÅ‚y natomiast bardzo krÃ³tkie i mieÅ›ciÅ‚y siÄ™ w przedziale od 0,83 do 1,32 sekundy dla caÅ‚ego zbioru testowego. W rezultacie to wÅ‚aÅ›nie model `small` okazaÅ‚ siÄ™ najbardziej zrÃ³wnowaÅ¼onym rozwiÄ…zaniem, oferujÄ…c najwyÅ¼szÄ… skutecznoÅ›Ä‡ przy relatywnie krÃ³tkim czasie treningu.

**JakoÅ›Ä‡ klasyfikacji**  
W kontekÅ›cie jakoÅ›ci klasyfikacji wszystkie testowane architektury poradziÅ‚y sobie znakomicie, generujÄ…c jedynie minimalne bÅ‚Ä™dy. Odsetek faÅ‚szywych negatywÃ³w wynosiÅ‚ zaledwie 0,09â€“0,12%, co oznacza, Å¼e tylko niewielka czÄ™Å›Ä‡ spamu pozostawaÅ‚a nieodfiltrowana. RÃ³wnie niski poziom faÅ‚szywych pozytywÃ³w (0,24â€“0,27%) wskazuje, Å¼e klasyfikatory rzadko bÅ‚Ä™dnie oznaczaÅ‚y prawidÅ‚owe wiadomoÅ›ci jako spam.

---

**PorÃ³wnanie z metodami z poprzednich zadaÅ„**
| Metoda | Najlepsza accuracy | Czas przetwarzania | Zalety | Wady |
|--------|-------------------|-------------------|--------|------|
| **Blacklista** | 61.83% | 2465s | Prosta, interpretowalna | Bardzo niska skutecznoÅ›Ä‡ |
| **LSH** | 94.50% | ~591s | Skalowalna, dobre podobieÅ„stwa | ZaleÅ¼na od parametrÃ³w |
| **Naive Bayes** | 99.24% | 70s | Szybki, wysoka skutecznoÅ›Ä‡ | ZaÅ‚oÅ¼enie niezaleÅ¼noÅ›ci |
| **DNN (small)** | **99.67%** | **~36s** | **NajwyÅ¼sza dokÅ‚adnoÅ›Ä‡**, dobre uogÃ³lnianie | Wymaga preprocessingu |

Przeprowadzone eksperymenty wykazaÅ‚y, Å¼e gÅ‚Ä™bokie sieci neuronowe (DNN) stanowiÄ… najbardziej efektywnÄ… metodÄ™ spoÅ›rÃ³d wszystkich testowanych podejÅ›Ä‡, osiÄ…gajÄ…c najwyÅ¼szÄ… dokÅ‚adnoÅ›Ä‡ na poziomie `99,67%`. Wynik ten jest o `0,43%` lepszy niÅ¼ w przypadku klasyfikatora Naive Bayes, co podkreÅ›la potencjaÅ‚ bardziej zaawansowanych modeli w analizie tekstu. Co istotne, najlepsze rezultaty uzyskano dziÄ™ki niezwykle prostej architekturze â€“ model skÅ‚adajÄ…cy siÄ™ z jednej warstwy ukrytej i 64 neuronÃ³w okazaÅ‚ siÄ™ najskuteczniejszy, co potwierdza, Å¼e w niektÃ³rych zadaniach dodatkowa zÅ‚oÅ¼onoÅ›Ä‡ nie przekÅ‚ada siÄ™ na wyÅ¼szÄ… jakoÅ›Ä‡.

Wszystkie warianty DNN charakteryzowaÅ‚y siÄ™ bardzo niskim poziomem bÅ‚Ä™dÃ³w. Odsetek faÅ‚szywych negatywÃ³w utrzymywaÅ‚ siÄ™ poniÅ¼ej `0,15%`, co oznacza, Å¼e niemal wszystkie wiadomoÅ›ci spam byÅ‚y skutecznie wykrywane. RÃ³wnie niski poziom faÅ‚szywych pozytywÃ³w â€“ poniÅ¼ej `0,3%` â€“ Å›wiadczy o wysokiej precyzji klasyfikatorÃ³w w odrÃ³Å¼nianiu prawidÅ‚owych wiadomoÅ›ci od spamu.

Pomimo zastosowania bardziej zaawansowanej techniki, czasy treningu i predykcji okazaÅ‚y siÄ™ konkurencyjne wzglÄ™dem prostszych metod. Modele DNN trenowaÅ‚y siÄ™ relatywnie szybko, a ich czas przetwarzania podczas klasyfikacji pozostawaÅ‚ bardzo krÃ³tki, co czyni je praktycznym narzÄ™dziem takÅ¼e w systemach dziaÅ‚ajÄ…cych w czasie rzeczywistym.

Analiza pokazaÅ‚a, Å¼e model `small` z funkcjÄ… aktywacji `ReLU` stanowi rozwiÄ…zanie optymalne. ÅÄ…czy on najwyÅ¼szÄ… skutecznoÅ›Ä‡ z dobrÄ… wydajnoÅ›ciÄ… obliczeniowÄ… i prostotÄ… implementacji. 

W porÃ³wnaniu do poczÄ…tkowych metod, takich jak blacklisty czy LSH, DNN stanowiÄ… ogromny krok naprzÃ³d, poprawiajÄ…c skutecznoÅ›Ä‡ odpowiednio o `37,84%` i `5,17%`. PoÅ‚Ä…czenie gÅ‚Ä™bokich sieci neuronowych z reprezentacjÄ… TF-IDF okazaÅ‚o siÄ™ zatem najskuteczniejszym podejÅ›ciem do klasyfikacji wiadomoÅ›ci e-mail, zapewniajÄ…c najlepszy balans miÄ™dzy dokÅ‚adnoÅ›ciÄ… a wydajnoÅ›ciÄ….


# TODO:
- DodaÄ‡ diagramy kodu z Mermaid Chart (jest podobno jako dodatek do Visual Studio Code) Diagramy majÄ… byÄ‡ jako skrypty, a nie jako obrazki