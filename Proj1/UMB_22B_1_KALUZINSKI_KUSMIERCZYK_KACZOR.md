# Uczenie Maszynowe w BezpieczeÅ„stwie
## Projekt 1
### Grupa 22B
### Autorzy: PrzemysÅ‚aw KaÅ‚uÅ¼iÅ„ski, Jakub KuÅ›mierczyk, MichaÅ‚ Kaczor

### Zadanie 1
PobraÄ‡, rozpakowaÄ‡ i przeanalizowaÄ‡ strukturÄ™ plikÃ³w i katalogÃ³w archiwum zawierajÄ…cego wiadomoÅ›ci poczty elektronicznej.  
Dane te dostÄ™pne sÄ… pod adresem:  
https://plg.uwaterloo.ca/~gvcormac/treccorpus07/  
**Uwaga.** Nie naleÅ¼y otwieraÄ‡ plikÃ³w z archiwum ani w przeglÄ…darce HTML ani w programie pocztowym!

#### Wyniki

PomyÅ›lnie pobrano i rozpakowano archiwum `TREC 2007 Public Corpus`, ktÃ³re bÄ™dzie wykorzystywane w dalszych zadaniach projektu. Ze wzglÄ™du na rozmiar archiwum (okoÅ‚o 450 MB) oraz potencjalnie niebezpiecznÄ… zawartoÅ›Ä‡ wiadomoÅ›ci spam (ktÃ³re mogÄ… zawieraÄ‡ zÅ‚oÅ›liwe linki lub nieodpowiednie treÅ›ci), archiwum zostaÅ‚o wykluczone z repozytorium Git poprzez dodanie do pliku `.gitignore`.

**Archiwum TREC 2007**  
TREC 2007 Public Corpus to publicznie dostÄ™pne archiwum wiadomoÅ›ci email uÅ¼ywane do badaÅ„ nad filtrowaniem spamu. ZbiÃ³r zostaÅ‚ opracowany w ramach Text Retrieval Conference (TREC) i stanowi standardowy benchmark do testowania algorytmÃ³w klasyfikacji wiadomoÅ›ci email.

Archiwum posiada nastÄ™pujÄ…cÄ… strukturÄ™ katalogÃ³w:  
trec07p/  
â”œâ”€â”€ data/ - GÅ‚Ã³wny folder z wiadomoÅ›ciami  
â”œâ”€â”€ full/ - Folder z peÅ‚nym indeksem  
â”œâ”€â”€ delay/ - Dane feedback tylko dla pierwszych 10,000 wiadomoÅ›ci  
â””â”€â”€ partial/ - Dane feedback tylko dla 30,388 wiadomoÅ›ci odpowiadajÄ…cych 1 odbiorcy

**Folder `data/`**:
- Zawiera 75,419 wiadomoÅ›ci email w postaci plikÃ³w tekstowych
- Pliki majÄ… nazwy w formacie `inmail.X`, gdzie X to liczba od 1 do 75419
- KaÅ¼dy plik zawiera peÅ‚nÄ… wiadomoÅ›Ä‡ email w formacie MIME

**Folder `full/`**:
- Zawiera plik `index` bÄ™dÄ…cy sÅ‚ownikiem klasyfikacji
- Format wpisÃ³w: `[etykieta] [Å›cieÅ¼ka_do_pliku]`, np. `spam ../data/inmail.1`
- Etykiety: "spam" (niechciane wiadomoÅ›ci) lub "ham" (poÅ¼Ä…dane wiadomoÅ›ci)

**Foldery dodatkowe (nieuÅ¼ywane w projekcie)**:
- `delay/` - zawiera dane feedback tylko dla pierwszych 10,000 wiadomoÅ›ci
- `partial/` - zawiera dane feedback tylko dla 30,388 wiadomoÅ›ci odpowiadajÄ…cych jednemu odbiorcy

---

**Statystyki zbioru danych**:
- **ÅÄ…czna liczba wiadomoÅ›ci**: 75,419
- **WiadomoÅ›ci ham (poÅ¼Ä…dane)**: 25,220 (33.4%)
- **WiadomoÅ›ci spam (niechciane)**: 50,199 (66.6%)
- **RozkÅ‚ad**: Przewaga wiadomoÅ›ci spam

### Zadanie 2
WykorzystujÄ…c informacje z wykÅ‚adu oraz stosujÄ…c technikÄ™ zakazanych sÅ‚Ã³w kluczowych (blacklist), dokonaÄ‡ klasyfikacji binarnej wiadomoÅ›ci z archiwum z podziaÅ‚em na: spam (wiadomoÅ›ci typu spam) oraz ham (wiadomoÅ›ci poÅ¼Ä…dane).

**Uwagi:**
1. Przed przystÄ…pieniem do procesu klasyfikacji usunÄ…Ä‡ z wiadomoÅ›ci stopping words (np. the, is, are, . . . ),
dokonaÄ‡ stemizacji sÅ‚Ã³w w wiadomoÅ›ciach oraz ekstrakcji tokenÃ³w.
2. Do realizacji zadania uÅ¼yÄ‡ jÄ™zyka Python oraz bibliotek: string, email, NLTK, os.
3. ZbiÃ³r zakazanych sÅ‚Ã³w kluczowych powinien byÄ‡ wygenerowany na podstawie danych z podzbioru treningowego,
natomiast ewaluacja danych uzyskanych z podzbioru testowego.
4. Wynikiem ewaluacji powinna byÄ‡ macierz konfuzji (procentowa) oraz wartoÅ›Ä‡ wskaÅºnika accuracy, rÃ³wnieÅ¼ w
postaci procentowej.

#### Implementacja

Ze wzglÄ™du na fakt, Å¼e kod implementujÄ…cy zadania 2 i 3 jest ze sobÄ… Å›ciÅ›le powiÄ…zany, to peÅ‚na implementacja obu zadaÅ„ zostaÅ‚a umieszczona w rozdziale **implementacja** zadania 3. 

#### Wyniki

Podobnie jak implementacja, wyniki obu zadaÅ„ 2 i 3 zostaÅ‚y przedstawione w rozdziale **wyniki** zadania 3, poniewaÅ¼ kod programu zwraca wyniki obu zadaÅ„ jednoczeÅ›nie.

### Zadanie 3
ZweryfikowaÄ‡ wpÅ‚yw stemizacji na pracÄ™ algorytmu zadania drugiego a nastÄ™pnie porÃ³wnaÄ‡ uzyskane wyniki.

#### Implementacja

**1. Konfiguracja globalna**

Na wstÄ™pie programu znajduje siÄ™ kod, ktÃ³ry definiuje staÅ‚e konfiguracyjne uÅ¼ywane w caÅ‚ym programie. UÅ‚atwia to dostosowanie parametrÃ³w bez koniecznoÅ›ci modyfikowania logiki programu.

**Kod:**
``` python
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
TOP_N = 100                             # liczba sÅ‚Ã³w w blacklist
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_stemming.txt"   # nazwa pliku wynikowego
```

**2. Funkcja `load_index`**

**WejÅ›cie:**  
- `index_path` (string) - Å›cieÅ¼ka do pliku z indeksem wiadomoÅ›ci

**WyjÅ›cie:**  
- `entries` (list) - lista krotek zawierajÄ…cych peÅ‚nÄ… Å›cieÅ¼kÄ™ do pliku i etykietÄ™ (spam/ham)

**Opis:**  
Funkcja wczytuje plik indeksu, gdzie kaÅ¼da linia zawiera etykietÄ™ (spam/ham) i Å›cieÅ¼kÄ™ do pliku z wiadomoÅ›ciÄ…. Parsuje kaÅ¼dÄ… liniÄ™, tworzy peÅ‚nÄ… Å›cieÅ¼kÄ™ do pliku (usuwajÄ…c "../" z oryginalnej Å›cieÅ¼ki) i zwraca listÄ™ wszystkich wpisÃ³w.

**Kod:**
``` python
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            label, path = line.strip().split()
            full_path = os.path.join(DATA_PATH, path.replace("../", ""))
            entries.append((full_path, label))
    return entries
```

---

**3. Funkcja `preprocess_text`**

**WejÅ›cie:**  
- `text` (string) - tekst wiadomoÅ›ci email do przetworzenia
- `use_stemming` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ stemizacjÄ™

**WyjÅ›cie:**  
- `tokens` (list) - lista przetworzonych tokenÃ³w (sÅ‚Ã³w)

**Opis:**  
Funkcja przeprowadza peÅ‚ne przetwarzanie tekstu: konwersja na maÅ‚e litery, usuwanie znakÃ³w interpunkcyjnych, tokenizacja na pojedyncze sÅ‚owa, usuwanie stopwords (sÅ‚Ã³w bez znaczenia) oraz opcjonalna stemizacja przy uÅ¼yciu algorytmu PorterStemmer. Zwraca listÄ™ oczyszczonych tokenÃ³w.

**Kod:**
``` python
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if w not in stop_words and w.isalpha()]

    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(w) for w in tokens]

    return tokens
```

---

**4. Funkcja `load_email_content`**

**WejÅ›cie:**  
- `filepath` (string) - Å›cieÅ¼ka do pliku z wiadomoÅ›ciÄ… email

**WyjÅ›cie:**  
- `text` (string) - wyekstrahowana treÅ›Ä‡ wiadomoÅ›ci lub pusty string w przypadku bÅ‚Ä™du

**Opis:**  
Funkcja wczytuje i parsuje wiadomoÅ›Ä‡ email przy uÅ¼yciu biblioteki email. ObsÅ‚uguje wiadomoÅ›ci wieloczÄ™Å›ciowe (multipart), dekoduje zawartoÅ›Ä‡ i zwraca czysty tekst wiadomoÅ›ci. W przypadku bÅ‚Ä™dÃ³w zwraca pusty string.

**Kod:**
``` python
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            if msg.is_multipart():
                parts = [p.get_payload(decode=True) for p in msg.get_payload() if p.get_payload()]
                text = " ".join([str(p) for p in parts])
            else:
                text = msg.get_payload(decode=True)
            if text:
                text = text.decode(errors="ignore") if isinstance(text, bytes) else text
                return text
            else:
                return ""
    except Exception:
        return ""
```

---

**5. Funkcja `build_blacklist`**

**WejÅ›cie:**  
- `train_data` (list) - lista krotek (tokens, label) z danych treningowych
- `top_n` (int) - liczba sÅ‚Ã³w do umieszczenia na blackliÅ›cie

**WyjÅ›cie:**  
- `blacklist` (list) - lista sÅ‚Ã³w kluczowych najbardziej charakterystycznych dla spamu

**Opis:**  
Funkcja analizuje dane treningowe, zliczajÄ…c wystÄ…pienia sÅ‚Ã³w w spamie i hamie. Dla kaÅ¼dego sÅ‚owa oblicza stosunek czÄ™stotliwoÅ›ci w spamie do czÄ™stotliwoÅ›ci w hamie. Zwraca listÄ™ `top_n` sÅ‚Ã³w z najwyÅ¼szym stosunkiem, ktÃ³re bÄ™dÄ… uÅ¼ywane jako zakazane sÅ‚owa kluczowe.

**Kod:**
``` python
def build_blacklist(train_data, top_n=100):
    spam_words = {}
    ham_words = {}
    for tokens, label in train_data:
        for token in tokens:
            if label == "spam":
                spam_words[token] = spam_words.get(token, 0) + 1
            else:
                ham_words[token] = ham_words.get(token, 0) + 1

    spam_ratio = {word: spam_words[word] / (ham_words.get(word, 0) + 1) for word in spam_words}
    sorted_words = sorted(spam_ratio.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:top_n]]
```

---

**6. Funkcja `classify_email`**

**WejÅ›cie:**  
- `tokens` (list) - lista tokenÃ³w z przetworzonej wiadomoÅ›ci
- `blacklist` (list) - lista zakazanych sÅ‚Ã³w kluczowych

**WyjÅ›cie:**  
- `"spam"` lub `"ham"` (string) - wynik klasyfikacji

**Opis:**  
Funkcja klasyfikuje wiadomoÅ›Ä‡ jako spam, jeÅ›li ktÃ³rykolwiek z tokenÃ³w znajduje siÄ™ na blackliÅ›cie. W przeciwnym przypadku klasyfikuje jako ham. Jest to prosty klasyfikator oparty na zasadzie "czarnej listy".

**Kod:**  
``` python
def classify_email(tokens, blacklist):
    return "spam" if any(word in blacklist for word in tokens) else "ham"
```

---

**7. Funkcja `evaluate_model`**

**WejÅ›cie:**  
- `train_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych treningowych
- `test_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych testowych
- `use_stemming` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ stemizacjÄ™

**WyjÅ›cie:**  
- `acc` (float) - dokÅ‚adnoÅ›Ä‡ klasyfikacji w procentach
- `cm_percent` (numpy.ndarray) - macierz konfuzji w procentach
- `elapsed` (float) - czas wykonania w sekundach

**Opis:**  
Funkcja przeprowadza peÅ‚ny proces uczenia i ewaluacji modelu: przetwarza dane treningowe, buduje blacklistÄ™, klasyfikuje wiadomoÅ›ci testowe i oblicza metryki wydajnoÅ›ci. Zwraca accuracy, macierz konfuzji i czas wykonania.

**Kod:**
``` python
def evaluate_model(train_entries, test_entries, use_stemming):
    start_time = time.time()

    train_data = []
    for path, label in train_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        train_data.append((tokens, label))

    blacklist = build_blacklist(train_data, top_n=TOP_N)

    y_true, y_pred = [], []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        prediction = classify_email(tokens, blacklist)
        y_true.append(label)
        y_pred.append(prediction)

    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_true, y_pred) * 100

    return acc, cm_percent, elapsed
```

---

**8. Funkcja `main`**

**WejÅ›cie:**  
- Brak parametrÃ³w wejÅ›ciowych

**WyjÅ›cie:**  
- Brak bezpoÅ›redniego wyjÅ›cia (funkcja wykonuje program i zapisuje wyniki do pliku)

**Opis:**  
GÅ‚Ã³wna funkcja programu, ktÃ³ra koordynuje caÅ‚y proces: wczytuje i tasuje dane, dzieli na zbiÃ³r treningowy i testowy, przeprowadza dwa eksperymenty (ze stemizacjÄ… i bez), porÃ³wnuje wyniki, wyÅ›wietla raport i zapisuje wyniki do pliku tekstowego.

**Kod:**
``` python
def main():
    print("ğŸ“‚ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    results_log = []

    # Test 1: ZE STEMIZACJÄ„
    print("ğŸ§  Test 1: ZE STEMIZACJÄ„")
    acc_stem, cm_stem, time_stem = evaluate_model(train_entries, test_entries, use_stemming=True)
    print(f"ğŸ¯ Accuracy (stem): {acc_stem:.2f}% | â± Czas: {time_stem:.2f}s")
    results_log.append(f"Test 1 (ze stemizacjÄ…): accuracy={acc_stem:.2f}%, czas={time_stem:.2f}s")

    # Test 2: BEZ STEMIZACJI
    print("\nğŸ§  Test 2: BEZ STEMIZACJI")
    acc_no_stem, cm_no_stem, time_no_stem = evaluate_model(train_entries, test_entries, use_stemming=False)
    print(f"ğŸ¯ Accuracy (no stem): {acc_no_stem:.2f}% | â± Czas: {time_no_stem:.2f}s")
    results_log.append(f"Test 2 (bez stemizacji): accuracy={acc_no_stem:.2f}%, czas={time_no_stem:.2f}s")

    # PorÃ³wnanie wynikÃ³w
    diff_acc = acc_stem - acc_no_stem
    diff_time = time_stem - time_no_stem

    summary = (
        "\nğŸ“Š PORÃ“WNANIE WYNIKÃ“W\n"
        f"Ze stemizacjÄ…:    {acc_stem:.2f}% ({time_stem:.2f}s)\n"
        f"Bez stemizacji:  {acc_no_stem:.2f}% ({time_no_stem:.2f}s)\n"
        f"ğŸ§© RÃ³Å¼nica dokÅ‚adnoÅ›ci: {diff_acc:+.2f}%\n"
        f"â± RÃ³Å¼nica czasu: {diff_time:+.2f}s (wartoÅ›Ä‡ dodatnia = wolniej ze stemizacjÄ…)\n"
    )

    print(summary)
    results_log.append(summary)

    # Macierze konfuzji
    matrix_report = (
        "\nğŸ“Š MACIERZ KONFUZJI (ZE STEMIZACJÄ„):\n"
        f"      spam      ham\n"
        f"spam  {cm_stem[0,0]:6.2f}%   {cm_stem[0,1]:6.2f}%\n"
        f"ham   {cm_stem[1,0]:6.2f}%   {cm_stem[1,1]:6.2f}%\n\n"
        "ğŸ“Š MACIERZ KONFUZJI (BEZ STEMIZACJI):\n"
        f"      spam      ham\n"
        f"spam  {cm_no_stem[0,0]:6.2f}%   {cm_no_stem[0,1]:6.2f}%\n"
        f"ham   {cm_no_stem[1,0]:6.2f}%   {cm_no_stem[1,1]:6.2f}%\n"
    )
    print(matrix_report)
    results_log.append(matrix_report)

    # Zapis wynikÃ³w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_log))

    print(f"ğŸ“ Wyniki zapisano do pliku: {RESULTS_FILE}")
```

---

**9. Kompletny kod**  
PoniÅ¼ej znajduje siÄ™ kompletny kod programu, ktÃ³ry moÅ¼na uruchomiÄ‡.

**Kod:**
``` python
import os
import string
import random
import time
from email import message_from_file
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.metrics import confusion_matrix, accuracy_score
import numpy as np

# === KONFIGURACJA ===
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
TOP_N = 100                             # liczba sÅ‚Ã³w w blacklist
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_stemming.txt"   # nazwa pliku wynikowego


# === FUNKCJE ===
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            label, path = line.strip().split()
            full_path = os.path.join(DATA_PATH, path.replace("../", ""))
            entries.append((full_path, label))
    return entries

# Funcja do przetwarzania tekstu. Przeprowadza takie funkcje jak: czyszczenie, tokenizacja, usuwanie stopwords i (opcjonalnie) stemizacja
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if w not in stop_words and w.isalpha()]

    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(w) for w in tokens]

    return tokens

# Wczytuje zawartoÅ›Ä‡ e-maila.
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            if msg.is_multipart():
                parts = [p.get_payload(decode=True) for p in msg.get_payload() if p.get_payload()]
                text = " ".join([str(p) for p in parts])
            else:
                text = msg.get_payload(decode=True)
            if text:
                text = text.decode(errors="ignore") if isinstance(text, bytes) else text
                return text
            else:
                return ""
    except Exception:
        return ""

# Tworzy listÄ™ sÅ‚Ã³w kluczowych na podstawie danych treningowych.
def build_blacklist(train_data, top_n=100):
    spam_words = {}
    ham_words = {}
    for tokens, label in train_data:
        for token in tokens:
            if label == "spam":
                spam_words[token] = spam_words.get(token, 0) + 1
            else:
                ham_words[token] = ham_words.get(token, 0) + 1

    spam_ratio = {word: spam_words[word] / (ham_words.get(word, 0) + 1) for word in spam_words}
    sorted_words = sorted(spam_ratio.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:top_n]]

# Zwraca etykietÄ™ spam/ham w zaleÅ¼noÅ›ci od obecnoÅ›ci sÅ‚Ã³w zakazanych.
def classify_email(tokens, blacklist):
    return "spam" if any(word in blacklist for word in tokens) else "ham"

# Trenuje i testuje klasyfikator; zwraca accuracy, macierz konfuzji i czas.
def evaluate_model(train_entries, test_entries, use_stemming):
    start_time = time.time()

    train_data = []
    for path, label in train_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        train_data.append((tokens, label))

    blacklist = build_blacklist(train_data, top_n=TOP_N)

    y_true, y_pred = [], []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        prediction = classify_email(tokens, blacklist)
        y_true.append(label)
        y_pred.append(prediction)

    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_true, y_pred) * 100

    return acc, cm_percent, elapsed


# === GÅÃ“WNY PROGRAM ===
def main():
    print("ğŸ“‚ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    results_log = []

    # Test 1: ZE STEMIZACJÄ„
    print("ğŸ§  Test 1: ZE STEMIZACJÄ„")
    acc_stem, cm_stem, time_stem = evaluate_model(train_entries, test_entries, use_stemming=True)
    print(f"ğŸ¯ Accuracy (stem): {acc_stem:.2f}% | â± Czas: {time_stem:.2f}s")
    results_log.append(f"Test 1 (ze stemizacjÄ…): accuracy={acc_stem:.2f}%, czas={time_stem:.2f}s")

    # Test 2: BEZ STEMIZACJI
    print("\nğŸ§  Test 2: BEZ STEMIZACJI")
    acc_no_stem, cm_no_stem, time_no_stem = evaluate_model(train_entries, test_entries, use_stemming=False)
    print(f"ğŸ¯ Accuracy (no stem): {acc_no_stem:.2f}% | â± Czas: {time_no_stem:.2f}s")
    results_log.append(f"Test 2 (bez stemizacji): accuracy={acc_no_stem:.2f}%, czas={time_no_stem:.2f}s")

    # PorÃ³wnanie wynikÃ³w
    diff_acc = acc_stem - acc_no_stem
    diff_time = time_stem - time_no_stem

    summary = (
        "\nğŸ“Š PORÃ“WNANIE WYNIKÃ“W\n"
        f"ZE stemizacjÄ…:    {acc_stem:.2f}% ({time_stem:.2f}s)\n"
        f"Bez stemizacji:  {acc_no_stem:.2f}% ({time_no_stem:.2f}s)\n"
        f"ğŸ§© RÃ³Å¼nica dokÅ‚adnoÅ›ci: {diff_acc:+.2f}%\n"
        f"â± RÃ³Å¼nica czasu: {diff_time:+.2f}s (wartoÅ›Ä‡ dodatnia = wolniej ze stemizacjÄ…)\n"
    )

    print(summary)
    results_log.append(summary)

    # Macierze konfuzji
    matrix_report = (
        "\nğŸ“Š MACIERZ KONFUZJI (ZE STEMIZACJÄ„):\n"
        f"      spam      ham\n"
        f"spam  {cm_stem[0,0]:6.2f}%   {cm_stem[0,1]:6.2f}%\n"
        f"ham   {cm_stem[1,0]:6.2f}%   {cm_stem[1,1]:6.2f}%\n\n"
        "ğŸ“Š MACIERZ KONFUZJI (BEZ STEMIZACJI):\n"
        f"      spam      ham\n"
        f"spam  {cm_no_stem[0,0]:6.2f}%   {cm_no_stem[0,1]:6.2f}%\n"
        f"ham   {cm_no_stem[1,0]:6.2f}%   {cm_no_stem[1,1]:6.2f}%\n"
    )
    print(matrix_report)
    results_log.append(matrix_report)

    # Zapis wynikÃ³w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_log))

    print(f"ğŸ“ Wyniki zapisano do pliku: {RESULTS_FILE}")


if __name__ == "__main__":
    main()
```

#### Wyniki

```text
ğŸ“‚ Wczytywanie danych...
ğŸ§  Test 1: ZE STEMIZACJÄ„
ğŸ¯ Accuracy (stem): 61.83% | â± Czas: 2465.29s

ğŸ§  Test 2: BEZ STEMIZACJI
ğŸ¯ Accuracy (no stem): 58.64% | â± Czas: 239.89s

ğŸ“Š PORÃ“WNANIE WYNIKÃ“W
Ze stemizacjÄ…:    61.83% (2465.29s)
Bez stemizacji:  58.64% (239.89s)
ğŸ§© RÃ³Å¼nica dokÅ‚adnoÅ›ci: +3.20%
â± RÃ³Å¼nica czasu: +2225.39s (wartoÅ›Ä‡ dodatnia = wolniej ze stemizacjÄ…)


ğŸ“Š MACIERZ KONFUZJI (ZE STEMIZACJÄ„):
      spam      ham
spam  28.65%    38.07%
ham   0.09%     33.18%

ğŸ“Š MACIERZ KONFUZJI (BEZ STEMIZACJI):
      spam      ham
spam  25.45%    41.28%
ham   0.09%     33.19%

ğŸ“ Wyniki zapisano do pliku: results_stemming.txt
```

### Zadanie 4
DokonaÄ‡ klasyfikacji binarnej wiadomoÅ›ci z archiwum (zadanie 1) na spam i ham, stosujÄ…c algorytmy rozmytego haszowania.

**Uwagi:**
1. Do tego celu uÅ¼yÄ‡ algorytmu LSH (MinHash, MinHashLSH) z biblioteki datasketch.
2. Wyniki pracy algorytmu przedstawiÄ‡ przy pomocy procentowej macierzy konfuzji i wskaÅºnika accuracy.
3. SprawdziÄ‡ pracÄ™ programu dla rÃ³Å¼nych wartoÅ›ci parametru threshold funkcji MinHashLSH.
4. PorÃ³wnaÄ‡ uzyskane wyniki z wynikami z poprzednich zadaÅ„.

#### Implementacja

**1. Konfiguracja globalna**

Na wstÄ™pie programu znajduje siÄ™ kod, ktÃ³ry definiuje staÅ‚e konfiguracyjne uÅ¼ywane w caÅ‚ym programie. UÅ‚atwia to dostosowanie parametrÃ³w bez koniecznoÅ›ci modyfikowania logiki programu.

**Kod:**  
``` python
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_lsh.txt"        # nazwa pliku wynikowego

# Parametry LSH / MinHash
NUM_PERM = 128                          # liczba permutacji w MinHash
SHINGLE_SIZE = 3                        # rozmiar shingli (k-gramÃ³w)
USE_STEMMING = True                     # czy stosowaÄ‡ stemizacjÄ™
THRESHOLDS = [0.1, 0.3, 0.5, 0.7, 0.9]  # testowane progi LSH
DEFAULT_LABEL = "ham"                   # etykieta domyÅ›lna, gdy brak dopasowaÅ„ w LSH

random.seed(42)                         # ustawienie ziarna losowoÅ›ci
```

**2. Funkcja `load_index`**

**WejÅ›cie:**  
- `index_path` (string) - Å›cieÅ¼ka do pliku z indeksem wiadomoÅ›ci

**WyjÅ›cie:**  
- `entries` (list) - lista krotek zawierajÄ…cych peÅ‚nÄ… Å›cieÅ¼kÄ™ do pliku i etykietÄ™ (spam/ham)

**Opis:**  
Funkcja wczytuje plik indeksu, parsuje kaÅ¼dÄ… liniÄ™ rozdzielajÄ…c jÄ… na etykietÄ™ i Å›cieÅ¼kÄ™ do pliku. Normalizuje Å›cieÅ¼ki usuwajÄ…c "../" i tworzy peÅ‚ne Å›cieÅ¼ki wzglÄ™dem gÅ‚Ã³wnego katalogu danych. Zwraca listÄ™ wszystkich wpisÃ³w gotowych do dalszego przetwarzania.

**Kod:**  
``` python
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                # Normalizuje Å›cieÅ¼kÄ™: '../data/inmail.X' -> 'trec07p/data/inmail.X'
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries
```

---

**3. Funkcja `load_email_content`**

**WejÅ›cie:**  
- `filepath` (string) - Å›cieÅ¼ka do pliku z wiadomoÅ›ciÄ… email

**WyjÅ›cie:**  
- `payload` (string) - wyekstrahowana treÅ›Ä‡ wiadomoÅ›ci lub pusty string w przypadku bÅ‚Ä™du

**Opis:**  
Funkcja wczytuje i parsuje wiadomoÅ›Ä‡ email przy uÅ¼yciu biblioteki email. ObsÅ‚uguje zarÃ³wno wiadomoÅ›ci wieloczÄ™Å›ciowe (multipart) jak i pojedyncze. Dla wiadomoÅ›ci wieloczÄ™Å›ciowych iteruje przez wszystkie czÄ™Å›ci i wyciÄ…ga tylko te o typie tekstowym. Dekoduje zawartoÅ›Ä‡ binarnÄ… i obsÅ‚uguje bÅ‚Ä™dy kodowania. W przypadku wyjÄ…tkÃ³w zwraca pusty string.

**Kod:**  
``` python
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            payload = ""
            if msg.is_multipart():
                # zÅ‚Ä…cz wszystkie czÄ™Å›ci tekstowe
                parts = []
                for part in msg.walk():
                    # tylko tekstowe czÄ™Å›ci (ignore attachments)
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return payload or ""
    except Exception:
        return ""
```

---

**4. Funkcja `preprocess_text`**

**WejÅ›cie:**  
- `text` (string) - tekst wiadomoÅ›ci email do przetworzenia
- `use_stemming` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ stemizacjÄ™

**WyjÅ›cie:**  
- `tokens` (list) - lista przetworzonych tokenÃ³w (sÅ‚Ã³w)

**Opis:**  
Funkcja przeprowadza peÅ‚ne przetwarzanie tekstu przed uÅ¼yciem w algorytmie LSH: konwersja na maÅ‚e litery, usuwanie znakÃ³w interpunkcyjnych, tokenizacja na pojedyncze sÅ‚owa, filtrowanie tylko sÅ‚Ã³w alfabetycznych, usuwanie stopwords (sÅ‚Ã³w bez znaczenia) oraz opcjonalna stemizacja przy uÅ¼yciu algorytmu PorterStemmer.

**Kod:**  
``` python
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(t) for t in tokens]
    return tokens
```

---

**5. Funkcja `get_shingles`**

**WejÅ›cie:**  
- `tokens` (list) - lista tokenÃ³w z przetworzonej wiadomoÅ›ci
- `k` (int) - rozmiar shingli (k-gramÃ³w)

**WyjÅ›cie:**  
- `shingles` (list) - lista shingli utworzonych z ciÄ…gÅ‚ych sekwencji tokenÃ³w

**Opis:**  
Funkcja tworzy k-gramy (shingle) z ciÄ…gÅ‚ych sekwencji tokenÃ³w. Dla podanej listy tokenÃ³w tworzy wszystkie moÅ¼liwe ciÄ…gÅ‚e sekwencje o dÅ‚ugoÅ›ci k, Å‚Ä…czÄ…c je w stringi. JeÅ›li dÅ‚ugoÅ›Ä‡ tokenÃ³w jest mniejsza niÅ¼ k, zwraca oryginalne tokeny jako fallback.

**Kod:**  
``` python
def get_shingles(tokens, k=3):
    if len(tokens) < k:
        # fallback: uÅ¼yj pojedynczych tokenÃ³w
        return tokens
    shingles = []
    for i in range(len(tokens) - k + 1):
        sh = " ".join(tokens[i:i + k])
        shingles.append(sh)
    return shingles
```

---

**6. Funkcja `build_minhash_from_shingles`**

**WejÅ›cie:**  
- `shingles` (list) - lista shingli (k-gramÃ³w)
- `num_perm` (int) - liczba permutacji dla algorytmu MinHash

**WyjÅ›cie:**  
- `m` (MinHash) - obiekt MinHash reprezentujÄ…cy dokument

**Opis:**  
Funkcja tworzy obiekt MinHash dla dokumentu na podstawie jego shingli. UÅ¼ywa zestawu unikalnych shingli aby uniknÄ…Ä‡ duplikatÃ³w. KaÅ¼dy shingle jest kodowany do postaci bajtÃ³w przed dodaniem do MinHash. Parametr num_perm okreÅ›la dokÅ‚adnoÅ›Ä‡ haszowania.

**Kod:**  
``` python
def build_minhash_from_shingles(shingles, num_perm=128):
    m = MinHash(num_perm=num_perm)
    # uÅ¼ywamy zestawu, aby uniknÄ…Ä‡ wielokrotnego dodawania tego samego shingla
    for s in set(shingles):
        m.update(s.encode("utf8"))
    return m
```

---

**7. Funkcja `prepare_train_min_hashes`**

**WejÅ›cie:**  
- `train_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych treningowych
- `use_stemming` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ stemizacjÄ™
- `shingle_k` (int) - rozmiar shingli
- `num_perm` (int) - liczba permutacji dla MinHash

**WyjÅ›cie:**  
- `id_to_minhash` (dict) - sÅ‚ownik mapujÄ…cy ID dokumentu na jego MinHash
- `id_to_label` (dict) - sÅ‚ownik mapujÄ…cy ID dokumentu na jego etykietÄ™

**Opis:**  
Funkcja przetwarza wszystkie dokumenty treningowe: wczytuje treÅ›Ä‡, przetwarza tekst, tworzy shingle, buduje MinHash dla kaÅ¼dego dokumentu. Przypisuje unikalne ID kaÅ¼demu dokumentowi i zwraca dwa sÅ‚owniki do dalszego uÅ¼ycia w LSH.

**Kod:**  
``` python
def prepare_train_min_hashes(train_entries, use_stemming=True, shingle_k=3, num_perm=128):
    id_to_minhash = {}
    id_to_label = {}
    for idx, (path, label) in enumerate(train_entries):
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        doc_id = f"doc{idx}"
        id_to_minhash[doc_id] = m
        id_to_label[doc_id] = label
    return id_to_minhash, id_to_label
```

---

**8. Funkcja `classify_with_lsh`**

**WejÅ›cie:**  
- `lsh` (MinHashLSH) - obiekt LSH z wstawionymi dokumentami treningowymi
- `train_label_map` (dict) - sÅ‚ownik mapujÄ…cy ID na etykiety treningowe
- `test_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych testowych
- `use_stemming` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ stemizacjÄ™
- `shingle_k` (int) - rozmiar shingli
- `num_perm` (int) - liczba permutacji dla MinHash

**WyjÅ›cie:**  
- `y_true` (list) - lista prawdziwych etykiet
- `y_pred` (list) - lista przewidywanych etykiet

**Opis:**  
Funkcja klasyfikuje dokumenty testowe uÅ¼ywajÄ…c LSH. Dla kaÅ¼dego dokumentu testowego: przetwarza tekst, tworzy shingle, buduje MinHash, pyta LSH o podobne dokumenty. JeÅ›li znaleziono dopasowania, przeprowadza gÅ‚osowanie wiÄ™kszoÅ›ciowe na podstawie etykiet dokumentÃ³w treningowych. JeÅ›li brak dopasowaÅ„, uÅ¼ywa etykiety domyÅ›lnej.

**Kod:**  
``` python
def classify_with_lsh(lsh, train_label_map, test_entries, use_stemming=True, shingle_k=3, num_perm=128):
    y_true = []
    y_pred = []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        matches = lsh.query(m)  # lista dopasowanych dokumentÃ³w treningowych
        if matches:
            # gÅ‚osowanie wiÄ™kszoÅ›ciowe etykiet
            votes = [train_label_map[mid] for mid in matches if mid in train_label_map]
            if votes:
                counter = Counter(votes)
                pred = counter.most_common(1)[0][0]
            else:
                pred = DEFAULT_LABEL
        else:
            pred = DEFAULT_LABEL
        y_true.append(label)
        y_pred.append(pred)
    return y_true, y_pred
```

---

**9. Funkcja `main`**

**WejÅ›cie:**  
- Brak parametrÃ³w wejÅ›ciowych

**WyjÅ›cie:**  
- Brak bezpoÅ›redniego wyjÅ›cia (funkcja wykonuje program i zapisuje wyniki do pliku)

**Opis:**  
GÅ‚Ã³wna funkcja programu koordynujÄ…ca caÅ‚y proces klasyfikacji LSH: wczytuje i tasuje dane, dzieli na zbiory treningowe i testowe, przygotowuje MinHash dla danych treningowych, testuje rÃ³Å¼ne wartoÅ›ci threshold dla LSH, oblicza metryki wydajnoÅ›ci dla kaÅ¼dego threshold i zapisuje szczegÃ³Å‚owe wyniki do pliku. Dla kaÅ¼dego threshold buduje nowy indeks LSH i przeprowadza klasyfikacjÄ™.

**Kod:**  
``` python
def main():
    print("ğŸ“‚ Wczytywanie indexu i danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"âš ï¸ SAMPLE_SIZE aktywne. WykorzystujÄ™ {len(index_entries)} pierwszych wpisÃ³w")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    print(f"ÅÄ…cznie: {len(index_entries)} dokumentÃ³w; trening: {len(train_entries)}; test: {len(test_entries)}")
    results_lines = []
    results_lines.append(f"LSH MinHash results\nSAMPLE_SIZE={SAMPLE_SIZE}\nNUM_PERM={NUM_PERM}\nSHINGLE_SIZE={SHINGLE_SIZE}\nUSE_STEMMING={USE_STEMMING}\n")

    # Przygotowuje MinHash na treningu (raz). BÄ™dzie ono wstawiane do nowych LSH dla rÃ³Å¼nych thresholdÃ³w
    print("ğŸ§  Budowanie MinHash dla zbioru treningowego...")
    t0 = time.time()
    train_mh_map, train_label_map = prepare_train_min_hashes(train_entries, use_stemming=USE_STEMMING,
                                                            shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
    t_prep = time.time() - t0
    print(f"Gotowe. Czas przygotowania MinHash treningu: {t_prep:.2f}s")
    results_lines.append(f"prepare_time={t_prep:.2f}s\n")

    # Dla kaÅ¼dego threshold buduje nowy MinHashLSH (z tym samym num_perm) i wstawia minhashy treningowe
    for thresh in THRESHOLDS:
        print(f"\nğŸ” Test dla threshold = {thresh}")
        results_lines.append(f"\nTHRESHOLD={thresh}\n")
        # buduje LSH z parametrem threshold
        t0 = time.time()
        lsh = MinHashLSH(threshold=thresh, num_perm=NUM_PERM)
        # wstawia minhashy treningowe
        for doc_id, mh in train_mh_map.items():
            lsh.insert(doc_id, mh)
        build_time = time.time() - t0
        print(f"LSH zbudowano w {build_time:.2f}s")

        # klasyfikacja testÃ³w
        t1 = time.time()
        y_true, y_pred = classify_with_lsh(lsh, train_label_map, test_entries,
                                          use_stemming=USE_STEMMING, shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
        elapsed = time.time() - t1

        # metryki
        labels = ["spam", "ham"]
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        cm_percent = cm / np.sum(cm) * 100
        acc = accuracy_score(y_true, y_pred) * 100

        # raport w konsoli
        print(f"ğŸ¯ Accuracy: {acc:.2f}% | â± Czas tworzenia LSH: {build_time:.2f}s | â± Czas klasyfikacji LSH: {elapsed:.2f}s")
        print("ğŸ“Š Confusion matrix (%):")
        print(f"      spam      ham")
        print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
        print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

        # zapis wynikÃ³w
        results_lines.append(f"accuracy={acc:.2f}%\n")
        results_lines.append(f"build_time={build_time:.2f}s classify_time={elapsed:.2f}s\n")
        results_lines.append("confusion_percent:\n")
        results_lines.append(f"spam_spam={cm_percent[0,0]:6.2f}% spam_ham={cm_percent[0,1]:6.2f}%\n")
        results_lines.append(f"ham_spam={cm_percent[1,0]:6.2f}% ham_ham={cm_percent[1,1]:6.2f}%\n")

    # zapis do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_lines))

    print(f"\nğŸ“ Wyniki zapisano do: {RESULTS_FILE}")
```

---

**10. Kompletny kod**  
PoniÅ¼ej znajduje siÄ™ kompletny kod programu, ktÃ³ry moÅ¼na uruchomiÄ‡.

**Kod:**  
``` python
import os
import string
import random
import time
from email import message_from_file
from collections import Counter, defaultdict

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

from datasketch import MinHash, MinHashLSH
from sklearn.metrics import confusion_matrix, accuracy_score
import numpy as np

# === KONFIGURACJA ===
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_lsh.txt"        # nazwa pliku wynikowego

# Parametry LSH / MinHash
NUM_PERM = 128                          # liczba permutacji w MinHash
SHINGLE_SIZE = 3                        # rozmiar shingli (k-gramÃ³w)
USE_STEMMING = True                     # czy stosowaÄ‡ stemizacjÄ™
THRESHOLDS = [0.1, 0.3, 0.5, 0.7, 0.9]  # testowane progi LSH
DEFAULT_LABEL = "ham"                   # etykieta domyÅ›lna, gdy brak dopasowaÅ„ w LSH

random.seed(42)                         # ustawienie ziarna losowoÅ›ci


# === POMOCNICZE FUNKCJE ===
# wczytuje index plikÃ³w i etykiet
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                # Normalizuje Å›cieÅ¼kÄ™: '../data/inmail.X' -> 'trec07p/data/inmail.X'
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries

# Wczytuje zawartoÅ›Ä‡ e-maila i zwraca string tekstowy (ignoruje bÅ‚Ä™dy kodowania)
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            payload = ""
            if msg.is_multipart():
                # zÅ‚Ä…cz wszystkie czÄ™Å›ci tekstowe
                parts = []
                for part in msg.walk():
                    # tylko tekstowe czÄ™Å›ci (ignore attachments)
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return payload or ""
    except Exception:
        return ""


# Przetwarza tekst: czyszczenie, tokenizacja, usuwanie stopwords i (opcjonalnie) stemizacja. zwraca listÄ™ tokenÃ³w
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(t) for t in tokens]
    return tokens

# Zwraca listÄ™ shingli (k-gramÃ³w) utworzonych z tokenÃ³w (continuous k-word shingles)
def get_shingles(tokens, k=3):
    if len(tokens) < k:
        # fallback: uÅ¼yj pojedynczych tokenÃ³w
        return tokens
    shingles = []
    for i in range(len(tokens) - k + 1):
        sh = " ".join(tokens[i:i + k])
        shingles.append(sh)
    return shingles


# Zwraca MinHash obliczony na zestawie shingles (unikatowych).
def build_minhash_from_shingles(shingles, num_perm=128):
    m = MinHash(num_perm=num_perm)
    # uÅ¼ywamy zestawu, aby uniknÄ…Ä‡ wielokrotnego dodawania tego samego shingla
    for s in set(shingles):
        m.update(s.encode("utf8"))
    return m


# === PROCEDURY TRENING / TEST ===
# Przygotowuje MinHash dla kaÅ¼dego dokumentu treningowego i mapuje identyfikatory na etykiety. Zwraca dwa sÅ‚owniki 
def prepare_train_min_hashes(train_entries, use_stemming=True, shingle_k=3, num_perm=128):
    id_to_minhash = {}
    id_to_label = {}
    for idx, (path, label) in enumerate(train_entries):
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        doc_id = f"doc{idx}"
        id_to_minhash[doc_id] = m
        id_to_label[doc_id] = label
    return id_to_minhash, id_to_label


# Dla kaÅ¼dego dokumentu testowego: oblicza MinHash, pyta LSH o dopasowania, jeÅ›li lista niepusta - dokonuje gÅ‚osowania etykiet (majority vote), jeÅ›li pusta - przypisuje DEFAULT_LABEL
def classify_with_lsh(lsh, train_label_map, test_entries, use_stemming=True, shingle_k=3, num_perm=128):
    y_true = []
    y_pred = []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        matches = lsh.query(m)  # lista dopasowanych dokumentÃ³w treningowych
        if matches:
            # gÅ‚osowanie wiÄ™kszoÅ›ciowe etykiet
            votes = [train_label_map[mid] for mid in matches if mid in train_label_map]
            if votes:
                counter = Counter(votes)
                pred = counter.most_common(1)[0][0]
            else:
                pred = DEFAULT_LABEL
        else:
            pred = DEFAULT_LABEL
        y_true.append(label)
        y_pred.append(pred)
    return y_true, y_pred


# === GÅÃ“WNY PROGRAM ===
def main():
    print("ğŸ“‚ Wczytywanie indexu i danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"âš ï¸ SAMPLE_SIZE aktywne. WykorzystujÄ™ {len(index_entries)} pierwszych wpisÃ³w")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    print(f"ÅÄ…cznie: {len(index_entries)} dokumentÃ³w; trening: {len(train_entries)}; test: {len(test_entries)}")
    results_lines = []
    results_lines.append(f"LSH MinHash results\nSAMPLE_SIZE={SAMPLE_SIZE}\nNUM_PERM={NUM_PERM}\nSHINGLE_SIZE={SHINGLE_SIZE}\nUSE_STEMMING={USE_STEMMING}\n")

    # Przygotowuje MinHash na treningu (raz). BÄ™dzie ono wstawiane do nowych LSH dla rÃ³Å¼nych thresholdÃ³w
    print("ğŸ§  Budowanie MinHash dla zbioru treningowego...")
    t0 = time.time()
    train_mh_map, train_label_map = prepare_train_min_hashes(train_entries, use_stemming=USE_STEMMING,
                                                            shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
    t_prep = time.time() - t0
    print(f"Gotowe. Czas przygotowania MinHash treningu: {t_prep:.2f}s")
    results_lines.append(f"prepare_time={t_prep:.2f}s\n")

    # Dla kaÅ¼dego threshold buduje nowy MinHashLSH (z tym samym num_perm) i wstawia minhashy treningowe
    for thresh in THRESHOLDS:
        print(f"\nğŸ” Test dla threshold = {thresh}")
        results_lines.append(f"\nTHRESHOLD={thresh}\n")
        # buduje LSH z parametrem threshold
        t0 = time.time()
        lsh = MinHashLSH(threshold=thresh, num_perm=NUM_PERM)
        # wstawia minhashy treningowe
        for doc_id, mh in train_mh_map.items():
            lsh.insert(doc_id, mh)
        build_time = time.time() - t0
        print(f"LSH zbudowano w {build_time:.2f}s")

        # klasyfikacja testÃ³w
        t1 = time.time()
        y_true, y_pred = classify_with_lsh(lsh, train_label_map, test_entries,
                                          use_stemming=USE_STEMMING, shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
        elapsed = time.time() - t1

        # metryki
        labels = ["spam", "ham"]
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        cm_percent = cm / np.sum(cm) * 100
        acc = accuracy_score(y_true, y_pred) * 100

        # raport w konsoli
        print(f"ğŸ¯ Accuracy: {acc:.2f}% | â± Czas tworzenia LSH: {build_time:.2f}s | â± Czas klasyfikacji LSH: {elapsed:.2f}s")
        print("ğŸ“Š Confusion matrix (%):")
        print(f"      spam      ham")
        print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
        print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

        # zapis wynikÃ³w
        results_lines.append(f"accuracy={acc:.2f}%\n")
        results_lines.append(f"build_time={build_time:.2f}s classify_time={elapsed:.2f}s\n")
        results_lines.append("confusion_percent:\n")
        results_lines.append(f"spam_spam={cm_percent[0,0]:6.2f}% spam_ham={cm_percent[0,1]:6.2f}%\n")
        results_lines.append(f"ham_spam={cm_percent[1,0]:6.2f}% ham_ham={cm_percent[1,1]:6.2f}%\n")

    # zapis do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_lines))

    print(f"\nğŸ“ Wyniki zapisano do: {RESULTS_FILE}")


if __name__ == "__main__":
    main()
```

#### Wyniki

``` text
ğŸ“‚ Wczytywanie indexu i danych...
ÅÄ…cznie: 75419 dokumentÃ³w; trening: 60335; test: 15084
ğŸ§  Budowanie MinHash dla zbioru treningowego...
Gotowe. Czas przygotowania MinHash treningu: 452.07s

ğŸ” Test dla threshold = 0.1
LSH zbudowano w 9.47s
ğŸ¯ Accuracy: 94.50% | â± Czas tworzenia LSH: 9.47s | â± Czas klasyfikacji LSH: 129.05s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  60.55%    5.32%
ham   0.18%     33.94%

ğŸ” Test dla threshold = 0.3
LSH zbudowano w 6.92s
ğŸ¯ Accuracy: 88.80% | â± Czas tworzenia LSH: 6.92s | â± Czas klasyfikacji LSH: 120.82s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  54.76%    11.12%
ham   0.09%     34.04%

ğŸ” Test dla threshold = 0.5
LSH zbudowano w 4.78s
ğŸ¯ Accuracy: 79.14% | â± Czas tworzenia LSH: 4.78s | â± Czas klasyfikacji LSH: 118.51s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  45.09%    20.79%
ham   0.07%     34.06%

ğŸ” Test dla threshold = 0.7
LSH zbudowano w 3.11s
ğŸ¯ Accuracy: 70.72% | â± Czas tworzenia LSH: 3.11s | â± Czas klasyfikacji LSH: 116.56s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  36.65%    29.23%
ham   0.05%     34.07%

ğŸ” Test dla threshold = 0.9
LSH zbudowano w 1.53s
ğŸ¯ Accuracy: 62.70% | â± Czas tworzenia LSH: 1.53s | â± Czas klasyfikacji LSH: 115.94s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  28.61%    37.26%
ham   0.04%     34.08%

ğŸ“ Wyniki zapisano do: results_lsh.txt
```

### Zadanie 5
DokonaÄ‡ klasyfikacji binarnej wiadomoÅ›ci z archiwum (zadanie 1) na spam i ham, stosujÄ…c algorytm Naive Bayes.

**Uwagi:**
1. Do realizacji zadania naleÅ¼y uÅ¼yÄ‡ implementacji algorytmu z biblioteki Scikit-learn. Algorytm dostÄ™pny jest poprzez obiekt MultinomialNB.
2. PorÃ³wnaÄ‡ dziaÅ‚anie algorytmu dla przypadkÃ³w:
   - algorytm pracuje na caÅ‚ych tematach i ciele wiadomoÅ›ci w postaci zwykÅ‚ego tekstu bez usuwania sÅ‚Ã³w przestankowych i stemizacji przy pomocy narzÄ™dzi z biblioteki NLTK.
   - algorytm pracuje na bazie stemizowanych danych z usuniÄ™tymi sÅ‚owami przestankowymi.
1. Uzyskane wyniki przedstawiÄ‡ przy pomocy macierzy konfuzji i wskaÅºnika accuracy.
2. PorÃ³wnaÄ‡ uzyskane wyniki do wynikÃ³w uzyskanych przy zastosowaniu metod z poprzednich zadaÅ„.

#### Implementacja

**1. Konfiguracja globalna**

Na wstÄ™pie programu znajduje siÄ™ kod, ktÃ³ry definiuje staÅ‚e konfiguracyjne uÅ¼ywane w caÅ‚ym programie. UÅ‚atwia to dostosowanie parametrÃ³w bez koniecznoÅ›ci modyfikowania logiki programu.

**Kod:**  
``` python
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_naive_bayes.txt"# nazwa pliku wynikowego

random.seed(42)                         # ustawienie ziarna losowoÅ›ci
```

**2. Funkcja `load_index`**

**WejÅ›cie:**  
- `index_path` (string) - Å›cieÅ¼ka do pliku z indeksem wiadomoÅ›ci

**WyjÅ›cie:**  
- `entries` (list) - lista krotek zawierajÄ…cych peÅ‚nÄ… Å›cieÅ¼kÄ™ do pliku i etykietÄ™ (spam/ham)

**Opis:**  
Funkcja wczytuje plik indeksu TREC07P, parsuje kaÅ¼dÄ… liniÄ™ rozdzielajÄ…c jÄ… na etykietÄ™ (spam/ham) i Å›cieÅ¼kÄ™ do pliku. Tworzy peÅ‚ne Å›cieÅ¼ki do plikÃ³w przez poÅ‚Ä…czenie Å›cieÅ¼ki bazowej DATA_PATH ze Å›cieÅ¼kÄ… z indeksu (po usuniÄ™ciu "../"). Zwraca listÄ™ wszystkich wpisÃ³w gotowych do przetwarzania.

**Kod:**  
``` python
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries
```

---

**3. Funkcja `load_email_content`**

**WejÅ›cie:**  
- `filepath` (string) - Å›cieÅ¼ka do pliku z wiadomoÅ›ciÄ… email

**WyjÅ›cie:**  
- `text` (string) - poÅ‚Ä…czony temat i treÅ›Ä‡ wiadomoÅ›ci lub pusty string w przypadku bÅ‚Ä™du

**Opis:**  
Funkcja wczytuje i parsuje wiadomoÅ›Ä‡ email, wyciÄ…gajÄ…c zarÃ³wno temat (Subject) jak i treÅ›Ä‡ wiadomoÅ›ci. ObsÅ‚uguje wiadomoÅ›ci wieloczÄ™Å›ciowe (multipart) - iteruje przez wszystkie czÄ™Å›ci i wyciÄ…ga tylko te o typie tekstowym. ÅÄ…czy temat z treÅ›ciÄ… w jeden string, co zapewnia, Å¼e algorytm Naive Bayes bÄ™dzie wykorzystywaÅ‚ caÅ‚Ä… dostÄ™pnÄ… informacjÄ™ tekstowÄ….

**Kod:**  
``` python
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            subject = msg.get("Subject", "")
            payload = ""
            if msg.is_multipart():
                parts = []
                for part in msg.walk():
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return subject + " " + payload
    except Exception:
        return ""
```

---

**4. Funkcja `preprocess_text`**

**WejÅ›cie:**  
- `text` (string) - tekst wiadomoÅ›ci email do przetworzenia

**WyjÅ›cie:**  
- `text` (string) - przetworzony tekst po stemizacji i usuniÄ™ciu stopwords

**Opis:**  
Funkcja przeprowadza peÅ‚ne przetwarzanie tekstu NLTK: konwersja na maÅ‚e litery, usuwanie znakÃ³w interpunkcyjnych, tokenizacja na pojedyncze sÅ‚owa, filtrowanie tylko sÅ‚Ã³w alfabetycznych, usuwanie stopwords (sÅ‚Ã³w bez znaczenia) oraz stemizacja przy uÅ¼yciu algorytmu PorterStemmer. Na koÅ„cu Å‚Ä…czy tokeny z powrotem w string dla kompatybilnoÅ›ci z CountVectorizer.

**Kod:**  
``` python
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(t) for t in tokens]
    return " ".join(tokens)
```

---

**5. Funkcja `prepare_data`**

**WejÅ›cie:**  
- `entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) do przetworzenia
- `use_preprocessing` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ przetwarzanie NLTK

**WyjÅ›cie:**  
- `texts` (list) - lista tekstÃ³w wiadomoÅ›ci (przetworzonych lub nie)
- `labels` (list) - lista etykiet (spam/ham)

**Opis:**  
Funkcja przetwarza wszystkie dokumenty z podanej listy. Dla kaÅ¼dego dokumentu wczytuje treÅ›Ä‡ emaila i opcjonalnie stosuje preprocessing NLTK w zaleÅ¼noÅ›ci od parametru `use_preprocessing`. Zwraca dwie listy: tekstÃ³w przygotowanych do wektoryzacji oraz odpowiadajÄ…cych im etykiet.

**Kod:**  
``` python
def prepare_data(entries, use_preprocessing=False):
    texts, labels = [], []
    for path, label in entries:
        text = load_email_content(path)
        if use_preprocessing:
            text = preprocess_text(text)
        texts.append(text)
        labels.append(label)
    return texts, labels
```

---

**6. Funkcja `run_naive_bayes`**

**WejÅ›cie:**  
- `train_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych treningowych
- `test_entries` (list) - lista krotek (Å›cieÅ¼ka, etykieta) dla danych testowych
- `use_preprocessing` (bool) - flaga okreÅ›lajÄ…ca czy stosowaÄ‡ przetwarzanie NLTK

**WyjÅ›cie:**  
- `acc` (float) - dokÅ‚adnoÅ›Ä‡ klasyfikacji w procentach
- `cm_percent` (numpy.ndarray) - macierz konfuzji w procentach
- `elapsed` (float) - czas wykonania w sekundach

**Opis:**  
Funkcja przeprowadza peÅ‚ny eksperyment z klasyfikatorem Naive Bayes: przygotowuje dane treningowe i testowe, tworzy macierz cech przy uÅ¼yciu CountVectorizer (bag-of-words), trenuje model MultinomialNB, dokonuje predykcji na danych testowych i oblicza metryki wydajnoÅ›ci. WyÅ›wietla szczegÃ³Å‚owe wyniki w konsoli i zwraca wartoÅ›ci do dalszej analizy.

**Kod:**  
``` python
def run_naive_bayes(train_entries, test_entries, use_preprocessing=False):
    print(f"\nğŸ§  Uruchamianie Naive Bayes ({'z preprocessingiem' if use_preprocessing else 'Bez preprocessingu'})...")
    start_time = time.time()

    # Przygotowanie danych
    X_train_texts, y_train = prepare_data(train_entries, use_preprocessing)
    X_test_texts, y_test = prepare_data(test_entries, use_preprocessing)

    # Konwersja do macierzy cech (bag of words)
    vectorizer = CountVectorizer()
    X_train = vectorizer.fit_transform(X_train_texts)
    X_test = vectorizer.transform(X_test_texts)

    # Trening
    model = MultinomialNB()
    model.fit(X_train, y_train)

    # Predykcja
    y_pred = model.predict(X_test)

    # Ewaluacja
    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_test, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_test, y_pred) * 100

    # WyÅ›wietlenie wynikÃ³w
    print(f"ğŸ¯ Accuracy: {acc:.2f}% | â± Czas wykonania: {elapsed:.2f}s")
    print("ğŸ“Š Confusion matrix (%):")
    print(f"      spam      ham")
    print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
    print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

    return acc, cm_percent, elapsed
```

---

**7. Funkcja `main`**

**WejÅ›cie:**  
- Brak parametrÃ³w wejÅ›ciowych

**WyjÅ›cie:**  
- Brak bezpoÅ›redniego wyjÅ›cia (funkcja wykonuje program i zapisuje wyniki do pliku)

**Opis:**  
GÅ‚Ã³wna funkcja programu koordynujÄ…ca eksperymenty z Naive Bayes: wczytuje i tasuje dane, dzieli na zbiory treningowe i testowe, przeprowadza dwa eksperymenty (bez przetwarzania tekstu i z peÅ‚nym przetwarzaniem NLTK), porÃ³wnuje wyniki pod wzglÄ™dem accuracy i macierzy konfuzji, oraz zapisuje szczegÃ³Å‚owe wyniki do pliku tekstowego. Eksperymenty pozwalajÄ… na porÃ³wnanie wpÅ‚ywu preprocessingu na skutecznoÅ›Ä‡ klasyfikacji.

**Kod:**  
``` python
def main():
    print("ğŸ“‚ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"âš ï¸ SAMPLE_SIZE aktywne. WykorzystujÄ™ {len(index_entries)} pierwszych wpisÃ³w")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]
    print(f"ÅÄ…cznie: {len(index_entries)} dokumentÃ³w; trening: {len(train_entries)}; test: {len(test_entries)}")

    # Wyniki
    results = []

    # Wersja bez preprocessingu (peÅ‚ny tekst)
    acc_raw, cm_raw, t_raw = run_naive_bayes(train_entries, test_entries, use_preprocessing=False)
    results.append(("Bez preprocessingu", acc_raw, cm_raw, t_raw))

    # Wersja z preprocessingiem (usuwanie stopwords i stemizacja)
    acc_clean, cm_clean, t_clean = run_naive_bayes(train_entries, test_entries, use_preprocessing=True)
    results.append(("Z preprocessingiem (NLTK)", acc_clean, cm_clean, t_clean))

    # Zapis wynikÃ³w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("Naive Bayes Results\n\n")
        for title, acc, cm, t in results:
            f.write(f"{title}\n")
            f.write(f"Accuracy: {acc:.2f}%\nCzas: {t:.2f}s\n")
            f.write("Confusion matrix (%):\n")
            f.write(f"spam_spam={cm[0,0]:.2f}% spam_ham={cm[0,1]:.2f}%\n")
            f.write(f"ham_spam={cm[1,0]:.2f}% ham_ham={cm[1,1]:.2f}%\n\n")

    print(f"\nğŸ“ Wyniki zapisano do: {RESULTS_FILE}")
```

---

**8. Kompletny kod**  
PoniÅ¼ej znajduje siÄ™ kompletny kod programu, ktÃ³ry moÅ¼na uruchomiÄ‡.

**Kod:**  
``` python
import os
import string
import random
import time
from email import message_from_file

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, accuracy_score
import numpy as np

# === KONFIGURACJA ===
INDEX_PATH = "trec07p/full/index"       # Å›cieÅ¼ka do indexu
DATA_PATH = "trec07p"                   # Å›cieÅ¼ka do danych
TRAIN_RATIO = 0.8                       # stosunek danych treningowych do testowych
SAMPLE_SIZE = None                      # ograniczenie liczby prÃ³bek, np. 2000 dla testÃ³w, None = caÅ‚oÅ›Ä‡
RESULTS_FILE = "results_naive_bayes.txt"# nazwa pliku wynikowego

random.seed(42)                         # ustawienie ziarna losowoÅ›ci


# === POMOCNICZE FUNKCJE ===
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries

# Wczytuje treÅ›Ä‡ e-maila (temat + ciaÅ‚o) jako zwykÅ‚y tekst
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            subject = msg.get("Subject", "")
            payload = ""
            if msg.is_multipart():
                parts = []
                for part in msg.walk():
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return subject + " " + payload
    except Exception:
        return ""

# Usuwa interpunkcjÄ™, stopwords i dokonuje stemizacji
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(t) for t in tokens]
    return " ".join(tokens)

# Zwraca listÄ™ tekstÃ³w i etykiet (spam/ham), z opcjonalnym preprocessingiem.
def prepare_data(entries, use_preprocessing=False):
    texts, labels = [], []
    for path, label in entries:
        text = load_email_content(path)
        if use_preprocessing:
            text = preprocess_text(text)
        texts.append(text)
        labels.append(label)
    return texts, labels


# === FUNKCJA EKSPERYMENTU ===
#  Trenuje i testuje klasyfikator MultinomialNB dla zbioru TREC07P. Zwraca accuracy, macierz konfuzji i czas wykonania.
def run_naive_bayes(train_entries, test_entries, use_preprocessing=False):
    print(f"\nğŸ§  Uruchamianie Naive Bayes ({'z preprocessingiem' if use_preprocessing else 'Bez preprocessingu'})...")
    start_time = time.time()

    # Przygotowanie danych
    X_train_texts, y_train = prepare_data(train_entries, use_preprocessing)
    X_test_texts, y_test = prepare_data(test_entries, use_preprocessing)

    # Konwersja do macierzy cech (bag of words)
    vectorizer = CountVectorizer()
    X_train = vectorizer.fit_transform(X_train_texts)
    X_test = vectorizer.transform(X_test_texts)

    # Trening
    model = MultinomialNB()
    model.fit(X_train, y_train)

    # Predykcja
    y_pred = model.predict(X_test)

    # Ewaluacja
    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_test, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_test, y_pred) * 100

    # WyÅ›wietlenie wynikÃ³w
    print(f"ğŸ¯ Accuracy: {acc:.2f}% | â± Czas wykonania: {elapsed:.2f}s")
    print("ğŸ“Š Confusion matrix (%):")
    print(f"      spam      ham")
    print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
    print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

    return acc, cm_percent, elapsed


# === GÅÃ“WNY PROGRAM ===
def main():
    print("ğŸ“‚ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"âš ï¸ SAMPLE_SIZE aktywne. WykorzystujÄ™ {len(index_entries)} pierwszych wpisÃ³w")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]
    print(f"ÅÄ…cznie: {len(index_entries)} dokumentÃ³w; trening: {len(train_entries)}; test: {len(test_entries)}")

    # Wyniki
    results = []

    # Wersja bez preprocessingu (peÅ‚ny tekst)
    acc_raw, cm_raw, t_raw = run_naive_bayes(train_entries, test_entries, use_preprocessing=False)
    results.append(("Bez preprocessingu", acc_raw, cm_raw, t_raw))

    # Wersja z preprocessingiem (usuwanie stopwords i stemizacja)
    acc_clean, cm_clean, t_clean = run_naive_bayes(train_entries, test_entries, use_preprocessing=True)
    results.append(("Z preprocessingiem (NLTK)", acc_clean, cm_clean, t_clean))

    # Zapis wynikÃ³w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("Naive Bayes Results\n\n")
        for title, acc, cm, t in results:
            f.write(f"{title}\n")
            f.write(f"Accuracy: {acc:.2f}%\nCzas: {t:.2f}s\n")
            f.write("Confusion matrix (%):\n")
            f.write(f"spam_spam={cm[0,0]:.2f}% spam_ham={cm[0,1]:.2f}%\n")
            f.write(f"ham_spam={cm[1,0]:.2f}% ham_ham={cm[1,1]:.2f}%\n\n")

    print(f"\nğŸ“ Wyniki zapisano do: {RESULTS_FILE}")

if __name__ == "__main__":
    main()
```


#### Wyniki

``` text
ğŸ“‚ Wczytywanie danych...
ÅÄ…cznie: 75419 dokumentÃ³w; trening: 60335; test: 15084

ğŸ§  Uruchamianie Naive Bayes (Bez preprocessingu)...
ğŸ¯ Accuracy: 99.24% | â± Czas wykonania: 69.66s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  65.46%    0.42%
ham   0.34%     33.78%

ğŸ§  Uruchamianie Naive Bayes (z preprocessingiem)...
ğŸ¯ Accuracy: 98.72% | â± Czas wykonania: 381.12s
ğŸ“Š Confusion matrix (%):
      spam      ham
spam  64.85%    1.03%
ham   0.25%     33.87%

ğŸ“ Wyniki zapisano do: results_naive_bayes.txt
```

