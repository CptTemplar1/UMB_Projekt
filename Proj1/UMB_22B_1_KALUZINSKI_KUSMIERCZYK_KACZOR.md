# Uczenie Maszynowe w Bezpiecze≈Ñstwie
## Projekt 1
### Grupa 22B
### Autorzy: Przemys≈Çaw Ka≈Çu≈ºi≈Ñski, Jakub Ku≈õmierczyk, Micha≈Ç Kaczor

### Zadanie 1
Pobraƒá, rozpakowaƒá i przeanalizowaƒá strukturƒô plik√≥w i katalog√≥w archiwum zawierajƒÖcego wiadomo≈õci poczty elektronicznej.  
Dane te dostƒôpne sƒÖ pod adresem:  
https://plg.uwaterloo.ca/~gvcormac/treccorpus07/  
**Uwaga.** Nie nale≈ºy otwieraƒá plik√≥w z archiwum ani w przeglƒÖdarce HTML ani w programie pocztowym!

#### Wyniki

### Zadanie 2
WykorzystujƒÖc informacje z wyk≈Çadu oraz stosujƒÖc technikƒô zakazanych s≈Ç√≥w kluczowych (blacklist), dokonaƒá klasyfikacji binarnej wiadomo≈õci z archiwum z podzia≈Çem na: spam (wiadomo≈õci typu spam) oraz ham (wiadomo≈õci po≈ºƒÖdane).

**Uwagi:**
1. Przed przystƒÖpieniem do procesu klasyfikacji usunƒÖƒá z wiadomo≈õci stopping words (np. the, is, are, . . . ),
dokonaƒá stemizacji s≈Ç√≥w w wiadomo≈õciach oraz ekstrakcji token√≥w.
2. Do realizacji zadania u≈ºyƒá jƒôzyka Python oraz bibliotek: string, email, NLTK, os.
3. Zbi√≥r zakazanych s≈Ç√≥w kluczowych powinien byƒá wygenerowany na podstawie danych z podzbioru treningowego,
natomiast ewaluacja danych uzyskanych z podzbioru testowego.
4. Wynikiem ewaluacji powinna byƒá macierz konfuzji (procentowa) oraz warto≈õƒá wska≈∫nika accuracy, r√≥wnie≈º w
postaci procentowej.

#### Implementacja

Kod jest taki sam dla zada≈Ñ 2 i 3

#### Wyniki

### Zadanie 3
Zweryfikowaƒá wp≈Çyw stemizacji na pracƒô algorytmu zadania drugiego a nastƒôpnie por√≥wnaƒá uzyskane wyniki.

#### Implementacja

**1. Konfiguracja globalna**

...

**Kod:**
``` python
INDEX_PATH = "trec07p/full/index"
DATA_PATH = "trec07p"
TRAIN_RATIO = 0.8
TOP_N = 100        # liczba s≈Ç√≥w w blacklist
SAMPLE_SIZE = None # ograniczenie liczby pr√≥bek, np. 2000 dla test√≥w, None = ca≈Ço≈õƒá
RESULTS_FILE = "results_stemming.txt"
```

**2. Funkcja `load_index`**

**Wej≈õcie:**  
- `index_path` (string) - ≈õcie≈ºka do pliku z indeksem wiadomo≈õci

**Wyj≈õcie:**  
- `entries` (list) - lista krotek zawierajƒÖcych pe≈ÇnƒÖ ≈õcie≈ºkƒô do pliku i etykietƒô (spam/ham)

**Opis:**  
Funkcja wczytuje plik indeksu, gdzie ka≈ºda linia zawiera etykietƒô (spam/ham) i ≈õcie≈ºkƒô do pliku z wiadomo≈õciƒÖ. Parsuje ka≈ºdƒÖ liniƒô, tworzy pe≈ÇnƒÖ ≈õcie≈ºkƒô do pliku (usuwajƒÖc "../" z oryginalnej ≈õcie≈ºki) i zwraca listƒô wszystkich wpis√≥w.

**Kod:**
``` python
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            label, path = line.strip().split()
            full_path = os.path.join(DATA_PATH, path.replace("../", ""))
            entries.append((full_path, label))
    return entries
```

---

**3. Funkcja `preprocess_text`**

**Wej≈õcie:**  
- `text` (string) - tekst wiadomo≈õci email do przetworzenia
- `use_stemming` (bool) - flaga okre≈õlajƒÖca czy stosowaƒá stemizacjƒô

**Wyj≈õcie:**  
- `tokens` (list) - lista przetworzonych token√≥w (s≈Ç√≥w)

**Opis:**  
Funkcja przeprowadza pe≈Çne przetwarzanie tekstu: konwersja na ma≈Çe litery, usuwanie znak√≥w interpunkcyjnych, tokenizacja na pojedyncze s≈Çowa, usuwanie stopwords (s≈Ç√≥w bez znaczenia) oraz opcjonalna stemizacja przy u≈ºyciu algorytmu PorterStemmer. Zwraca listƒô oczyszczonych token√≥w.

**Kod:**
``` python
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if w not in stop_words and w.isalpha()]

    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(w) for w in tokens]

    return tokens
```

---

**4. Funkcja `load_email_content`**

**Wej≈õcie:**  
- `filepath` (string) - ≈õcie≈ºka do pliku z wiadomo≈õciƒÖ email

**Wyj≈õcie:**  
- `text` (string) - wyekstrahowana tre≈õƒá wiadomo≈õci lub pusty string w przypadku b≈Çƒôdu

**Opis:**  
Funkcja wczytuje i parsuje wiadomo≈õƒá email przy u≈ºyciu biblioteki email. Obs≈Çuguje wiadomo≈õci wieloczƒô≈õciowe (multipart), dekoduje zawarto≈õƒá i zwraca czysty tekst wiadomo≈õci. W przypadku b≈Çƒôd√≥w zwraca pusty string.

**Kod:**
``` python
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            if msg.is_multipart():
                parts = [p.get_payload(decode=True) for p in msg.get_payload() if p.get_payload()]
                text = " ".join([str(p) for p in parts])
            else:
                text = msg.get_payload(decode=True)
            if text:
                text = text.decode(errors="ignore") if isinstance(text, bytes) else text
                return text
            else:
                return ""
    except Exception:
        return ""
```

---

**5. Funkcja `build_blacklist`**

**Wej≈õcie:**  
- `train_data` (list) - lista krotek (tokens, label) z danych treningowych
- `top_n` (int) - liczba s≈Ç√≥w do umieszczenia na blackli≈õcie

**Wyj≈õcie:**  
- `blacklist` (list) - lista s≈Ç√≥w kluczowych najbardziej charakterystycznych dla spamu

**Opis:**  
Funkcja analizuje dane treningowe, zliczajƒÖc wystƒÖpienia s≈Ç√≥w w spamie i hamie. Dla ka≈ºdego s≈Çowa oblicza stosunek czƒôstotliwo≈õci w spamie do czƒôstotliwo≈õci w hamie. Zwraca listƒô `top_n` s≈Ç√≥w z najwy≈ºszym stosunkiem, kt√≥re bƒôdƒÖ u≈ºywane jako zakazane s≈Çowa kluczowe.

**Kod:**
``` python
def build_blacklist(train_data, top_n=100):
    spam_words = {}
    ham_words = {}
    for tokens, label in train_data:
        for token in tokens:
            if label == "spam":
                spam_words[token] = spam_words.get(token, 0) + 1
            else:
                ham_words[token] = ham_words.get(token, 0) + 1

    spam_ratio = {word: spam_words[word] / (ham_words.get(word, 0) + 1) for word in spam_words}
    sorted_words = sorted(spam_ratio.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:top_n]]
```

---

**6. Funkcja `classify_email`**

**Wej≈õcie:**  
- `tokens` (list) - lista token√≥w z przetworzonej wiadomo≈õci
- `blacklist` (list) - lista zakazanych s≈Ç√≥w kluczowych

**Wyj≈õcie:**  
- `"spam"` lub `"ham"` (string) - wynik klasyfikacji

**Opis:**  
Funkcja klasyfikuje wiadomo≈õƒá jako spam, je≈õli kt√≥rykolwiek z token√≥w znajduje siƒô na blackli≈õcie. W przeciwnym przypadku klasyfikuje jako ham. Jest to prosty klasyfikator oparty na zasadzie "czarnej listy".

**Kod:**  
``` python
def classify_email(tokens, blacklist):
    return "spam" if any(word in blacklist for word in tokens) else "ham"
```

---

**7. Funkcja `evaluate_model`**

**Wej≈õcie:**  
- `train_entries` (list) - lista krotek (≈õcie≈ºka, etykieta) dla danych treningowych
- `test_entries` (list) - lista krotek (≈õcie≈ºka, etykieta) dla danych testowych
- `use_stemming` (bool) - flaga okre≈õlajƒÖca czy stosowaƒá stemizacjƒô

**Wyj≈õcie:**  
- `acc` (float) - dok≈Çadno≈õƒá klasyfikacji w procentach
- `cm_percent` (numpy.ndarray) - macierz konfuzji w procentach
- `elapsed` (float) - czas wykonania w sekundach

**Opis:**  
Funkcja przeprowadza pe≈Çny proces uczenia i ewaluacji modelu: przetwarza dane treningowe, buduje blacklistƒô, klasyfikuje wiadomo≈õci testowe i oblicza metryki wydajno≈õci. Zwraca accuracy, macierz konfuzji i czas wykonania.

**Kod:**
``` python
def evaluate_model(train_entries, test_entries, use_stemming):
    start_time = time.time()

    train_data = []
    for path, label in train_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        train_data.append((tokens, label))

    blacklist = build_blacklist(train_data, top_n=TOP_N)

    y_true, y_pred = [], []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        prediction = classify_email(tokens, blacklist)
        y_true.append(label)
        y_pred.append(prediction)

    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_true, y_pred) * 100

    return acc, cm_percent, elapsed
```

---

**8. Funkcja `main`**

**Wej≈õcie:**  
- Brak parametr√≥w wej≈õciowych

**Wyj≈õcie:**  
- Brak bezpo≈õredniego wyj≈õcia (funkcja wykonuje program i zapisuje wyniki do pliku)

**Opis:**  
G≈Ç√≥wna funkcja programu, kt√≥ra koordynuje ca≈Çy proces: wczytuje i tasuje dane, dzieli na zbi√≥r treningowy i testowy, przeprowadza dwa eksperymenty (ze stemizacjƒÖ i bez), por√≥wnuje wyniki, wy≈õwietla raport i zapisuje wyniki do pliku tekstowego.

**Kod:**
``` python
def main():
    print("üìÇ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    results_log = []

    # Test 1: ZE STEMIZACJƒÑ
    print("üß† Test 1: Z STEMIZACJƒÑ")
    acc_stem, cm_stem, time_stem = evaluate_model(train_entries, test_entries, use_stemming=True)
    print(f"üéØ Accuracy (stem): {acc_stem:.2f}% | ‚è± Czas: {time_stem:.2f}s")
    results_log.append(f"Test 1 (z stemizacjƒÖ): accuracy={acc_stem:.2f}%, czas={time_stem:.2f}s")

    # Test 2: BEZ STEMIZACJI
    print("\nüß† Test 2: BEZ STEMIZACJI")
    acc_no_stem, cm_no_stem, time_no_stem = evaluate_model(train_entries, test_entries, use_stemming=False)
    print(f"üéØ Accuracy (no stem): {acc_no_stem:.2f}% | ‚è± Czas: {time_no_stem:.2f}s")
    results_log.append(f"Test 2 (bez stemizacji): accuracy={acc_no_stem:.2f}%, czas={time_no_stem:.2f}s")

    # Por√≥wnanie wynik√≥w
    diff_acc = acc_stem - acc_no_stem
    diff_time = time_stem - time_no_stem

    summary = (
        "\nüìä POR√ìWNANIE WYNIK√ìW\n"
        f"Z stemizacjƒÖ:    {acc_stem:.2f}% ({time_stem:.2f}s)\n"
        f"Bez stemizacji:  {acc_no_stem:.2f}% ({time_no_stem:.2f}s)\n"
        f"üß© R√≥≈ºnica dok≈Çadno≈õci: {diff_acc:+.2f}%\n"
        f"‚è± R√≥≈ºnica czasu: {diff_time:+.2f}s (warto≈õƒá dodatnia = wolniej ze stemizacjƒÖ)\n"
    )

    print(summary)
    results_log.append(summary)

    # Macierze konfuzji
    matrix_report = (
        "\nüìä MACIERZ KONFUZJI (Z STEMIZACJƒÑ):\n"
        f"      spam      ham\n"
        f"spam  {cm_stem[0,0]:6.2f}%   {cm_stem[0,1]:6.2f}%\n"
        f"ham   {cm_stem[1,0]:6.2f}%   {cm_stem[1,1]:6.2f}%\n\n"
        "üìä MACIERZ KONFUZJI (BEZ STEMIZACJI):\n"
        f"      spam      ham\n"
        f"spam  {cm_no_stem[0,0]:6.2f}%   {cm_no_stem[0,1]:6.2f}%\n"
        f"ham   {cm_no_stem[1,0]:6.2f}%   {cm_no_stem[1,1]:6.2f}%\n"
    )
    print(matrix_report)
    results_log.append(matrix_report)

    # Zapis wynik√≥w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_log))

    print(f"üìÅ Wyniki zapisano do pliku: {RESULTS_FILE}")
```

---

**9. Kompletny kod**  
Poni≈ºej znajduje siƒô kompletny kod programu, kt√≥ry mo≈ºna uruchomiƒá.

**Kod:**
``` python
import os
import string
import random
import time
from email import message_from_file
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.metrics import confusion_matrix, accuracy_score
import numpy as np

# === KONFIGURACJA ===
INDEX_PATH = "trec07p/full/index"
DATA_PATH = "trec07p"
TRAIN_RATIO = 0.8
TOP_N = 100        # liczba s≈Ç√≥w w blacklist
SAMPLE_SIZE = None # ograniczenie liczby pr√≥bek, np. 2000 dla test√≥w, None = ca≈Ço≈õƒá
RESULTS_FILE = "results_stemming.txt"

# === FUNKCJE ===
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            label, path = line.strip().split()
            full_path = os.path.join(DATA_PATH, path.replace("../", ""))
            entries.append((full_path, label))
    return entries

# Funcja do przetwarzania tekstu. Przeprowadza takie funkcje jak: czyszczenie, tokenizacja, usuwanie stopwords i (opcjonalnie) stemizacja
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if w not in stop_words and w.isalpha()]

    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(w) for w in tokens]

    return tokens

# Wczytuje zawarto≈õƒá e-maila.
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            if msg.is_multipart():
                parts = [p.get_payload(decode=True) for p in msg.get_payload() if p.get_payload()]
                text = " ".join([str(p) for p in parts])
            else:
                text = msg.get_payload(decode=True)
            if text:
                text = text.decode(errors="ignore") if isinstance(text, bytes) else text
                return text
            else:
                return ""
    except Exception:
        return ""

# Tworzy listƒô s≈Ç√≥w kluczowych na podstawie danych treningowych.
def build_blacklist(train_data, top_n=100):
    spam_words = {}
    ham_words = {}
    for tokens, label in train_data:
        for token in tokens:
            if label == "spam":
                spam_words[token] = spam_words.get(token, 0) + 1
            else:
                ham_words[token] = ham_words.get(token, 0) + 1

    spam_ratio = {word: spam_words[word] / (ham_words.get(word, 0) + 1) for word in spam_words}
    sorted_words = sorted(spam_ratio.items(), key=lambda x: x[1], reverse=True)
    return [w for w, _ in sorted_words[:top_n]]

# Zwraca etykietƒô spam/ham w zale≈ºno≈õci od obecno≈õci s≈Ç√≥w zakazanych.
def classify_email(tokens, blacklist):
    return "spam" if any(word in blacklist for word in tokens) else "ham"

# Trenuje i testuje klasyfikator; zwraca accuracy, macierz konfuzji i czas.
def evaluate_model(train_entries, test_entries, use_stemming):
    start_time = time.time()

    train_data = []
    for path, label in train_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        train_data.append((tokens, label))

    blacklist = build_blacklist(train_data, top_n=TOP_N)

    y_true, y_pred = [], []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        prediction = classify_email(tokens, blacklist)
        y_true.append(label)
        y_pred.append(prediction)

    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_true, y_pred) * 100

    return acc, cm_percent, elapsed


# === G≈Å√ìWNY PROGRAM ===
def main():
    print("üìÇ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    results_log = []

    # Test 1: ZE STEMIZACJƒÑ
    print("üß† Test 1: Z STEMIZACJƒÑ")
    acc_stem, cm_stem, time_stem = evaluate_model(train_entries, test_entries, use_stemming=True)
    print(f"üéØ Accuracy (stem): {acc_stem:.2f}% | ‚è± Czas: {time_stem:.2f}s")
    results_log.append(f"Test 1 (z stemizacjƒÖ): accuracy={acc_stem:.2f}%, czas={time_stem:.2f}s")

    # Test 2: BEZ STEMIZACJI
    print("\nüß† Test 2: BEZ STEMIZACJI")
    acc_no_stem, cm_no_stem, time_no_stem = evaluate_model(train_entries, test_entries, use_stemming=False)
    print(f"üéØ Accuracy (no stem): {acc_no_stem:.2f}% | ‚è± Czas: {time_no_stem:.2f}s")
    results_log.append(f"Test 2 (bez stemizacji): accuracy={acc_no_stem:.2f}%, czas={time_no_stem:.2f}s")

    # Por√≥wnanie wynik√≥w
    diff_acc = acc_stem - acc_no_stem
    diff_time = time_stem - time_no_stem

    summary = (
        "\nüìä POR√ìWNANIE WYNIK√ìW\n"
        f"Z stemizacjƒÖ:    {acc_stem:.2f}% ({time_stem:.2f}s)\n"
        f"Bez stemizacji:  {acc_no_stem:.2f}% ({time_no_stem:.2f}s)\n"
        f"üß© R√≥≈ºnica dok≈Çadno≈õci: {diff_acc:+.2f}%\n"
        f"‚è± R√≥≈ºnica czasu: {diff_time:+.2f}s (warto≈õƒá dodatnia = wolniej ze stemizacjƒÖ)\n"
    )

    print(summary)
    results_log.append(summary)

    # Macierze konfuzji
    matrix_report = (
        "\nüìä MACIERZ KONFUZJI (Z STEMIZACJƒÑ):\n"
        f"      spam      ham\n"
        f"spam  {cm_stem[0,0]:6.2f}%   {cm_stem[0,1]:6.2f}%\n"
        f"ham   {cm_stem[1,0]:6.2f}%   {cm_stem[1,1]:6.2f}%\n\n"
        "üìä MACIERZ KONFUZJI (BEZ STEMIZACJI):\n"
        f"      spam      ham\n"
        f"spam  {cm_no_stem[0,0]:6.2f}%   {cm_no_stem[0,1]:6.2f}%\n"
        f"ham   {cm_no_stem[1,0]:6.2f}%   {cm_no_stem[1,1]:6.2f}%\n"
    )
    print(matrix_report)
    results_log.append(matrix_report)

    # Zapis wynik√≥w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_log))

    print(f"üìÅ Wyniki zapisano do pliku: {RESULTS_FILE}")


if __name__ == "__main__":
    main()
```

#### Wyniki

### Zadanie 4
Dokonaƒá klasyfikacji binarnej wiadomo≈õci z archiwum (zadanie 1) na spam i ham, stosujƒÖc algorytmy rozmytego haszowania.

**Uwagi:**
1. Do tego celu u≈ºyƒá algorytmu LSH (MinHash, MinHashLSH) z biblioteki datasketch.
2. Wyniki pracy algorytmu przedstawiƒá przy pomocy procentowej macierzy konfuzji i wska≈∫nika accuracy.
3. Sprawdziƒá pracƒô programu dla r√≥≈ºnych warto≈õci parametru threshold funkcji MinHashLSH.
4. Por√≥wnaƒá uzyskane wyniki z wynikami z poprzednich zada≈Ñ.

#### Implementacja

**1. Konfiguracja globalna**

...

**Kod:**  
``` python
INDEX_PATH = "trec07p/full/index"
DATA_PATH = "trec07p"
TRAIN_RATIO = 0.8
SAMPLE_SIZE = None  # ograniczenie liczby pr√≥bek, np. 2000 dla test√≥w, None = ca≈Ço≈õƒá
RESULTS_FILE = "results_lsh.txt"

# Parametry LSH / MinHash
NUM_PERM = 128
SHINGLE_SIZE = 3  # rozmiar shingli (k-gram√≥w)
USE_STEMMING = True  # czy stosowaƒá stemizacjƒô
THRESHOLDS = [0.1, 0.3, 0.5, 0.7, 0.9]  # testowane progi LSH
DEFAULT_LABEL = "ham" # etykieta domy≈õlna, gdy brak dopasowa≈Ñ w LSH

random.seed(42)
```

**2. Funkcja `load_index`**

**Wej≈õcie:**  
- `index_path` (string) - ≈õcie≈ºka do pliku z indeksem wiadomo≈õci

**Wyj≈õcie:**  
- `entries` (list) - lista krotek zawierajƒÖcych pe≈ÇnƒÖ ≈õcie≈ºkƒô do pliku i etykietƒô (spam/ham)

**Opis:**  
Funkcja wczytuje plik indeksu, parsuje ka≈ºdƒÖ liniƒô rozdzielajƒÖc jƒÖ na etykietƒô i ≈õcie≈ºkƒô do pliku. Normalizuje ≈õcie≈ºki usuwajƒÖc "../" i tworzy pe≈Çne ≈õcie≈ºki wzglƒôdem g≈Ç√≥wnego katalogu danych. Zwraca listƒô wszystkich wpis√≥w gotowych do dalszego przetwarzania.

**Kod:**  
``` python
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                # Normalizuje ≈õcie≈ºkƒô: '../data/inmail.X' -> 'trec07p/data/inmail.X'
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries
```

---

**3. Funkcja `load_email_content`**

**Wej≈õcie:**  
- `filepath` (string) - ≈õcie≈ºka do pliku z wiadomo≈õciƒÖ email

**Wyj≈õcie:**  
- `payload` (string) - wyekstrahowana tre≈õƒá wiadomo≈õci lub pusty string w przypadku b≈Çƒôdu

**Opis:**  
Funkcja wczytuje i parsuje wiadomo≈õƒá email przy u≈ºyciu biblioteki email. Obs≈Çuguje zar√≥wno wiadomo≈õci wieloczƒô≈õciowe (multipart) jak i pojedyncze. Dla wiadomo≈õci wieloczƒô≈õciowych iteruje przez wszystkie czƒô≈õci i wyciƒÖga tylko te o typie tekstowym. Dekoduje zawarto≈õƒá binarnƒÖ i obs≈Çuguje b≈Çƒôdy kodowania. W przypadku wyjƒÖtk√≥w zwraca pusty string.

**Kod:**  
``` python
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            payload = ""
            if msg.is_multipart():
                # z≈ÇƒÖcz wszystkie czƒô≈õci tekstowe
                parts = []
                for part in msg.walk():
                    # tylko tekstowe czƒô≈õci (ignore attachments)
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return payload or ""
    except Exception:
        return ""
```

---

**4. Funkcja `preprocess_text`**

**Wej≈õcie:**  
- `text` (string) - tekst wiadomo≈õci email do przetworzenia
- `use_stemming` (bool) - flaga okre≈õlajƒÖca czy stosowaƒá stemizacjƒô

**Wyj≈õcie:**  
- `tokens` (list) - lista przetworzonych token√≥w (s≈Ç√≥w)

**Opis:**  
Funkcja przeprowadza pe≈Çne przetwarzanie tekstu przed u≈ºyciem w algorytmie LSH: konwersja na ma≈Çe litery, usuwanie znak√≥w interpunkcyjnych, tokenizacja na pojedyncze s≈Çowa, filtrowanie tylko s≈Ç√≥w alfabetycznych, usuwanie stopwords (s≈Ç√≥w bez znaczenia) oraz opcjonalna stemizacja przy u≈ºyciu algorytmu PorterStemmer.

**Kod:**  
``` python
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(t) for t in tokens]
    return tokens
```

---

**5. Funkcja `get_shingles`**

**Wej≈õcie:**  
- `tokens` (list) - lista token√≥w z przetworzonej wiadomo≈õci
- `k` (int) - rozmiar shingli (k-gram√≥w)

**Wyj≈õcie:**  
- `shingles` (list) - lista shingli utworzonych z ciƒÖg≈Çych sekwencji token√≥w

**Opis:**  
Funkcja tworzy k-gramy (shingle) z ciƒÖg≈Çych sekwencji token√≥w. Dla podanej listy token√≥w tworzy wszystkie mo≈ºliwe ciƒÖg≈Çe sekwencje o d≈Çugo≈õci k, ≈ÇƒÖczƒÖc je w stringi. Je≈õli d≈Çugo≈õƒá token√≥w jest mniejsza ni≈º k, zwraca oryginalne tokeny jako fallback.

**Kod:**  
``` python
def get_shingles(tokens, k=3):
    if len(tokens) < k:
        # fallback: u≈ºyj pojedynczych token√≥w
        return tokens
    shingles = []
    for i in range(len(tokens) - k + 1):
        sh = " ".join(tokens[i:i + k])
        shingles.append(sh)
    return shingles
```

---

**6. Funkcja `build_minhash_from_shingles`**

**Wej≈õcie:**  
- `shingles` (list) - lista shingli (k-gram√≥w)
- `num_perm` (int) - liczba permutacji dla algorytmu MinHash

**Wyj≈õcie:**  
- `m` (MinHash) - obiekt MinHash reprezentujƒÖcy dokument

**Opis:**  
Funkcja tworzy obiekt MinHash dla dokumentu na podstawie jego shingli. U≈ºywa zestawu unikalnych shingli aby uniknƒÖƒá duplikat√≥w. Ka≈ºdy shingle jest kodowany do postaci bajt√≥w przed dodaniem do MinHash. Parametr num_perm okre≈õla dok≈Çadno≈õƒá haszowania.

**Kod:**  
``` python
def build_minhash_from_shingles(shingles, num_perm=128):
    m = MinHash(num_perm=num_perm)
    # u≈ºywamy zestawu, aby uniknƒÖƒá wielokrotnego dodawania tego samego shingla
    for s in set(shingles):
        m.update(s.encode("utf8"))
    return m
```

---

**7. Funkcja `prepare_train_min_hashes`**

**Wej≈õcie:**  
- `train_entries` (list) - lista krotek (≈õcie≈ºka, etykieta) dla danych treningowych
- `use_stemming` (bool) - flaga okre≈õlajƒÖca czy stosowaƒá stemizacjƒô
- `shingle_k` (int) - rozmiar shingli
- `num_perm` (int) - liczba permutacji dla MinHash

**Wyj≈õcie:**  
- `id_to_minhash` (dict) - s≈Çownik mapujƒÖcy ID dokumentu na jego MinHash
- `id_to_label` (dict) - s≈Çownik mapujƒÖcy ID dokumentu na jego etykietƒô

**Opis:**  
Funkcja przetwarza wszystkie dokumenty treningowe: wczytuje tre≈õƒá, przetwarza tekst, tworzy shingle, buduje MinHash dla ka≈ºdego dokumentu. Przypisuje unikalne ID ka≈ºdemu dokumentowi i zwraca dwa s≈Çowniki do dalszego u≈ºycia w LSH.

**Kod:**  
``` python
def prepare_train_min_hashes(train_entries, use_stemming=True, shingle_k=3, num_perm=128):
    id_to_minhash = {}
    id_to_label = {}
    for idx, (path, label) in enumerate(train_entries):
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        doc_id = f"doc{idx}"
        id_to_minhash[doc_id] = m
        id_to_label[doc_id] = label
    return id_to_minhash, id_to_label
```

---

**8. Funkcja `classify_with_lsh`**

**Wej≈õcie:**  
- `lsh` (MinHashLSH) - obiekt LSH z wstawionymi dokumentami treningowymi
- `train_label_map` (dict) - s≈Çownik mapujƒÖcy ID na etykiety treningowe
- `test_entries` (list) - lista krotek (≈õcie≈ºka, etykieta) dla danych testowych
- `use_stemming` (bool) - flaga okre≈õlajƒÖca czy stosowaƒá stemizacjƒô
- `shingle_k` (int) - rozmiar shingli
- `num_perm` (int) - liczba permutacji dla MinHash

**Wyj≈õcie:**  
- `y_true` (list) - lista prawdziwych etykiet
- `y_pred` (list) - lista przewidywanych etykiet

**Opis:**  
Funkcja klasyfikuje dokumenty testowe u≈ºywajƒÖc LSH. Dla ka≈ºdego dokumentu testowego: przetwarza tekst, tworzy shingle, buduje MinHash, pyta LSH o podobne dokumenty. Je≈õli znaleziono dopasowania, przeprowadza g≈Çosowanie wiƒôkszo≈õciowe na podstawie etykiet dokument√≥w treningowych. Je≈õli brak dopasowa≈Ñ, u≈ºywa etykiety domy≈õlnej.

**Kod:**  
``` python
def classify_with_lsh(lsh, train_label_map, test_entries, use_stemming=True, shingle_k=3, num_perm=128):
    y_true = []
    y_pred = []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        matches = lsh.query(m)  # lista dopasowanych dokument√≥w treningowych
        if matches:
            # g≈Çosowanie wiƒôkszo≈õciowe etykiet
            votes = [train_label_map[mid] for mid in matches if mid in train_label_map]
            if votes:
                counter = Counter(votes)
                pred = counter.most_common(1)[0][0]
            else:
                pred = DEFAULT_LABEL
        else:
            pred = DEFAULT_LABEL
        y_true.append(label)
        y_pred.append(pred)
    return y_true, y_pred
```

---

**9. Funkcja `main`**

**Wej≈õcie:**  
- Brak parametr√≥w wej≈õciowych

**Wyj≈õcie:**  
- Brak bezpo≈õredniego wyj≈õcia (funkcja wykonuje program i zapisuje wyniki do pliku)

**Opis:**  
G≈Ç√≥wna funkcja programu koordynujƒÖca ca≈Çy proces klasyfikacji LSH: wczytuje i tasuje dane, dzieli na zbiory treningowe i testowe, przygotowuje MinHash dla danych treningowych, testuje r√≥≈ºne warto≈õci threshold dla LSH, oblicza metryki wydajno≈õci dla ka≈ºdego threshold i zapisuje szczeg√≥≈Çowe wyniki do pliku. Dla ka≈ºdego threshold buduje nowy indeks LSH i przeprowadza klasyfikacjƒô.

**Kod:**  
``` python
def main():
    print("üìÇ Wczytywanie indexu i danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"‚ö†Ô∏è SAMPLE_SIZE active: using first {len(index_entries)} entries")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    print(f"≈ÅƒÖcznie: {len(index_entries)} dokument√≥w; trening: {len(train_entries)}; test: {len(test_entries)}")
    results_lines = []
    results_lines.append(f"LSH MinHash results\nSAMPLE_SIZE={SAMPLE_SIZE}\nNUM_PERM={NUM_PERM}\nSHINGLE_SIZE={SHINGLE_SIZE}\nUSE_STEMMING={USE_STEMMING}\n")

    # Przygotowuje MinHash na treningu (raz). Bƒôdzie ono wstawiane do nowych LSH dla r√≥≈ºnych threshold√≥w
    print("üß† Budowanie MinHash dla zbioru treningowego...")
    t0 = time.time()
    train_mh_map, train_label_map = prepare_train_min_hashes(train_entries, use_stemming=USE_STEMMING,
                                                            shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
    t_prep = time.time() - t0
    print(f"Gotowe. Czas przygotowania MinHash treningu: {t_prep:.2f}s")
    results_lines.append(f"prepare_time={t_prep:.2f}s\n")

    # Dla ka≈ºdego threshold buduje nowy MinHashLSH (z tym samym num_perm) i wstawia minhashy treningowe
    for thresh in THRESHOLDS:
        print(f"\nüîé Test dla threshold = {thresh}")
        results_lines.append(f"\nTHRESHOLD={thresh}\n")
        # buduje LSH z parametrem threshold
        t0 = time.time()
        lsh = MinHashLSH(threshold=thresh, num_perm=NUM_PERM)
        # wstawia minhashy treningowe
        for doc_id, mh in train_mh_map.items():
            lsh.insert(doc_id, mh)
        build_time = time.time() - t0
        print(f"LSH zbudowano w {build_time:.2f}s")

        # klasyfikacja test√≥w
        t1 = time.time()
        y_true, y_pred = classify_with_lsh(lsh, train_label_map, test_entries,
                                          use_stemming=USE_STEMMING, shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
        elapsed = time.time() - t1

        # metryki
        labels = ["spam", "ham"]
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        cm_percent = cm / np.sum(cm) * 100
        acc = accuracy_score(y_true, y_pred) * 100

        # raport w konsoli
        print(f"üéØ Accuracy: {acc:.2f}% | build_time: {build_time:.2f}s | classify_time: {elapsed:.2f}s")
        print("üìä Confusion matrix (%):")
        print(f"      spam      ham")
        print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
        print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

        # zapis wynik√≥w
        results_lines.append(f"accuracy={acc:.2f}%\n")
        results_lines.append(f"build_time={build_time:.2f}s classify_time={elapsed:.2f}s\n")
        results_lines.append("confusion_percent:\n")
        results_lines.append(f"spam_spam={cm_percent[0,0]:6.2f}% spam_ham={cm_percent[0,1]:6.2f}%\n")
        results_lines.append(f"ham_spam={cm_percent[1,0]:6.2f}% ham_ham={cm_percent[1,1]:6.2f}%\n")

    # zapis do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_lines))

    print(f"\nüìÅ Wyniki zapisano do: {RESULTS_FILE}")
```

---

**10. Kompletny kod**  
Poni≈ºej znajduje siƒô kompletny kod programu, kt√≥ry mo≈ºna uruchomiƒá.

**Kod:**  
``` python
import os
import string
import random
import time
from email import message_from_file
from collections import Counter, defaultdict

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

from datasketch import MinHash, MinHashLSH
from sklearn.metrics import confusion_matrix, accuracy_score
import numpy as np

# === KONFIGURACJA ===
INDEX_PATH = "trec07p/full/index"
DATA_PATH = "trec07p"
TRAIN_RATIO = 0.8
SAMPLE_SIZE = None  # ograniczenie liczby pr√≥bek, np. 2000 dla test√≥w, None = ca≈Ço≈õƒá
RESULTS_FILE = "results_lsh.txt"

# Parametry LSH / MinHash
NUM_PERM = 128
SHINGLE_SIZE = 3  # rozmiar shingli (k-gram√≥w)
USE_STEMMING = True  # czy stosowaƒá stemizacjƒô
THRESHOLDS = [0.1, 0.3, 0.5, 0.7, 0.9]  # testowane progi LSH
DEFAULT_LABEL = "ham" # etykieta domy≈õlna, gdy brak dopasowa≈Ñ w LSH

random.seed(42)


# === POMOCNICZE FUNKCJE ===
# wczytuje index plik√≥w i etykiet
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                # Normalizuje ≈õcie≈ºkƒô: '../data/inmail.X' -> 'trec07p/data/inmail.X'
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries

# Wczytuje zawarto≈õƒá e-maila i zwraca string tekstowy (ignoruje b≈Çƒôdy kodowania)
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            payload = ""
            if msg.is_multipart():
                # z≈ÇƒÖcz wszystkie czƒô≈õci tekstowe
                parts = []
                for part in msg.walk():
                    # tylko tekstowe czƒô≈õci (ignore attachments)
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return payload or ""
    except Exception:
        return ""


# Przetwarza tekst: czyszczenie, tokenizacja, usuwanie stopwords i (opcjonalnie) stemizacja. zwraca listƒô token√≥w
def preprocess_text(text, use_stemming=True):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    if use_stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(t) for t in tokens]
    return tokens

# Zwraca listƒô shingli (k-gram√≥w) utworzonych z token√≥w (continuous k-word shingles)
def get_shingles(tokens, k=3):
    if len(tokens) < k:
        # fallback: u≈ºyj pojedynczych token√≥w
        return tokens
    shingles = []
    for i in range(len(tokens) - k + 1):
        sh = " ".join(tokens[i:i + k])
        shingles.append(sh)
    return shingles


# Zwraca MinHash obliczony na zestawie shingles (unikatowych).
def build_minhash_from_shingles(shingles, num_perm=128):
    m = MinHash(num_perm=num_perm)
    # u≈ºywamy zestawu, aby uniknƒÖƒá wielokrotnego dodawania tego samego shingla
    for s in set(shingles):
        m.update(s.encode("utf8"))
    return m


# === PROCEDURY TRENING / TEST ===
# Przygotowuje MinHash dla ka≈ºdego dokumentu treningowego i mapuje identyfikatory na etykiety. Zwraca dwa s≈Çowniki 
def prepare_train_min_hashes(train_entries, use_stemming=True, shingle_k=3, num_perm=128):
    id_to_minhash = {}
    id_to_label = {}
    for idx, (path, label) in enumerate(train_entries):
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        doc_id = f"doc{idx}"
        id_to_minhash[doc_id] = m
        id_to_label[doc_id] = label
    return id_to_minhash, id_to_label


# Dla ka≈ºdego dokumentu testowego: oblicza MinHash, pyta LSH o dopasowania, je≈õli lista niepusta - dokonuje g≈Çosowania etykiet (majority vote), je≈õli pusta - przypisuje DEFAULT_LABEL
def classify_with_lsh(lsh, train_label_map, test_entries, use_stemming=True, shingle_k=3, num_perm=128):
    y_true = []
    y_pred = []
    for path, label in test_entries:
        text = load_email_content(path)
        tokens = preprocess_text(text, use_stemming)
        shingles = get_shingles(tokens, k=shingle_k)
        m = build_minhash_from_shingles(shingles, num_perm=num_perm)
        matches = lsh.query(m)  # lista dopasowanych dokument√≥w treningowych
        if matches:
            # g≈Çosowanie wiƒôkszo≈õciowe etykiet
            votes = [train_label_map[mid] for mid in matches if mid in train_label_map]
            if votes:
                counter = Counter(votes)
                pred = counter.most_common(1)[0][0]
            else:
                pred = DEFAULT_LABEL
        else:
            pred = DEFAULT_LABEL
        y_true.append(label)
        y_pred.append(pred)
    return y_true, y_pred


# === G≈Å√ìWNY PROGRAM ===
def main():
    print("üìÇ Wczytywanie indexu i danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"‚ö†Ô∏è SAMPLE_SIZE active: using first {len(index_entries)} entries")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]

    print(f"≈ÅƒÖcznie: {len(index_entries)} dokument√≥w; trening: {len(train_entries)}; test: {len(test_entries)}")
    results_lines = []
    results_lines.append(f"LSH MinHash results\nSAMPLE_SIZE={SAMPLE_SIZE}\nNUM_PERM={NUM_PERM}\nSHINGLE_SIZE={SHINGLE_SIZE}\nUSE_STEMMING={USE_STEMMING}\n")

    # Przygotowuje MinHash na treningu (raz). Bƒôdzie ono wstawiane do nowych LSH dla r√≥≈ºnych threshold√≥w
    print("üß† Budowanie MinHash dla zbioru treningowego...")
    t0 = time.time()
    train_mh_map, train_label_map = prepare_train_min_hashes(train_entries, use_stemming=USE_STEMMING,
                                                            shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
    t_prep = time.time() - t0
    print(f"Gotowe. Czas przygotowania MinHash treningu: {t_prep:.2f}s")
    results_lines.append(f"prepare_time={t_prep:.2f}s\n")

    # Dla ka≈ºdego threshold buduje nowy MinHashLSH (z tym samym num_perm) i wstawia minhashy treningowe
    for thresh in THRESHOLDS:
        print(f"\nüîé Test dla threshold = {thresh}")
        results_lines.append(f"\nTHRESHOLD={thresh}\n")
        # buduje LSH z parametrem threshold
        t0 = time.time()
        lsh = MinHashLSH(threshold=thresh, num_perm=NUM_PERM)
        # wstawia minhashy treningowe
        for doc_id, mh in train_mh_map.items():
            lsh.insert(doc_id, mh)
        build_time = time.time() - t0
        print(f"LSH zbudowano w {build_time:.2f}s")

        # klasyfikacja test√≥w
        t1 = time.time()
        y_true, y_pred = classify_with_lsh(lsh, train_label_map, test_entries,
                                          use_stemming=USE_STEMMING, shingle_k=SHINGLE_SIZE, num_perm=NUM_PERM)
        elapsed = time.time() - t1

        # metryki
        labels = ["spam", "ham"]
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        cm_percent = cm / np.sum(cm) * 100
        acc = accuracy_score(y_true, y_pred) * 100

        # raport w konsoli
        print(f"üéØ Accuracy: {acc:.2f}% | build_time: {build_time:.2f}s | classify_time: {elapsed:.2f}s")
        print("üìä Confusion matrix (%):")
        print(f"      spam      ham")
        print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
        print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

        # zapis wynik√≥w
        results_lines.append(f"accuracy={acc:.2f}%\n")
        results_lines.append(f"build_time={build_time:.2f}s classify_time={elapsed:.2f}s\n")
        results_lines.append("confusion_percent:\n")
        results_lines.append(f"spam_spam={cm_percent[0,0]:6.2f}% spam_ham={cm_percent[0,1]:6.2f}%\n")
        results_lines.append(f"ham_spam={cm_percent[1,0]:6.2f}% ham_ham={cm_percent[1,1]:6.2f}%\n")

    # zapis do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(results_lines))

    print(f"\nüìÅ Wyniki zapisano do: {RESULTS_FILE}")


if __name__ == "__main__":
    main()
```

#### Wyniki

### Zadanie 5
Dokonaƒá klasyfikacji binarnej wiadomo≈õci z archiwum (zadanie 1) na spam i ham, stosujƒÖc algorytm Naive Bayes.

**Uwagi:**
1. Do realizacji zadania nale≈ºy u≈ºyƒá implementacji algorytmu z biblioteki Scikit-learn. Algorytm dostƒôpny jest poprzez obiekt MultinomialNB.
2. Por√≥wnaƒá dzia≈Çanie algorytmu dla przypadk√≥w:
   - algorytm pracuje na ca≈Çych tematach i ciele wiadomo≈õci w postaci zwyk≈Çego tekstu bez usuwania s≈Ç√≥w przestankowych i stemizacji przy pomocy narzƒôdzi z biblioteki NLTK.
   - algorytm pracuje na bazie stemizowanych danych z usuniƒôtymi s≈Çowami przestankowymi.
1. Uzyskane wyniki przedstawiƒá przy pomocy macierzy konfuzji i wska≈∫nika accuracy.
2. Por√≥wnaƒá uzyskane wyniki do wynik√≥w uzyskanych przy zastosowaniu metod z poprzednich zada≈Ñ.

#### Implementacja

**1. Konfiguracja globalna**

...

**Kod:**  
``` python
INDEX_PATH = "trec07p/full/index"
DATA_PATH = "trec07p"
TRAIN_RATIO = 0.8
SAMPLE_SIZE = None  # ograniczenie liczby pr√≥bek, np. 2000 dla test√≥w, None = ca≈Ço≈õƒá
RESULTS_FILE = "results_naive_bayes.txt"

random.seed(42)
```

**2. Funkcja `load_index`**

**Wej≈õcie:**  
- `index_path` (string) - ≈õcie≈ºka do pliku z indeksem wiadomo≈õci

**Wyj≈õcie:**  
- `entries` (list) - lista krotek zawierajƒÖcych pe≈ÇnƒÖ ≈õcie≈ºkƒô do pliku i etykietƒô (spam/ham)

**Opis:**  
Funkcja wczytuje plik indeksu TREC07P, parsuje ka≈ºdƒÖ liniƒô rozdzielajƒÖc jƒÖ na etykietƒô (spam/ham) i ≈õcie≈ºkƒô do pliku. Tworzy pe≈Çne ≈õcie≈ºki do plik√≥w przez po≈ÇƒÖczenie ≈õcie≈ºki bazowej DATA_PATH ze ≈õcie≈ºkƒÖ z indeksu (po usuniƒôciu "../"). Zwraca listƒô wszystkich wpis√≥w gotowych do przetwarzania.

**Kod:**  
``` python
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries
```

---

**3. Funkcja `load_email_content`**

**Wej≈õcie:**  
- `filepath` (string) - ≈õcie≈ºka do pliku z wiadomo≈õciƒÖ email

**Wyj≈õcie:**  
- `text` (string) - po≈ÇƒÖczony temat i tre≈õƒá wiadomo≈õci lub pusty string w przypadku b≈Çƒôdu

**Opis:**  
Funkcja wczytuje i parsuje wiadomo≈õƒá email, wyciƒÖgajƒÖc zar√≥wno temat (Subject) jak i tre≈õƒá wiadomo≈õci. Obs≈Çuguje wiadomo≈õci wieloczƒô≈õciowe (multipart) - iteruje przez wszystkie czƒô≈õci i wyciƒÖga tylko te o typie tekstowym. ≈ÅƒÖczy temat z tre≈õciƒÖ w jeden string, co zapewnia, ≈ºe algorytm Naive Bayes bƒôdzie wykorzystywa≈Ç ca≈ÇƒÖ dostƒôpnƒÖ informacjƒô tekstowƒÖ.

**Kod:**  
``` python
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            subject = msg.get("Subject", "")
            payload = ""
            if msg.is_multipart():
                parts = []
                for part in msg.walk():
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return subject + " " + payload
    except Exception:
        return ""
```

---

**4. Funkcja `preprocess_text`**

**Wej≈õcie:**  
- `text` (string) - tekst wiadomo≈õci email do przetworzenia

**Wyj≈õcie:**  
- `text` (string) - przetworzony tekst po stemizacji i usuniƒôciu stopwords

**Opis:**  
Funkcja przeprowadza pe≈Çne przetwarzanie tekstu NLTK: konwersja na ma≈Çe litery, usuwanie znak√≥w interpunkcyjnych, tokenizacja na pojedyncze s≈Çowa, filtrowanie tylko s≈Ç√≥w alfabetycznych, usuwanie stopwords (s≈Ç√≥w bez znaczenia) oraz stemizacja przy u≈ºyciu algorytmu PorterStemmer. Na ko≈Ñcu ≈ÇƒÖczy tokeny z powrotem w string dla kompatybilno≈õci z CountVectorizer.

**Kod:**  
``` python
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(t) for t in tokens]
    return " ".join(tokens)
```

---

**5. Funkcja `prepare_data`**

**Wej≈õcie:**  
- `entries` (list) - lista krotek (≈õcie≈ºka, etykieta) do przetworzenia
- `use_preprocessing` (bool) - flaga okre≈õlajƒÖca czy stosowaƒá przetwarzanie NLTK

**Wyj≈õcie:**  
- `texts` (list) - lista tekst√≥w wiadomo≈õci (przetworzonych lub nie)
- `labels` (list) - lista etykiet (spam/ham)

**Opis:**  
Funkcja przetwarza wszystkie dokumenty z podanej listy. Dla ka≈ºdego dokumentu wczytuje tre≈õƒá emaila i opcjonalnie stosuje preprocessing NLTK w zale≈ºno≈õci od parametru `use_preprocessing`. Zwraca dwie listy: tekst√≥w przygotowanych do wektoryzacji oraz odpowiadajƒÖcych im etykiet.

**Kod:**  
``` python
def prepare_data(entries, use_preprocessing=False):
    texts, labels = [], []
    for path, label in entries:
        text = load_email_content(path)
        if use_preprocessing:
            text = preprocess_text(text)
        texts.append(text)
        labels.append(label)
    return texts, labels
```

---

**6. Funkcja `run_naive_bayes`**

**Wej≈õcie:**  
- `train_entries` (list) - lista krotek (≈õcie≈ºka, etykieta) dla danych treningowych
- `test_entries` (list) - lista krotek (≈õcie≈ºka, etykieta) dla danych testowych
- `use_preprocessing` (bool) - flaga okre≈õlajƒÖca czy stosowaƒá przetwarzanie NLTK

**Wyj≈õcie:**  
- `acc` (float) - dok≈Çadno≈õƒá klasyfikacji w procentach
- `cm_percent` (numpy.ndarray) - macierz konfuzji w procentach
- `elapsed` (float) - czas wykonania w sekundach

**Opis:**  
Funkcja przeprowadza pe≈Çny eksperyment z klasyfikatorem Naive Bayes: przygotowuje dane treningowe i testowe, tworzy macierz cech przy u≈ºyciu CountVectorizer (bag-of-words), trenuje model MultinomialNB, dokonuje predykcji na danych testowych i oblicza metryki wydajno≈õci. Wy≈õwietla szczeg√≥≈Çowe wyniki w konsoli i zwraca warto≈õci do dalszej analizy.

**Kod:**  
``` python
def run_naive_bayes(train_entries, test_entries, use_preprocessing=False):
    print(f"\nüß† Uruchamianie Naive Bayes ({'z preprocessingiem' if use_preprocessing else 'bez preprocessing'})...")
    start_time = time.time()

    # Przygotowanie danych
    X_train_texts, y_train = prepare_data(train_entries, use_preprocessing)
    X_test_texts, y_test = prepare_data(test_entries, use_preprocessing)

    # Konwersja do macierzy cech (bag of words)
    vectorizer = CountVectorizer()
    X_train = vectorizer.fit_transform(X_train_texts)
    X_test = vectorizer.transform(X_test_texts)

    # Trening
    model = MultinomialNB()
    model.fit(X_train, y_train)

    # Predykcja
    y_pred = model.predict(X_test)

    # Ewaluacja
    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_test, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_test, y_pred) * 100

    # Wy≈õwietlenie wynik√≥w
    print(f"üéØ Accuracy: {acc:.2f}% | Czas wykonania: {elapsed:.2f}s")
    print("üìä Confusion matrix (%):")
    print(f"      spam      ham")
    print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
    print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

    return acc, cm_percent, elapsed
```

---

**7. Funkcja `main`**

**Wej≈õcie:**  
- Brak parametr√≥w wej≈õciowych

**Wyj≈õcie:**  
- Brak bezpo≈õredniego wyj≈õcia (funkcja wykonuje program i zapisuje wyniki do pliku)

**Opis:**  
G≈Ç√≥wna funkcja programu koordynujƒÖca eksperymenty z Naive Bayes: wczytuje i tasuje dane, dzieli na zbiory treningowe i testowe, przeprowadza dwa eksperymenty (bez przetwarzania tekstu i z pe≈Çnym przetwarzaniem NLTK), por√≥wnuje wyniki pod wzglƒôdem accuracy i macierzy konfuzji, oraz zapisuje szczeg√≥≈Çowe wyniki do pliku tekstowego. Eksperymenty pozwalajƒÖ na por√≥wnanie wp≈Çywu preprocessingu na skuteczno≈õƒá klasyfikacji.

**Kod:**  
``` python
def main():
    print("üìÇ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"‚ö†Ô∏è SAMPLE_SIZE active: using first {len(index_entries)} entries")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]
    print(f"≈ÅƒÖcznie: {len(index_entries)} dokument√≥w; trening: {len(train_entries)}; test: {len(test_entries)}")

    # Wyniki
    results = []

    # Wersja bez preprocessingu (pe≈Çny tekst)
    acc_raw, cm_raw, t_raw = run_naive_bayes(train_entries, test_entries, use_preprocessing=False)
    results.append(("Bez preprocessing", acc_raw, cm_raw, t_raw))

    # Wersja z preprocessingiem (usuwanie stopwords i stemizacja)
    acc_clean, cm_clean, t_clean = run_naive_bayes(train_entries, test_entries, use_preprocessing=True)
    results.append(("Z preprocessingiem (NLTK)", acc_clean, cm_clean, t_clean))

    # Zapis wynik√≥w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("Naive Bayes Results\n\n")
        for title, acc, cm, t in results:
            f.write(f"{title}\n")
            f.write(f"Accuracy: {acc:.2f}%\nCzas: {t:.2f}s\n")
            f.write("Confusion matrix (%):\n")
            f.write(f"spam_spam={cm[0,0]:.2f}% spam_ham={cm[0,1]:.2f}%\n")
            f.write(f"ham_spam={cm[1,0]:.2f}% ham_ham={cm[1,1]:.2f}%\n\n")

    print(f"\nüìÅ Wyniki zapisano do: {RESULTS_FILE}")
```

---

**8. Kompletny kod**  
Poni≈ºej znajduje siƒô kompletny kod programu, kt√≥ry mo≈ºna uruchomiƒá.

**Kod:**  
``` python
import os
import string
import random
import time
from email import message_from_file

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, accuracy_score
import numpy as np

# === KONFIGURACJA ===
INDEX_PATH = "trec07p/full/index"
DATA_PATH = "trec07p"
TRAIN_RATIO = 0.8
SAMPLE_SIZE = None  # ograniczenie liczby pr√≥bek, np. 2000 dla test√≥w, None = ca≈Ço≈õƒá
RESULTS_FILE = "results_naive_bayes.txt"

random.seed(42)


# === POMOCNICZE FUNKCJE ===
def load_index(index_path):
    entries = []
    with open(index_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                label, path = parts[0], parts[1]
                full_path = os.path.join(DATA_PATH, path.replace("../", ""))
                entries.append((full_path, label))
    return entries

# Wczytuje tre≈õƒá e-maila (temat + cia≈Ço) jako zwyk≈Çy tekst
def load_email_content(filepath):
    try:
        with open(filepath, "r", encoding="latin-1") as f:
            msg = message_from_file(f)
            subject = msg.get("Subject", "")
            payload = ""
            if msg.is_multipart():
                parts = []
                for part in msg.walk():
                    ctype = part.get_content_type()
                    if ctype.startswith("text/"):
                        p = part.get_payload(decode=True)
                        if p:
                            parts.append(p)
                payload = " ".join(str(p) for p in parts)
            else:
                p = msg.get_payload(decode=True)
                payload = p if p else ""
            if isinstance(payload, bytes):
                payload = payload.decode(errors="ignore")
            return subject + " " + payload
    except Exception:
        return ""

# Usuwa interpunkcjƒô, stopwords i dokonuje stemizacji
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    sw = set(stopwords.words("english"))
    tokens = [t for t in tokens if t.isalpha() and t not in sw]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(t) for t in tokens]
    return " ".join(tokens)

# Zwraca listƒô tekst√≥w i etykiet (spam/ham), z opcjonalnym preprocessingiem.
def prepare_data(entries, use_preprocessing=False):
    texts, labels = [], []
    for path, label in entries:
        text = load_email_content(path)
        if use_preprocessing:
            text = preprocess_text(text)
        texts.append(text)
        labels.append(label)
    return texts, labels


# === FUNKCJA EKSPERYMENTU ===
#  Trenuje i testuje klasyfikator MultinomialNB dla zbioru TREC07P. Zwraca accuracy, macierz konfuzji i czas wykonania.
def run_naive_bayes(train_entries, test_entries, use_preprocessing=False):
    print(f"\nüß† Uruchamianie Naive Bayes ({'z preprocessingiem' if use_preprocessing else 'bez preprocessing'})...")
    start_time = time.time()

    # Przygotowanie danych
    X_train_texts, y_train = prepare_data(train_entries, use_preprocessing)
    X_test_texts, y_test = prepare_data(test_entries, use_preprocessing)

    # Konwersja do macierzy cech (bag of words)
    vectorizer = CountVectorizer()
    X_train = vectorizer.fit_transform(X_train_texts)
    X_test = vectorizer.transform(X_test_texts)

    # Trening
    model = MultinomialNB()
    model.fit(X_train, y_train)

    # Predykcja
    y_pred = model.predict(X_test)

    # Ewaluacja
    elapsed = time.time() - start_time
    labels = ["spam", "ham"]
    cm = confusion_matrix(y_test, y_pred, labels=labels)
    cm_percent = cm / np.sum(cm) * 100
    acc = accuracy_score(y_test, y_pred) * 100

    # Wy≈õwietlenie wynik√≥w
    print(f"üéØ Accuracy: {acc:.2f}% | Czas wykonania: {elapsed:.2f}s")
    print("üìä Confusion matrix (%):")
    print(f"      spam      ham")
    print(f"spam  {cm_percent[0,0]:6.2f}%   {cm_percent[0,1]:6.2f}%")
    print(f"ham   {cm_percent[1,0]:6.2f}%   {cm_percent[1,1]:6.2f}%")

    return acc, cm_percent, elapsed


# === G≈Å√ìWNY PROGRAM ===
def main():
    print("üìÇ Wczytywanie danych...")
    index_entries = load_index(INDEX_PATH)
    random.shuffle(index_entries)

    if SAMPLE_SIZE:
        index_entries = index_entries[:SAMPLE_SIZE]
        print(f"‚ö†Ô∏è SAMPLE_SIZE active: using first {len(index_entries)} entries")

    split_point = int(len(index_entries) * TRAIN_RATIO)
    train_entries = index_entries[:split_point]
    test_entries = index_entries[split_point:]
    print(f"≈ÅƒÖcznie: {len(index_entries)} dokument√≥w; trening: {len(train_entries)}; test: {len(test_entries)}")

    # Wyniki
    results = []

    # Wersja bez preprocessingu (pe≈Çny tekst)
    acc_raw, cm_raw, t_raw = run_naive_bayes(train_entries, test_entries, use_preprocessing=False)
    results.append(("Bez preprocessing", acc_raw, cm_raw, t_raw))

    # Wersja z preprocessingiem (usuwanie stopwords i stemizacja)
    acc_clean, cm_clean, t_clean = run_naive_bayes(train_entries, test_entries, use_preprocessing=True)
    results.append(("Z preprocessingiem (NLTK)", acc_clean, cm_clean, t_clean))

    # Zapis wynik√≥w do pliku
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write("Naive Bayes Results\n\n")
        for title, acc, cm, t in results:
            f.write(f"{title}\n")
            f.write(f"Accuracy: {acc:.2f}%\nCzas: {t:.2f}s\n")
            f.write("Confusion matrix (%):\n")
            f.write(f"spam_spam={cm[0,0]:.2f}% spam_ham={cm[0,1]:.2f}%\n")
            f.write(f"ham_spam={cm[1,0]:.2f}% ham_ham={cm[1,1]:.2f}%\n\n")

    print(f"\nüìÅ Wyniki zapisano do: {RESULTS_FILE}")

if __name__ == "__main__":
    main()
```


#### Wyniki

