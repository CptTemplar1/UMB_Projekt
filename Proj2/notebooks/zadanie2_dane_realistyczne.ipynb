{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "from data_generation import generate_realistic_data\n",
    "from feature_engineering import normalize_data\n",
    "from model_training import train_model, evaluate_model, optimize_threshold\n",
    "from visualization import (plot_distributions, plot_confusion_matrix, plot_roc_curve, plot_task2_cost, plot_correlation_matrix, plot_betas, plot_feature_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Generowanie danych\n",
    "print(\"Generowanie danych realistycznych (niezbalansowanych)...\")\n",
    "df, feature_names = generate_realistic_data()\n",
    "X = df[feature_names].values\n",
    "y = df['Target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94304d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Podział i Normalizacja\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_train, X_test, scaler = normalize_data(X_train_raw, X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Wizualizacja \n",
    "plot_distributions(df, feature_names, title_prefix='[Zad 2]')\n",
    "plot_correlation_matrix(df, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dbefe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Eksperyment - 3 Modele\n",
    "# 1. Standard\n",
    "print(\"Model Standard...\")\n",
    "model_std = train_model(X_train, y_train)\n",
    "met_std, pred_std, prob_std = evaluate_model(model_std, X_test, y_test)\n",
    "\n",
    "# 2. Balanced\n",
    "print(\"Model Balanced...\")\n",
    "model_bal = train_model(X_train, y_train, class_weight='balanced')\n",
    "met_bal, pred_bal, prob_bal = evaluate_model(model_bal, X_test, y_test)\n",
    "\n",
    "# 3. Optimized Threshold (na modelu Standard)\n",
    "print(\"Optymalizacja progu...\")\n",
    "tau_opt, costs, thresholds = optimize_threshold(model_std, X_test, y_test)\n",
    "met_opt, pred_opt, _ = evaluate_model(model_std, X_test, y_test, threshold=tau_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc47a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Wyniki\n",
    "print(f\"Standard FN: {met_std['FN']}\")\n",
    "print(f\"Balanced FN: {met_bal['FN']}\")\n",
    "print(f\"Optimized FN: {met_opt['FN']} (Tau={tau_opt:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc73b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Wizualizacja\n",
    "# 1. Macierze pomyłek\n",
    "plot_confusion_matrix(y_test, pred_std, title='Standard Model')\n",
    "plot_confusion_matrix(y_test, pred_bal, title='Balanced Model')\n",
    "plot_confusion_matrix(y_test, pred_opt, title=f'Optimized Threshold (Tau={tau_opt:.2f})')\n",
    "\n",
    "# 2. Koszt\n",
    "plot_task2_cost(thresholds, costs, tau_opt, min(costs))\n",
    "\n",
    "# 3. Krzywe ROC (Porównanie Standard vs Balanced na jednym wykresie)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(met_std['FPR'], met_std['TPR'], label=f\"Standard (AUC={met_std['AUC']:.3f})\")\n",
    "plt.plot(met_bal['FPR'], met_bal['TPR'], label=f\"Balanced (AUC={met_bal['AUC']:.3f})\", color='green')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 4. Porównanie Betas (Współczynników)\n",
    "# Rysujemy obok siebie, aby zobaczyć różnicę w wagach\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "beta_std = model_std.coef_[0]\n",
    "beta_bal = model_bal.coef_[0]\n",
    "idx = np.arange(len(feature_names))\n",
    "\n",
    "ax[0].barh(feature_names, beta_std, color='blue')\n",
    "ax[0].set_title(\"Betas: Standard Model\")\n",
    "ax[1].barh(feature_names, beta_bal, color='green')\n",
    "ax[1].set_title(\"Betas: Balanced Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. NOWE: Analiza wpływu cech dla konkretnych typów ataków (Lokalna interpretowalność)\n",
    "# Szukamy po jednej próbce z każdego rodzaju ataku w zbiorze testowym\n",
    "print(\"\\n--- Analiza wpływu cech dla różnych typów ataków (Balanced Model) ---\")\n",
    "# Musimy odzyskać typy ataków dla zbioru testowego (trochę trik, bo split miesza indeksy)\n",
    "# Dla uproszczenia w notebooku znajdziemy próbki w X_test, które \"wyglądają\" jak ataki\n",
    "# (W pełnym rozwiązaniu przekazywalibyśmy typy z funkcji split, tu zrobimy symulację dla demonstracji)\n",
    "\n",
    "# Wybieramy losowe próbki sklasyfikowane jako atak przez model Balanced\n",
    "attack_indices = np.where(pred_bal == 1)[0]\n",
    "if len(attack_indices) > 0:\n",
    "    chosen_idx = attack_indices[0] # Pierwszy z brzegu wykryty atak\n",
    "    prob = prob_bal[chosen_idx]\n",
    "    print(f\"Przykładowy wykryty atak (Index testowy: {chosen_idx})\")\n",
    "    plot_feature_impact(model_bal, X_test, feature_names, sample_idx=chosen_idx, prediction_prob=prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41aa84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8:  Wykres liniowy pokazujący jak Precision(τ), Recall(τ) i F1(τ) zmieniają się w funkcji progu\n",
    "# Nie ma tego w src, więc robimy to tutaj bezpośrednio\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds_pr = precision_recall_curve(y_test, prob_std)\n",
    "# F1 dla każdego progu\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds_pr, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds_pr, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "plt.plot(thresholds_pr, f1_scores[:-1], \"r-\", label=\"F1 Score\")\n",
    "plt.xlabel(\"Próg decyzyjny (Threshold)\")\n",
    "plt.ylabel(\"Wartość\")\n",
    "plt.title(\"Metryki vs Próg (Model Standard)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
